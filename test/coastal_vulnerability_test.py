from __future__ import print_function

import unittest
import logging
import os
import subprocess
import math
import numpy as np
import random
import time
import csv
import glob

from osgeo import gdal
from osgeo import ogr
from nose.plugins.skip import SkipTest

import invest_test_core
from invest_natcap import raster_utils
from invest_natcap.coastal_vulnerability \
    import coastal_vulnerability as cv
from invest_natcap.coastal_vulnerability \
    import coastal_vulnerability_core as cvc


LOGGER = logging.getLogger('coastal_vulnerability_test')
logging.basicConfig(format='%(asctime)s %(name)-15s %(levelname)-8s \
    %(message)s', level=logging.DEBUG, datefmt='%m/%d/%Y %H:%M:%S ')

class TestCoastalVulnerability(unittest.TestCase):
    """Main testing class for the coastal vulnerability tests"""
    
    def make_input_data(self):
        args = {}

        args['execution_mode'] = 'generate all data'
        #args['execution_mode'] = 'fast'
        args['area_computed'] = 'both' 
        #args['area_computed'] = 'sheltered' 
        args['workspace_dir'] = \
        u'data/test_out/coastal_vulnerability/'
        args['output_base_directory'] = \
        u'data/test_out/coastal_vulnerability/'
        args['aoi_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/AOI_BarkClay.shp')
        args['landmass_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/global_polygon.shp')
        args['population_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/global_pop/w001001.adf')
        args['climatic_forcing_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/WaveWatchIII.shp')
        args['bathymetry_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/claybark_dem/hdr.adf')
        args['geomorphology_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/Geomorphology_BarkClay.shp')
        args['relief_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/claybark_dem/hdr.adf')
        args['habitats_csv_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/NaturalHabitat_WCVI.csv')
        args['habitats_directory_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/NaturalHabitat')
        args['surge_potential_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/claybark_dem/hdr.adf')
        args['continental_shelf_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/continentalShelf.shp')
        args['sea_level_rise_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/ExampleSLR.shp')
        args['additional_layer_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/ExampleSLR.shp')
        args['structures_uri'] = os.path.join(args['workspace_dir'], \
        u'../../coastal_vulnerability_test_data/Structures_BarkClay.shp')

        args['climatic_forcing_constant'] = "5"
        args['bathymetry_constant'] = "5"
        args['elevation_averaging_radius'] = 5000
        args['relief_constant'] = "5"
        args['geomorphology_constant'] = "5"
        args['habitats_constant'] = "5"
        args['surge_potential_constant'] = "5"
        args['sea_level_rise_constant'] = "5"
        args['additional_layer_constant'] = "5"
        args['structures_constant'] = "5"

        args['mean_sea_level_datum'] = 0
        args['depth_contour'] = 150
        args['cell_size'] = 250       # cell size in meters
        args['exposure_proportion'] = 0.8   # Used to differentiate between
                                            # sheltered and exposed shoreline
        args['max_fetch'] = 12000      # max fetch in meters
        args['rays_per_sector'] = 1    # number of rays in sectors
        args['H_threshold'] = 50000     # H threshold in km as specified in 
                                        # equ 4.8 in the user guide
        args['depth_threshold'] = 10
        args['spread_radius'] = 250    # spread in meters
        args['urban_center_threshold'] = 5000   # for the urban vulnerability 
                                                # histogram
        args['population_radius'] = 1000    # radius in meters within which the
                                            # population size is assessed.
        # Preliminary tests...
        assert(os.path.isdir(args['workspace_dir']))
        print('cwd', os.getcwd())
        print('aoi', args['aoi_uri'])
        assert(os.path.isfile(args['aoi_uri']))
        assert(os.path.isfile(args['landmass_uri']))
        if 'climatic_forcing_uri' in args:
            assert(os.path.isfile(args['climatic_forcing_uri']))
        if 'bathymetry_uri' in args:
            assert(os.path.isfile(args['bathymetry_uri']))
        if 'relief_uri' in args:
            assert(os.path.isfile(args['relief_uri']))
        if 'geomorphology_uri' in args:
            assert(os.path.isfile(args['geomorphology_uri']))
        if 'habitat_csv_uri' in args:
            assert(os.path.isfile(args['habitat_csv_uri']))
            assert(os.path.isdir(args['habitat_directory']))
        if 'surge_potential_uri' in args:
            assert(os.path.isfile(args['surge_potential_uri']))
        if 'sea_level_rise_uri' in args:
            assert(os.path.isfile(args['sea_level_rise_uri']))
        if 'global_population_uri' in args:
            assert(os.path.isfile(args['global_population_uri']))
        if 'structures_uri' in args:
            assert(os.path.isfile(args['structures_uri']))

        return args

    def setUp(self):
        """ Set up a test coastline on which to use the marching square
        algorithm 
        
            - inputs: none (hard-coded coastline, hard-coded ray directions)
            - output: an array of (type?) 2-tuples (direction, length)
        """
        print('Generating input data...')
        self.args = self.make_input_data()
        self.args['regression_data_directory'] = \
        os.path.join(self.args['workspace_dir'], \
        '../../coastal_vulnerability_regression_data').encode('utf-8')
        self.args['intermediate_directory'] = \
            os.path.join(self.args['workspace_dir'], \
            'intermediate').encode('utf-8')
        self.args['outputs_directory'] = \
            os.path.join(self.args['workspace_dir'], 'outputs').encode('utf-8')
        self.args['aoi_uri'] = self.args['aoi_uri'].encode('utf-8')
        self.args['shore_uri'] = \
            os.path.join(self.args['regression_data_directory'],
            'shore_of_interest_both.tif').encode('utf-8')
        assert(os.path.isfile(self.args['shore_uri']))

    def test_output_area(self):
        """ The user can specify the shore area over which to compute 
        the output: either over the sheltered segments, or over all the 
        segments. Testing that the area over which to compute the output is
        consistent with the option chosen by the user.
            
            Inputs:

            Outputs:
                None.
        """
        # Load the fetch regression data
        distances_uri = os.path.join(self.args['regression_data_directory'], \
            'distances.csv')
        depths_uri = os.path.join(self.args['regression_data_directory'], \
            'depths.csv')
        regression_distances = {}
        regression_depths = {}
        with open(distances_uri, 'rb') as distances:
            reader = csv.reader(distances)
            for entry in reader:
                key = tuple(map(float, entry[0][1:-1].split(',')))
                entry[1] = map(float,entry[1][1:-1].replace('\n','').split())
                value = np.array(entry[1])
                regression_distances[key] = value
        with open(depths_uri, 'rb') as depths:
            reader = csv.reader(depths)
            for entry in reader:
                key = tuple(map(float, entry[0][1:-1].split(',')))
                entry[1] = map(float,entry[1][1:-1].replace('\n','').split())
                value = np.array(entry[1])
                regression_depths[key] = value
        args = {}
        args['intermediate_directory'] = self.args['intermediate_directory']
        args['outputs_directory'] = self.args['outputs_directory']
        args['aoi_uri'] = self.args['aoi_uri']
        args['shore_uri'] = self.args['shore_uri']
        args['sector_count'] = self.args['sector_count']
        args['exposure_proportion'] = self.args['exposure_proportion']
        args['max_fetch'] = 20000
        args['cell_size'] = 250
        args['depth_threshold'] = 50
        args['fetch_distances'] = regression_distances
        args['fetch_depths'] = regression_depths
        args['prefix'] = 'test'
        # First option: compute over all the shoreline segments
        args['area_computed'] = 'both'
        # Run the computation        
        data_uri = cvc.compute_segment_exposure(args) 
        # Extract the computed and regression data for comparison
        computed_dataset = gdal.Open(data_uri['shore_uri'])
        regression_dataset=os.path.join(self.args['regression_data_directory'],\
            'shore_of_interest_both.tif')
        # Assert if computed and regression data differ
        invest_test_core.assertTwoDatasetEqualURI(self, computed_dataset, \
            regression_dataset)
        # Second option: compute over the sheltered shoreline segments only
        args['area_computed'] = 'sheltered'
        # Run the computation
        data_uri = cvc.compute_segment_exposure(args) 
        # Extract the computed and regression data for comparison
        computed_dataset = gdal.Open(data_uri['shore_uri'])
        regression_dataset=os.path.join(self.args['regression_data_directory'],\
            'shore_of_interest_sheltered.tif')
        # Assert if computed and regression data differ
        invest_test_core.assertTwoDatasetEqualURI(self, computed_dataset, \
            regression_dataset)

        assert(False)

    def test_compute_structure_protection(self):
        args = {}
        args['aoi_uri'] = self.args['aoi_uri'].encode('utf-8')
        args['shore_uri'] = self.args['shore_uri'].encode('utf-8')
        args['structures_uri'] = self.args['structures_uri'].encode('utf-8')
        args['cell_size'] = 250
        args['intermediate_directory'] = self.args['intermediate_directory']
        args['prefix'] = 'test'
        # Run the computation        
        data_uri = cvc.compute_structure_protection(args) 
        # Extract the computed and regression data for comparison
        computed_dataset = data_uri['structure_edges'].encode('utf-8')
        regression_dataset=os.path.join(self.args['regression_data_directory'],\
            'structure_edges.tif').encode('utf-8')
        # Assert if computed and regression data differ
        invest_test_core.assertTwoDatasetEqualURI(self, computed_dataset, \
            regression_dataset)

    def test_compute_segment_exposure(self):
        # Load the fetch regression data
        distances_uri = os.path.join(self.args['regression_data_directory'], \
            'distances.csv')
        depths_uri = os.path.join(self.args['regression_data_directory'], \
            'depths.csv')
        regression_distances = {}
        regression_depths = {}
        #with open(distances_uri, 'rb') as distances:
        #    reader = csv.reader(distances)
        #    for entry in reader:
        #        key = tuple(map(float, entry[0][1:-1].split(',')))
        #        entry[1] = map(float,entry[1][1:-1].replace('\n','').split())
        #        value = np.array(entry[1])
        #        regression_distances[key] = value
        #with open(depths_uri, 'rb') as depths:
        #    reader = csv.reader(depths)
        #    for entry in reader:
        #        key = tuple(map(float, entry[0][1:-1].split(',')))
        #        entry[1] = map(float,entry[1][1:-1].replace('\n','').split())
        #        value = np.array(entry[1])
        #        regression_depths[key] = value
        args = {}
        args['area_computed'] = self.args['area_computed']
        args['intermediate_directory'] = self.args['intermediate_directory']
        args['outputs_directory'] = self.args['outputs_directory']
        args['aoi_uri'] = self.args['aoi_uri']
        args['shore_uri'] = self.args['shore_uri']
        args['sector_count'] = self.args['sector_count']
        args['exposure_proportion'] = self.args['exposure_proportion']
        args['max_fetch'] = 20000
        args['cell_size'] = 250
        args['depth_threshold'] = 30
        args['prefix'] = 'test-'
        sector_range = range(args['sector_count'])
        sectors_rad = np.array(sector_range, dtype=float) *2 *math.pi / \
            args['sector_count']
        landmass_uri = os.path.join(self.args['regression_data_directory'], \
            'landmass.tif')
        land_raster = gdal.Open(landmass_uri)
        land_array = land_raster.GetRasterBand(1).ReadAsArray()
        shore_raster = gdal.Open(args['shore_uri'])
        shore_array = shore_raster.GetRasterBand(1).ReadAsArray()
        bathymetry_uri = os.path.join(self.args['regression_data_directory'], \
            'bathymetry.tif')
        bathymetry_raster = gdal.Open(bathymetry_uri)
        bathymetry_array = bathymetry_raster.GetRasterBand(1).ReadAsArray()
        regression_distances, regression_depths = \
            cvc.compute_fetch_fast(land_array, sectors_rad, \
            args['max_fetch']/args['cell_size'], shore_array, bathymetry_array)

        args['fetch_distances'] = regression_distances
        args['fetch_depths'] = regression_depths
        # Run the computation
        data_uri = cvc.compute_segment_exposure(args)
        # Extract the computed and regression data for comparison
        #computed_dataset = os.path.join(self.args['regression_data_directory'],\
        #    '1_a-shore_exposure.tif')
        computed_dataset = data_uri['shore_exposure']
        regression_dataset=os.path.join(self.args['regression_data_directory'],\
            'shore_exposure.tif')
        # Assert if computed and regression data differ
        invest_test_core.assertTwoDatasetEqualURI(self, computed_dataset, \
            regression_dataset)

    def validate_with_regression_data(regression_dir, data_dir):
        """ Compare regression files to their countreparts in data_dir.
            The algorithm tests all '.tif' and '.csv' files in regression_dir
            and compares them to their homologues in data_dir. Once all files
            are validated, the algorithm proceeds to the sub-directories in 
            a depth-first search.
            
            Inputs:
                -regression_dir: URI to the parent directory that contains the
                regression data. Only .tif and .csv files and subdirectories 
                in this directory will be checked in data_dir.
                -data_dir: the data directory against which the regression
                files should be tested against.
                
            Returns 'True' if all regression tests pass, False otherwise."""
        # Optimistic start
        test_pass = True
        # Find all the files and directories in the regression and data
        # directories
        regr_path, regr_dirs, regr_files = os.walk(regression_dir)
        data_path, data_dirs, data_files = os.walk(data_dir)

        # Check that all regression directories exist in the data to be 
        # tested
        regr_dirs_set = set(regr_dirs)
        data_dirs_set = set(data_dirs)
        if regr_dirs_set <= data_dirs_set:
            # Tell what files in data are missing
            message = \
                'Missing directories: ' + str(regr_dirs_set - data_dirs_set)
            LOGGER.warning(message)
            test_pass = False

        # Check that all files exist in the data to be tested
        regr_files_set = set(regr_files)
        data_files_set = set(data_files)
        if regr_files_set <= data_files_set:
            # Tell what files in data are missing
            message = \
                'Missing files: ' + str(regr_files_set - data_files_set)
            LOGGER.warning(message)
            test_pass = False
        
        # Separate .tif from .csv files 
        tif_regr_files = [f for f in regr_files if '.tif' in f]
        csv_regr_files = [f for f in regr_files if '.csv' in f]

        for tif_file in tif_regr_files:
            regr_uri = os.path.join(regr_path, tif_file)
            data_uri = os.path.join(data_path, tif_file)

        return test_pass

    def test_execute(self):
        base_wd = os.getcwd()
        cvc.preprocess_inputs(self.args)
        cvc.execute(self.args)
        # Compare output rasters with regression data
        self.args['regression_data_directory'] = \
            os.path.join(base_wd, self.args['regression_data_directory'])
        # Test intermediate files
        regression_entries = \
            os.path.join(self.args['regression_data_directory'],\
            'intermediate_regression_data')
        os.chdir(regression_entries)
        regression_files = glob.glob('*.tif')
        os.chdir(base_wd)
        os.chdir(self.args['intermediate_directory'])
        computed_files = glob.glob('*.tif')
        os.chdir(base_wd)
        for regression_file in regression_files:
            if regression_file in computed_files:
                print('found', regression_file)
                invest_test_core.assertTwoDatasetEqualURI(self, \
                    os.path.join(regression_entries, regression_file), \
                    os.path.join(self.args['intermediate_directory'], regression_file))
            else:
                assert regression_file in computed_files, \
                    'Missing output file %s' % regression_file
        # Test outputs files
        regression_entries = \
            os.path.join(self.args['regression_data_directory'],\
            'outputs_regression_data')
        os.chdir(regression_entries)
        regression_files = glob.glob('*.tif')
        os.chdir(base_wd)
        os.chdir(self.args['outputs_directory'])
        computed_files = glob.glob('*.tif')
        os.chdir(base_wd)
        for regression_file in regression_files:
            if regression_file in computed_files:
                print('found', regression_file)
                invest_test_core.assertTwoDatasetEqualURI(self, \
                    os.path.join(regression_entries, regression_file), \
                    os.path.join(self.args['outputs_directory'], regression_file))
            else:
                assert regression_file in computed_files, \
                    'Missing output file %s' % regression_file

    
    # TODO: use adjusted_ranks when it will be accessible...
    # TODO: test limit cases
    def test_adjust_layer_ranks(self):
        one_value = np.array([[5,5,0],[5,5,0]])
        adjusted_value = cvc.adjust_layer_ranks(one_value)
        unique_value = np.unique(adjusted_value)
        assert( (unique_value.size == 2) and \
                (unique_value[0] == 0) and \
                (unique_value[1] == 3))

        two_values = np.array([[3,1,0],[3,1,0]])
        adjusted_value = cvc.adjust_layer_ranks(two_values)
        unique_value = np.unique(adjusted_value)
        assert( (unique_value.size == 3) and \
                (unique_value[0] == 0) and \
                (unique_value[1] == 3) and \
                (unique_value[2] == 4))
    
        three_values = np.array([[5,1,0],[1,2,0]])
        adjusted_value = cvc.adjust_layer_ranks(three_values)
        unique_value = np.unique(adjusted_value)
        assert( (unique_value.size == 4) and \
                (unique_value[0] == 0) and \
                (unique_value[1] == 2) and \
                (unique_value[2] == 3) and \
                (unique_value[3] == 4))

        four_values = np.array([[1,2,0],[4,3,0]])
        adjusted_value = cvc.adjust_layer_ranks(four_values)
        unique_value = np.unique(adjusted_value)
        assert( (unique_value.size == 5) and \
                (unique_value[0] == 0) and \
                (unique_value[1] == 2) and \
                (unique_value[2] == 3) and \
                (unique_value[3] == 4) and \
                (unique_value[4] == 5))
        
        return True
        
       
    def test_combined_rank(self):
        def compute_combined_ranking(RankList):
            """Original equation that computes the combined habitat ranking.
                
                Input: RankList: a list of habitat ranks.
                
                Output: a combined habitat ranking. This equation is the verbose,
                unaltered python code used in the initial version of the model."""
            RankListMinus5 = [(5.0 - i) for i in RankList]
            SqRankList = [(i*i) for i in RankListMinus5]
            HabEq = \
            (1.5*max(RankListMinus5))**2+np.sum(SqRankList)-(max(RankListMinus5))**2
            rankValue = 4.8-0.5*((HabEq)**0.5)
            return rankValue

        # A few constants:
        kelp            = 4
        seagrass        = 4
        low_dune        = 3
        high_dune       = 2
        marsh           = 2
        coral           = 1
        mangrove        = 1
        coastal_forest  = 1 # If set to rank 3, we recover table in appendix B

        habitats = [[kelp],
                    [seagrass],
                    [seagrass, kelp],
                    [low_dune],
                    [coastal_forest],
                    [coastal_forest, kelp],
                    [low_dune, kelp],
                    [coastal_forest, seagrass],
                    [low_dune, seagrass],
                    [coastal_forest, kelp, seagrass],
                    [low_dune, kelp, seagrass],
                    [low_dune, coastal_forest],
                    [low_dune, coastal_forest, kelp],
                    [low_dune, coastal_forest, seagrass],
                    [low_dune, coastal_forest, kelp, seagrass],
                    [high_dune],
                    [marsh],
                    [high_dune, kelp],
                    [marsh, kelp],
                    [high_dune, seagrass],
                    [marsh, seagrass],
                    [high_dune, kelp, seagrass],
                    [high_dune, coastal_forest],
                    [high_dune, kelp, coastal_forest],
                    [marsh, coastal_forest, kelp],
                    [high_dune, seagrass, coastal_forest],
                    [marsh, coastal_forest, seagrass],
                    [high_dune, seagrass, kelp, coastal_forest],
                    [marsh, coastal_forest, kelp, seagrass],
                    [coral],
                    [mangrove],
                    [coral, seagrass],
                    [mangrove, seagrass],
                    [coral, coastal_forest],
                    [coral, low_dune],
                    [mangrove, seagrass, coastal_forest],
                    [coral, seagrass, coastal_forest],
                    [coral, seagrass, low_dune],
                    [coral, high_dune],
                    [coral, marsh],
                    [coral, seagrass, high_dune],
                    [coral, seagrass, marsh],
                    [coral, seagrass, low_dune, coastal_forest],
                    [mangrove, coral],
                    [coral, seagrass, high_dune, coastal_forest],
                    [mangrove, coral, seagrass],
                    [mangrove, coral, seagrass, coastal_forest]]
        

        ranking = np.array([compute_combined_ranking(np.array(habitats[i])) \
                            for i in range(len(habitats))])
        result = np.array([ cvc.combined_rank(np.array(habitats[i])) \
                            for i in range(len(habitats))])
        agreement = (result - ranking) < 0.001

        assert agreement.all()

    # TODO: load regression data from appropriate directory!!!!
    def test_detect_shore(self):
        """ test shore detection.
        
            inputs: none
            
            succeeds if the computed shore from the test raster is identical to
            the sample shore from the regression land file."""
        # Load regression shore data
        shore_raster_uri = os.path.join(self.args['regression_data_directory'], \
            'shore_of_interest_both.tif')
        ds = gdal.Open(shore_raster_uri)
        band = ds.GetRasterBand(1)
        regression_shore = band.ReadAsArray()
        # Load AOI
        aoi_raster_uri = os.path.join(self.args['regression_data_directory'], \
            'aoi.tif')
        aoi_raster = gdal.Open(aoi_raster_uri)
        aoi_band = aoi_raster.GetRasterBand(1)
        aoi_array = aoi_band.ReadAsArray()

        # No sea, no shore:
        no_sea = np.ones_like(regression_shore)
        shore = cvc.detect_shore(no_sea)
        self.assertEquals(np.sum(shore), 0)

        # No land, no shore:
        no_land = np.zeros_like(regression_shore)
        shore = cvc.detect_shore(no_sea)
        self.assertEquals(np.sum(shore), 0)

        # Cross-checking with a regression file:
        landmass_uri = os.path.join(self.args['regression_data_directory'], \
            'landmass.tif')
        landmass_raster = gdal.Open(landmass_uri)
        landmass_band = landmass_raster.GetRasterBand(1)
        landmass_array = landmass_band.ReadAsArray()
        landmass_array[aoi_array == 0] = cvc.nodata()
        shore = cvc.detect_shore(landmass_array)

        self.assertEquals(np.sum(np.fabs(shore - regression_shore)), 0)


    def test_fetch_vectors(self):
        """ Test the fetch direction according to Greg Guannel's explanation.
            Each fetch sector has an angle associated to it, but extracting 
            the vector direction in the raster array coordinates 
            is not trivial. The convention, in Greg's terms:
                -angle   0 degrees: the wind blows from north, towards south.
                -angle  90 degrees: the wind blows from east towards west.
                -angle 180 degrees: the wind blows from south towards north.
                -angle 270 degrees: the wind blows from west towards east.
                
            The fetch rays start from the shore and travel seaward (traveling
            TO), whereas the wind blows from the ocean towards the coast
            (blowing FROM). So here is the angle convention for the fetch rays:
                -angle   0 degrees: fetch rays travel from south towards north
                -angle  90 degrees: fetch rays travel from west to east
                -angle 180 degrees: fetch rays travel from north to south
                -angle 270 degrees: fetch rays travel from east to west
                
            The raster coordinates are indexed as [row, column] where the row 
            number increases going down, and the column index increases going
            right. For a raster where north is up, the vector coordinates that
            correspond to the compass rose are:
                -angle   0 degrees: north [-1,  0]
                -angle  90 degrees: east  [ 0,  1]
                -angle 180 degrees: south [ 1,  0]
                -angle 270 degrees: west  [ 0, -1]
            which is also the fetch directions."""
        angles = [0., math.pi / 2., math.pi, 3. * math.pi / 2.]
        result = np.array([[-1., 0.], [0., 1], [1., 0.], [0., -1.]])

        vectors = cvc.fetch_vectors(angles)

        self.assertEquals(np.sum(np.fabs(vectors - result)), 0.)

    def test_cast_ray(self):
        """ Test the ray-casting algorithm on an artificial test coastline
            
            inputs: none -- the coastline is generated on-the-fly
            
            Succeeds if the distances computed by the ray casting algorithm
                agree with the ones generated on-the-fly by the test.
            
            Algorithm summary:
                1- Picks random angles, distances, and stopping conditions
                    (max. distance reached or stop on land), and determine
                    where the ray casting algorithm is supposed to stop.
                2- Combine these conditions into a series of independent test
                    cases that are fed to cast_ray(), which returns a series of
                    computed distances.
                3- The computed distances are checked against the randomly 
                    generated ones to see if they agree with each other."""
        # Size of the test area 
        sides =100      # Size of the test array in cells (sides * sides)
        border =2       # Number of land cells around the perimeter

        coastline = np.ones((sides, sides), dtype=np.int) * cvc.land()
        coastline[border:(sides-border), border:(sides-border)] = cvc.sea()
        bathymetry = np.ones_like(coastline)

        # Origin is at sea, in the middle of the test area
        origin = np.array([ coastline.shape[0]/2, coastline.shape[1]/2])
        # The rays will radiate from the origin up to a maximum radius
        radius = min(coastline.shape)/4

        # Number of tests
        test_count = 10000

        # Pick random angles, distances, and stop conditions
        angle = np.array([random.random()*2.0*math.pi 
                            for x in range(test_count)])
        
        distance = np.array([random.random()*(radius-2.0) +2.0 
                            for i in range(test_count)])

        should_stop_on_land = np.array([random.choice((True, False))
                            for i in range(test_count)])

        # The angle and distance point to a location which coordinates are
        # discreet. The distance has to be recomputed to accurately reflect the
        # discreet coordinates.
        i = np.array(np.cos(angle)*distance).astype(int)
        j = np.array(np.sin(angle)*distance).astype(int)

        test_distances = np.sqrt(np.square(i) + np.square(j))
        
        # direction vectors used for the test
        direction = np.array([i, j]).T.astype(float)

        # Used to clear the land cell that triggers the stop_on_land test
        # end condition
        old_raster_val = 0

        # conpute distances using cast_ray
        computed_distances = np.empty_like(distance)
        average_depth = np.empty_like(distance)

        for i in range(direction.shape[0]):
            assert(max(np.fabs(direction[i]))) # Zero vectors not allowed
            # If should stop on land, insert land and double maximum distance
            test_distance = test_distances[i]

            if should_stop_on_land[i]:
                test_distance *= 2.0
                step = direction[i] / max(np.fabs(direction[i])) # Move land...
                land = np.around(origin + direction[i] + step)   # ...one cell further
                old_raster_val = coastline[land[0]][land[1]]
                coastline[land[0]][land[1]] = 1
            
            computed_distances[i], average_depth[i] = \
                cvc.cast_ray(origin, direction[i], test_distance, coastline, \
                bathymetry)

            if should_stop_on_land[i]:\
                coastline[land[0]][land[1]] = old_raster_val
 
        # Compute the discrepancies between test and computed distances
        discrepancies = test_distances - computed_distances
        error_indices = \
            np.where(np.fabs(discrepancies)>0.000000001)[0]
       
        self.assertEquals(len(error_indices), 0)

    def test_compute_fetch(self):
        """ Test fetch computation with random directions, max distances and
        termination conditions.
        
            args['fetch_test_raster']: raster filename used to test the
                fetch algorithm.
            args['fetch_expected_result']: comma-separated filename that
                contains the expected result of the fetch calculation.
                
            succeeds if computing fetch over 'fetch_test_raster' yields the
                same result as in 'fetch_expected_result', fails otherwise."""
        land_rasters = {}
        shore_rasters = {}
        bathymetries = {}
        solutions = {}
        distances = {}
        LOGGER.debug('Testing slow fetch algorithm distances against \
            hand-designed shoreline')
        # --------------------------------------------------------------------
        # Simple test cases that can be computed by hand:
        #   1- Minimalistic: only 1 shore point, 0 fetch distances
        #   2- Non-square land raster, variable fetch distances (0 or 1)
        #   3- Variable fetch distances beyond 1
        #   4- Minimalistic island (1 point)
        #   5- Small non-trivial island (L-shaped)
        # --------------------------------------------------------------------
        d_max = 4 
        # The 4 directions in order are: left, down, right, up.
        direction_count = 4 
        
        # adjust the directions to be between 0 and 2*PI
        directions  = np.array(range(direction_count)).astype(float) *\
                        2.0 * math.pi / float(direction_count)

        #   1- Minimalistic: only 1 shore point, 0 fetch distance
        # Land is 1, water is 0
        land_rasters[0] = np.array([[1, 1, 1],\
                                    [1, 0, 1],\
                                    [1, 1, 1]]).astype('float')
        # Shore is water bordered by a land cell, encoded as 1. The rest is 0.
        # In this simple example, the shore is the only cell with 0, the rest
        # is not, so the shore raster is the logical inverse of land_raster
        shore_rasters[0] = (land_rasters[0] == 0).astype('float')
        # Bathymetry is set to zero, since we don't use this information yet
        bathymetries[0] = np.zeros_like(land_rasters[0])
        # Expected solution:
        solutions[0] = {(1, 1):[0.5, 0.5, 0.5, 0.5]}
        # compute the fetch and test equality against solution
        distances[0] = cvc.compute_fetch_fast(land_rasters[0], directions, d_max, 
            shore_rasters[0], bathymetries[0])[0]
        # Test fetch against solution
        entries_equal = [(distances[0][key]==solutions[0][key]).all() 
            for key in distances[0].keys()]
        self.assertEquals(all(entries_equal), True)

        #   2- Non-square land raster, variable fetch distances (0 or 1)
        land_rasters[1] = np.array([[1, 1, 1, 1],\
                                    [1, 0, 0, 1],\
                                    [1, 1, 1, 1]]).astype('float')
        shore_rasters[1] = (land_rasters[1] == 0).astype('float')
        bathymetries[1] = np.zeros_like(land_rasters[1])
        solutions[1] = {(1, 1): [0.5, 0.5, 1.5, 0.5],
                        (1, 2): [1.5, 0.5, 0.5, 0.5]}
        distances[1] = cvc.compute_fetch_fast(land_rasters[1], directions, d_max, 
            shore_rasters[1], bathymetries[1])[0]
        entries_equal = [(distances[1][key]==solutions[1][key]).all() 
            for key in distances[1].keys()]
        self.assertEquals(all(entries_equal), True)
        
        #   3- Variable fetch distances beyond 1
        land_rasters[2] = np.array([[1, 1, 1, 1, 1, 1],\
            [1, 0, 0, 0, 0, 1],\
            [1, 0, 0, 0, 0, 1],\
            [1, 1, 1, 1, 1, 1]]).astype('float')
        shore_rasters[2] = (land_rasters[2] == 0).astype('float')
        bathymetries[2] = np.zeros_like(land_rasters[2])
        solutions[2] = {(1, 1): [0.5, 1.5, 3.5, 0.5],
            (1, 2): [1.5, 1.5, 2.5, 0.5],
            (1, 3): [2.5, 1.5, 1.5, 0.5],
            (1, 4): [3.5, 1.5, 0.5, 0.5],
            (2, 1): [0.5, 0.5, 3.5, 1.5],
            (2, 2): [1.5, 0.5, 2.5, 1.5],
            (2, 3): [2.5, 0.5, 1.5, 1.5],
            (2, 4): [3.5, 0.5, 0.5, 1.5]}
        distances[2] = cvc.compute_fetch_fast(land_rasters[2], directions, d_max, 
            shore_rasters[2], bathymetries[2])[0]
        entries_equal = [(distances[2][key]==solutions[2][key]).all() 
            for key in distances[2].keys()]
        self.assertEquals(all(entries_equal), True)

        #   4- Minimalistic island (1 point)
        land_rasters[3] = np.array([[0, 0, 0, 0, 0, 0],\
            [0, 0, 0, 0, 0, 0],\
            [0, 1, 0, 0, 0, 0],\
            [0, 0, 0, 0, 0, 0],\
            [0, 0, 0, 0, 0, 0],\
            [0, 0, 0, 0, 0, 0]]).astype('float')
        shore_rasters[3] = np.array([[0, 0, 0, 0, 0, 0],\
            [1, 1, 1, 0, 0, 0],\
            [1, 0, 1, 0, 0, 0],\
            [1, 1, 1, 0, 0, 0],\
            [0, 0, 0, 0, 0, 0],\
            [0, 0, 0, 0, 0, 0]]).astype('float')
        bathymetries[3] = np.zeros_like(land_rasters[3])
        solutions[3] = {(1, 0): [0.5, 4.0, 4.0, 1.5],
            (1, 1): [1.5, 0.5, 4.0, 1.5],
            (1, 2): [2.5, 4.0, 3.5, 1.5],
            (2, 0): [0.5, 3.5, 0.5, 2.5],
            (2, 2): [0.5, 3.5, 3.5, 2.5],
            (3, 0): [0.5, 2.5, 4.0, 3.5],
            (3, 1): [1.5, 2.5, 4.0, 0.5],
            (3, 2): [2.5, 2.5, 3.5, 3.5]}
        distances[3] = cvc.compute_fetch_fast(land_rasters[3], directions, d_max, 
            shore_rasters[3], bathymetries[3])[0]
        entries_equal = [(distances[3][key]==solutions[3][key]).all() 
            for key in distances[3].keys()]
        self.assertEquals(all(entries_equal), True)

        #   5- Small non-trivial island (L-shaped)
        land_rasters[4] = np.array([[0, 0, 0, 0, 0, 0],\
            [0, 0, 0, 0, 0, 0],\
            [0, 0, 1, 0, 0, 0],\
            [0, 0, 1, 1, 0, 0],\
            [0, 0, 0, 0, 0, 0],\
            [0, 0, 0, 0, 0, 0]]).astype('float')
        shore_rasters[4] = np.array([[0, 0, 0, 0, 0, 0],\
            [0, 1, 1, 1, 0, 0],\
            [0, 1, 0, 1, 1, 0],\
            [0, 1, 0, 0, 1, 0],\
            [0, 1, 1, 1, 1, 0],\
            [0, 0, 0, 0, 0, 0]]).astype('float')
        bathymetries[4] = np.zeros_like(land_rasters[4])
        solutions[4] = {(1, 1): [1.5, 4.0, 4.0, 1.5],
            (1, 2): [2.5, 0.5, 3.5, 1.5],
            (1, 3): [3.5, 1.5, 2.5, 1.5],
            (2, 1): [1.5, 3.5, 0.5, 2.5],
            (2, 3): [0.5, 0.5, 2.5, 2.5],
            (2, 4): [1.5, 3.5, 1.5, 2.5],
            (3, 1): [1.5, 2.5, 0.5, 3.5],
            (3, 4): [0.5, 2.5, 1.5, 3.5],
            (4, 1): [1.5, 1.5, 4.0, 4.0],
            (4, 2): [2.5, 1.5, 3.5, 0.5],
            (4, 3): [3.5, 1.5, 2.5, 0.5],
            (4, 4): [4.0, 1.5, 1.5, 4.0]}
        distances[4] = cvc.compute_fetch_fast(land_rasters[4], directions, d_max, 
            shore_rasters[4], bathymetries[4])[0]
        entries_equal = [(distances[4][key]==solutions[4][key]).all() 
            for key in distances[4].keys()]
        self.assertEquals(all(entries_equal), True)
        # -------------------------------------------------------------------- 
        LOGGER.debug('Testing fast fetch algorithm distances against \
            hand-designed shoreline')
        for test in range(len(land_rasters)):
            fetch = cvc.compute_fetch_fast(land_rasters[test], directions,
                d_max, shore_rasters[test], bathymetries[test])
            entries_equal = [(distances[test][key]==fetch[0][key]).all() 
                for key in distances[test].keys()]
            if not all(entries_equal):
                print('different for test', test)
                for point in distances[test]:
                    print(point, 'slow', distances[test][point],
                        'fast', fetch[0][point])
            self.assertEquals(all(entries_equal), True)
        # -------------------------------------------------------------------- 
        LOGGER.debug('Testing depths and distances algorithm against \
            regression data')
        # Load the regression data
        regression_data_path = 'data/coastal_vulnerability_regression_data'

        landmass_uri = os.path.join(regression_data_path, 'landmass.tif')
        bathymetry_uri = os.path.join(regression_data_path, 'bathymetry.tif')
        shore_uri = os.path.join(regression_data_path, 'shore_of_interest_both.tif')

        landmass_dataset = gdal.Open(landmass_uri)
        bathymetry_dataset = gdal.Open(bathymetry_uri)
        shore_dataset = gdal.Open(shore_uri)

        landmass_array = landmass_dataset.GetRasterBand(1).ReadAsArray()
        bathymetry_array = bathymetry_dataset.GetRasterBand(1).ReadAsArray()
        shore_array = shore_dataset.GetRasterBand(1).ReadAsArray()
        # adjust the directions to be between 0 and 2*PI
        d_max = 10
        direction_count = 16
        directions  = np.array(range(direction_count)).astype(float) *\
                        2.0 * math.pi / float(direction_count)
        # compute the fetch
        fast_fetch = cvc.compute_fetch_fast(landmass_array, directions, d_max, 
            shore_array, bathymetry_array)
        distances_uri = os.path.join(regression_data_path, 'distances.csv')
        depths_uri = os.path.join(regression_data_path, 'depths.csv')
        fast_distances = fast_fetch[0]
        fast_depths = fast_fetch[1]
        # Load the fetch regression files
        regression_distances = {}
        regression_depths = {}

        # Loading regression data
        with open(distances_uri, 'rb') as distances:
            reader = csv.reader(distances)
            for entry in reader:
                key = tuple(map(float, entry[0][1:-1].split(',')))
                entry[1] = map(float,entry[1][1:-1].replace('\n','').split())
                value = np.array(entry[1])
                regression_distances[key] = value
        with open(depths_uri, 'rb') as depths:
            reader = csv.reader(depths)
            for entry in reader:
                key = tuple(map(float, entry[0][1:-1].split(',')))
                entry[1] = map(float,entry[1][1:-1].replace('\n','').split())
                value = np.array(entry[1])
                regression_depths[key] = value
        print('regression data', len(regression_distances))
        # Test equality between computed and regression data
        fast_distance_difference = []
        fast_depth_difference = []
        for key in fast_fetch[0].keys():
            segment_difference = \
                np.fabs(fast_fetch[0][key]-regression_distances[key]) < 1.e-13
            fast_distance_difference.append(segment_difference.all())
            if not fast_distance_difference[-1]:
                print('regression and fast distances disagree for', key)
                print('is_equal', segment_difference)
                print('difference', fast_fetch[0][key]-regression_distances[key])
                print('computed', fast_fetch[0][key])
                print('loaded', regression_distances[key])
        for key in fast_fetch[1].keys():
            segment_difference = \
                np.fabs(fast_fetch[1][key]-regression_depths[key]) < 1.0e-13
            fast_depth_difference.append(segment_difference.all())
            if not fast_depth_difference[-1]:
                print('regression and fast depths disagree for', key)
                print('is_equal', segment_difference)
                print('difference', fast_fetch[1][key]-regression_depths[key])
                print('computed', fast_fetch[1][key])
                print('loaded', regression_depths[key])
        self.assertEquals(all(fast_distance_difference), True)
        self.assertEquals(all(fast_depth_difference), True)

        return True 

    def tare_down(self):
        """ Clean up code."""
        # Do nothing for now 
        pass
