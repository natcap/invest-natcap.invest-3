""" Coastal vulnerability model core functions """
import os
import sys
import csv
import re
import math
import time 
import numpy as np
import scipy as sp
from scipy.interpolate import LinearNDInterpolator as ip
from scipy.ndimage import morphology

from osgeo import gdal
from osgeo import ogr
from osgeo import osr

import logging
from invest_natcap import raster_utils

class NotAtSea(Exception): 
    """ Exception raised by cast_ray when the point for which to compute the
    fetch is not at sea """
    pass

class NotProjected(Exception):
    """ Exception raised if trying to work on an object that needs to be
    projected """
    pass

LOGGER = logging.getLogger('coastal_vulnerability_core')
logging.basicConfig(format='%(asctime)s %(name)-15s %(levelname)-8s \
    %(message)s', level=logging.DEBUG, datefmt='%m/%d/%Y %H:%M:%S ')

def execute(args):
    """ Entry point for coastal vulnerability core
    
        For lots of debug output and data storage code, 
        check out revision 4318:705d54ae6c37

        args['foo'] - actual data structure the way I want them look like


        returns nothing"""

    pass

def sea():
    """ Return the code for a sea cell """
    return 0

def land():
    """ Return the code for a land cell """
    return 1

def nodata():
    """ Return the code for a nodata cell """
    return -1.0

def shore():
    """ Return the code for a shore cell """
    return 1

def is_sea(position, raster):
    """ Determine whether the cell is at sea or not.

        - position: current cell position
        - raster: array used to determine the cell value
                
        Returns True if the cell is at sea, False otrherwise."""
    return (raster[position[0], position[1]] == sea())

def is_land(position, raster):
    """ Determine whether the cell is on land or not. 

        - position: current cell position
        - raster: raster where the cell value is stored
                
        Returns True if the cell is at sea, False otrherwise."""
    return (raster[position[0], position[1]] == land())


# TODO: write a unit test for this function
GLOBAL_COUNTER = 0   # global value used to keep track of counts
def unique_value():
    """Return a unique integer that can be used as a chronological indicator.
        
        Input: nothing.
        
        Output: an integer extracted from time.clock(), modified if identical
            to the previous value."""
    # We're going to use this value to keep track of our unique value
    global GLOBAL_COUNTER
    # Given a .01s resolution, we assume we won't create more than 10^6 files/s
    current_value = int(time.clock()) * 10000
    # If more than 1 file is created within the time resolution, increment
    if current_value == GLOBAL_COUNTER: current_value += 1
    # Update global counter, so that we don't return the same value next time
    GLOBAL_COUNTER = current_value
    return current_value

# TODO: comment this
def set_raster_uri_dictionary(raster_uri_dict):
    def array_from_raster_uri(key):
        raster_uri = raster_uri_dict[key]
        dataset = gdal.Open(raster_uri)
        return dataset.GetRasterBand(1).ReadAsArray()
    return array_from_raster_uri

# TODO: comment this
def product(matrix_list):
    result = matrix_list[0]
    for n in range(len(matrix_list) -1):
        result *= matrix_list[n+1]
    return result

def compute_vulnerability_index(args):
    """ Main function that computes a coastal vulnerability index.
    
        returns something that is to be determined."""
    # Set some constants first:
    layer_names = ['surge_potential', \
                   'sea_level_rise', \
                   'habitats', \
                   'geomorphology', \
                   'relief', \
                   'REI', \
                   'E_w']
    # Set to save in 'outputs_directory'
    outputs_directory = args['outputs_directory']
    save_output=set_save_info(outputs_directory,args['aoi'],args['cell_size'])
    
    # Compute all the necessary data and store the result in a dictionary that
    # holds the URI of all the intermediate files
    print('computing surge potential...')
    data_uri = compute_surge_potential(args)
    print('computing sea level rise...')
    data_uri = dict(data_uri.items() + compute_sea_level_rise(args).items())
    print('computing natural habitats...')
    data_uri = dict(data_uri.items() + \
        compute_natural_habitats_vulnerability(args).items())
    print('computing geomorphology...')
    data_uri = dict(data_uri.items() + compute_geomorphology(args).items())
    print('computing relief rank...')
    data_uri = dict(data_uri.items() + compute_relief_rank(args).items())
    print('computing wind exposure...')
    data_uri = dict(data_uri.items() + compute_wind_exposure(args).items())
    print('computing wave exposure...')
    data_uri = dict(data_uri.items() + compute_wave_exposure(args).items())

    n = {}  # List of available layers, as in the supplementary notes.
    # Build 'n', the list of available layers
    for layer in layer_names:
        if layer in data_uri:
            n[layer] = data_uri[layer]

    # Equation (2) in the supplementary notes.
    R = set_raster_uri_dictionary(data_uri)
    # Try using a generator expression
    R_segments = np.power(product([R(k) for k in n]), 1./len(n))
    
    data_uri['R'] = save_output(R_segments, 'R.tif')
    
    return R_segments

def compute_coastal_population(args):
    """Compute population living along the shore within a given radius.
    
        Inputs:
            - args['aoi']: area of interest within which to save the data
            - args['shore']: coastline within the AOI
            - args['cell_size']: size of a pixel in meters
            - args['intermediate_directory']:where intermediate files are stored
            - args['global_population']: global population density raster.
            - args['population_radius']: used to compute the population density.

        Outputs:
            - Return a uri dictionary of all the files created to generate the
              population density along the coastline

        Intermediate outputs:
            - resampled_global_population.tif: generated only if the original
                global population file has a different resolution than cell_size
            - clipped_global_population.tif: raster data clipped to the AOI
            - pop_density.tif: raster where each point indicates the total 
                population living within 'population_radius' from that point
            - coastal_population.tif: same as pop_density, but only for points
                along the shore."""
    aoi = args['aoi']
    shore = args['shore']
    cell_size = args['cell_size']
    intermediate_directory = args['intermediate_directory']
    global_population = args['global_population']
    population_radius = args['population_radius']
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 

    # TODO: replace this with adjust_shapefile_to_aoi
    # The aoi should be projected
    srs = osr.SpatialReference()
    srs.ImportFromWkt(shapefile_wkt_projection(aoi))
    assert(srs.IsProjected())
    raster = global_population
    # Try to resample the raster to cell_size
    srs = osr.SpatialReference()
    srs.ImportFromWkt(raster_wkt_projection(raster))
    if srs.IsProjected():
        pixel_size = raster_utils.pixel_size(raster)
        if not pixel_size == cell_size:
            data_uri['resampled_global_population'] = \
            os.path.join(intermediate_directory, \
            'resampled_global_population.tif')
            raster=raster_utils.reproject_dataset(raster,\
            cell_size, raster.GetProjection(), \
            data_uri['resampled_global_population'])
    # Test that the projections are identical
    projections = [raster_wkt_projection(raster), 
                   shapefile_wkt_projection(aoi)]
    projected_aoi = aoi
    print('projections match:', projections_match(projections))
    if not projections_match(projections):
        raster_wkt = raster_wkt_projection(raster)
        data_uri['global_population_reprojected_aoi'] = \
            os.path.join(intermediate_directory, \
            'global_population_reprojected_aoi.shp')
        projected_aoi = raster_utils.reproject_datasource(aoi, raster_wkt, \
            data_uri['global_population_reprojected_aoi'])
    # Clip the raster to the AOI
    data_uri['clipped_global_population'] = \
        os.path.join(intermediate_directory,'clipped_global_population.tif')
    raster = raster_utils.clip_dataset(raster, projected_aoi, \
        data_uri['clipped_global_population'])
    # Resample or reproject population raster back to AOI's projection.
    pixel_size = raster_utils.pixel_size(raster)
    if not projections_match(projections): 
        data_uri['reprojected_global_population'] = \
        os.path.join(intermediate_directory,'reprojected_global_population.tif')
        aoi_wkt = shapefile_wkt_projection(aoi)
        raster = raster_utils.reproject_dataset(raster, cell_size, aoi_wkt, \
            data_uri['reprojected_global_population'], gdal.GDT_Int32)
    elif not pixel_size == cell_size:
        data_uri['resampled_global_population'] = \
        os.path.join(intermediate_directory,'resampled_global_population.tif')
        aoi_wkt = shapefile_wkt_projection(aoi)
        raster = raster_utils.reproject_dataset(raster, cell_size, aoi_wkt, \
            data_uri['resampled_global_population'])
    # Since the dataset size might have changed, adjust its size
    data_uri['clipped_population'] = \
        os.path.join(intermediate_directory, 'clipped_population.tif')
    clipped_pop = raster_utils.clip_dataset(raster, aoi, \
        data_uri['clipped_population'])
    aoi_raster = raster_from_shapefile(aoi, aoi, cell_size)
    data_uri['resized_population'] = \
        os.path.join(intermediate_directory, 'resized_population.tif')
    resized_pop = raster_utils.vectorize_rasters([clipped_pop, aoi_raster], \
        lambda x,y: x*y, aoi, data_uri['resized_population'], \
        gdal.GDT_Float32, no_data)
    print('raster size:', raster.GetRasterBand(1).ReadAsArray().shape)
    print('resized_pop size:', \
        resized_pop.GetRasterBand(1).ReadAsArray().shape)
    no_data = raster_utils.extract_band_and_nodata(aoi_raster)[1]
    print('aoi_raster size:', \
        aoi_raster.GetRasterBand(1).ReadAsArray().shape, \
        'nodata:', no_data)

    # Create a kernel to gather the total population within a given radius
    R = (population_radius/cell_size) if population_radius >= cell_size else 1. 
    kernel = disc_kernel(R)
    # Convolve the population with the kernel
    pop_array = resized_pop.GetRasterBand(1).ReadAsArray()
    pop_density = sp.signal.convolve2d(pop_array, kernel,mode='same')
    data_uri['pop_density'] = save_intermediate(pop_density, "pop_density.tif")
    pop_density = gdal.Open(data_uri['pop_density'])
    # Compute the population along the coast
    data_uri['coastal_population'] = \
        os.path.join(intermediate_directory, 'coastal_population.tif')
    no_data = raster_utils.calculate_value_not_in_dataset(pop_density)
    print('pop_array:', pop_array.shape)
    print('pop_density:', pop_density.GetRasterBand(1).ReadAsArray().shape)
    print('coastal_pop size:', coastal_pop.GetRasterBand(1).ReadAsArray().shape)
    # Done
    return data_uri
   
def compute_surge_potential(args):
    """Compute surge potential index as described in the user manual.
    
        Inputs:
            - args['DEM']: bathymetry data.
            - args['landmass']: shapefile containing land coverage data
            - args['aoi']:the area of interest over which to compute the measure
            - args['shore']: the shoreline (land = 1, sea = 0).
            - args['cell_size']: granularity of the rasterization in meters
            - args['intermediate_directory']: where intermediate files are
                stored

        Output:
            - Return R_surge as described in the user guide.
            
        Intermediate outputs:
            - rasterized_sea_level_rise.tif:rasterized version of the shapefile
            - shore_level_rise.tif: sea level rise along the shore.
            - sea_level_rise.tif: sea level rise index along the shore."""
    DEM = args['DEM']
    landmass = args['landmass']
    aoi = args['aoi']
    shore = args['shore']
    cell_size = args['cell_size']
    intermediate_directory = args['intermediate_directory']
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    
    # Test that the projections are identical
    # TODO: replace this with adjust_shapefile_to_aoi
    projections = [raster_wkt_projection(DEM), shapefile_wkt_projection(aoi)]
    assert(projections_match(projections))
    # Extract pixel size
    pixel_size = pixel_size_within_aoi(aoi, DEM)
    # Resample the DEM to cell_size
    resampled_DEM_uri=os.path.join(intermediate_directory,'resampled_DEM.tif')
    data_uri['resampled_DEM'] = resampled_DEM_uri
    DEM = raster_utils.reproject_dataset(DEM, cell_size, \
        DEM.GetProjection(), resampled_DEM_uri)
    # Clip the DEM to the AOI
    data_uri['clipped_DEM'] = \
        os.path.join(intermediate_directory,'clipped_DEM.tif')
    DEM = \
        raster_utils.clip_dataset(DEM, aoi, data_uri['clipped_DEM'])
   
    #clipped_DEM =adjust_raster_to_aoi(DEM,aoi,cell_size,intermediate_directory)
    # Get land coverage from land polygon 
    land_coverage = raster_from_shapefile(landmass, aoi, cell_size)
    land_coverage_array = land_coverage.GetRasterBand(1).ReadAsArray()
    data_uri['land_coverage'] = \
        save_intermediate(land_coverage_array, 'land_coverage.tif')
    # Get the land elevation and land coverage from the DEM.
    land_elevation_uri=os.path.join(intermediate_directory,'land_elevation.tif')
    land_elevation = \
        combine_rasters(land_coverage, DEM, land_elevation_uri)
    data_uri['land_elevation'] = land_elevation_uri
    # Compute the edge of the continental shelf
    continental_shelf = land_elevation.GetRasterBand(1).ReadAsArray()
    continental_shelf[continental_shelf >= -150] = 1 # on the shelf
    continental_shelf[continental_shelf  < -150] = 0 # off the shelf
    data_uri['continental_shelf'] = \
        save_intermediate(continental_shelf, 'continental_shelf.tif')
    # Compute the distances on the shelf from the edge
    distance_to_shelf = morphology.distance_transform_edt(continental_shelf)
    data_uri['distance_to_shelf'] = \
        save_intermediate(distance_to_shelf, 'distance_to_shelf.tif')
    # Compute the surge along the shore in meters
    shore_shelf_distance = distance_to_shelf * cell_size
    data_uri['shore_shelf_distance'] = \
        save_intermediate(shore_shelf_distance, 'shore_shelf_distance.tif')
    # Use percentiles to assign vulnerability index
    R_surge = shore_shelf_distance / np.amax(shore_shelf_distance)
    R_surge[R_surge < 0.01] = 1
    R_surge[R_surge < 0.25] = 2
    R_surge[R_surge < 0.75] = 3
    R_surge[R_surge < 0.90] = 4
    R_surge[R_surge < 1.00] = 5
    R_surge = R_surge * shore
    data_uri['surge_potential'] = \
        save_intermediate(R_surge, 'surge_potential.tif')
    # done
    return data_uri

# TODO: handle sea level rise and rate differently! But first, I need this data
def compute_sea_level_rise(args):
    """Compute the sea level rise index as described in the user manual.
    
        Inputs:
            - sea_level_rise: shapefile with the sea level rise data.
            - aoi: the area of interest over which to compute the measure.
            - shore: the shoreline (land = 1, sea = 0).
            - cell_size: granularity of the rasterization in meters
            - args['intermediate_directory']: where intermediate files are
                stored

        Output:
            - Return a dictionary of all the intermediate file URIs.

        Intermediate outputs:
            - rasterized_sea_level_rise.tif:rasterized version of the shapefile
            - shore_level_rise.tif: sea level rise along the shore.
            - sea_level_rise.tif: sea level rise index along the shore. If all 
                the shore has the same value, assign the moderate index value 3.
            """
    sea_level_rise = args['sea_level_rise']
    aoi = args['aoi']
    shore = args['shore']
    cell_size = args['cell_size']
    intermediate_directory = args['intermediate_directory']
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    # Reproject if needed
    aoi_wkt = shapefile_wkt_projection(aoi)
    slr_wkt = shapefile_wkt_projection(sea_level_rise)
    if not projections_match([aoi_wkt, slr_wkt]):
        sea_level_rise=raster_utils.reproject_datasource(sea_level_rise, \
            aoi_wkt, os.path.join(intermediate_directory, \
            'reprojected_sea_level_rise.shp'))
    # Extract the array from the shapefile
    sea_rise_array = \
        array_from_shapefile(sea_level_rise, aoi, cell_size, field='RANK')
    data_uri['rasterized_sea_level_rise'] = \
        save_intermediate(sea_rise_array, 'rasterized_sea_level_rise.tif')
    # Compute the sea level rise along the shore
    R_slr = sea_rise_array * shore
    data_uri['shore_level_rise'] = \
        save_intermediate(R_slr, 'shore_level_rise.tif')
    # Convert the rise to a rank
    shore_indices = R_slr > 0.                 # useful shorthand
    shore_min = np.amin(R_slr[shore_indices])  # min shore value
    shore_max = np.amax(R_slr[shore_indices])  # max shore value
    amplitude = shore_max - shore_min                   # normalization factor
    # Amplitude is zero: set shore to moderate index 3
    if amplitude == 0:
        R_slr[R_slr > 0.] = 3
    # Otherwise, shift to zero, normalize to 1 and use percentiles
    else:
        R_slr[shore_indices] = (R_slr[shore_indices] - shore_min) / amplitude
        R_slr[R_slr <= 0.1] = 5
        R_slr[R_slr <= 0.25] = 4
        R_slr[R_slr <= 0.75] = 3
        R_slr[R_slr <= 0.9] = 2
        R_slr[R_slr <= 1.0] = 1
        R_slr = R_slr * shore
    data_uri['sea_level_rise'] = \
        save_intermediate(R_slr, 'sea_level_rise.tif')
    # Done
    return data_uri

def compute_natural_habitats_vulnerability(args):
    """Compute the natural habitat rank as described in the user manual.
    
        Inputs:
            - habitats_csv: list of habitats.
            - habitats_dir: directory where to find habitat shapefiles.
            - aoi: the area of interest over which to compute the measure.
            - cell_size: granularity of the rasterization in meters
            - args['intermediate_directory']: where intermediate files are
                stored

        Output:
            - Return a dictionary of all the intermediate file URIs.
            
        Intermediate outputs:
            - For each habitat (habitat name 'ABCD', with id 'X') shapefile:
                - ABCD_X_raster.tif: rasterized shapefile data.
                - ABCD_influence.tif: habitat area of influence. Convolution
                  between the rasterized shape data and a circular kernel of
                    radius the habitat's area of influence, TRUNCATED TO
                    CELL_SIZE!!!
                - ABCD_influence_on_shore.tif: habitat influence along the shore
            - habitats_available_data.tif: combined habitat rank along the
                shore using equation 4.4 in the user guide.
            - habitats_missing_data.tif: shore section without habitat data.
            - habitats.tif: shore ranking using habitat and default ranks."""
    habitats_csv = args['habitat_csv']
    habitats_dir = args['habitat_directory']
    aoi = args['aoi']
    shore = args['shore']
    cell_size = args['cell_size']
    intermediate_directory = args['intermediate_directory']
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    # Predefined constants
    NAME                = 0 # Habitat name (string)
    ID                  = 1 # Habitat ID (unique int identifier)
    RANK                = 2 # Habitat rank (int, 1 to 5)
    PROTECTION_DISTANCE = 3 # Habitat protection distance in meters (int)
    FILE_NAME           = 4 # Name of the habitat shapefile
    ATTRIBUTE_COUNT     = 5 # Total number of attributes
    extension = '.shp'      # Shapefile extension to look for
    # Read the CSV file
    csv_reader = csv.reader(open(habitats_csv)) 
    # Load CSV data
    csv_data = {}
    for item in csv_reader:
        if item[ID] != 'ID':            # Skip header information
            csv_data[item[ID]] = item
    # Retreive shapefiles by extension
    file_list = os.listdir(habitats_dir)
    for entry in file_list:
        # Look for files with '.shp' extension...
        # ...AND '.shp' only: skip extentions like '.shp.xml'
        if extension in entry and \
            (len(entry) == entry.find(extension)+len(extension)):
            # Extract the basename, so that we can find the habitat ID
            basename = entry[0:entry.find(extension)]
            # Find the habitat ID for this file
            habitat_id = basename[basename.find('_')+1:]
            # If it's a known habitat ID, add the file to the habitat data
            if habitat_id in csv_data:
                csv_data[habitat_id].append(os.path.join(habitats_dir, entry))
    
    R = {}  # Dictionary: key=segment coord., value=list of fronting habitats
    # Process each habitat
    for habitat in csv_data:
        habitat_data = csv_data[habitat]
        # Check if there is a habitat layer to work with:
        if len(habitat_data) != ATTRIBUTE_COUNT:
            LOGGER.warning( \
            'No shapefile data associated to habitat %s. Skipping.' % \
            habitat_data[NAME])
        else:
            # A few shorthand names, for convenience
            habitat_name        =     habitat_data[NAME]
            habitat_id          = int(habitat_data[ID])
            habitat_rank        = int(habitat_data[RANK])
            habitat_distance    = int(habitat_data[PROTECTION_DISTANCE])
            # Open the habitat shapefile
            datasource = ogr.Open(habitat_data[FILE_NAME])
            # Reproject shapefile if necessary:
            datasource = adjust_shapefile_to_aoi(datasource, aoi)
            # Burn the shapefile and extract the array
            habitat_array = array_from_shapefile(datasource, aoi, cell_size)
            data_uri[habitat_name + '_' + str(habitat_id) + '_raster'] = \
                save_intermediate( habitat_array, \
                        habitat_name + '_' + str(habitat_id) + '_raster.tif')
            # Create a kernel to detect where the habitat influence the shore
            kernel = disc_kernel(habitat_distance / cell_size)
            # Area of influence by convolving the kernel with the habitat raster
            influenced_area = \
                sp.signal.convolve2d(habitat_array, kernel, mode='same')
            data_uri[habitat_name + '_influence'] = \
                save_intermediate(influenced_area,habitat_name+'_influence.tif')
            # Compute the influence of the habitat along the shore
            shore_influence = influenced_area * shore
            data_uri[habitat_name + '_influence_on_shore'] = \
                save_intermediate(shore_influence, \
                habitat_name + '_influence_on_shore.tif')
            # Mark all the shore segments under the influence of this habitat
            [I, J] = np.where(shore_influence > 0)
            for point in zip(I,J):
                if point in R: 
                    np.concatenate([R[point], np.array([habitat_rank])])
                else:
                    R[point] = np.array([habitat_rank])
    
    shore_data = np.zeros_like(shore)   # Will hold known ranking data
    # For each point influenced by a natural habitat
    for point in R:
        # Compute the vulnerability index
        R[point] = combined_rank(R[point])
        # Fill the array with shore data
        shore_data[point[0], point[1]] = R[point]
        
    data_uri['habitats_available_data'] = \
        save_intermediate(shore_data, 'habitats_available_data.tif')
    # Default shore vulnerability index
    default_data = (shore_data == 0).astype(int) * np.copy(shore) * 3
    data_uri['habitats_missing_data'] = \
        save_intermediate(default_data, 'habitats_missing_data.tif')
    # Actual shore vulnerability index
    R_hab = shore_data + default_data
    data_uri['habitats'] = \
        save_intermediate(R_hab, 'habitats.tif')
    return data_uri

def compute_geomorphology(args):
    """ Compute the geomorphology index as is described in InVEST's user guide.
    
        Inputs:
            - geomorphology: shapefile of the gemorphology ranking along the
              coastline.
            - aoi: region of interest.
            - shore: the shoreline (land = 1, sea = 0).
            - cell_size: granularity of all the rasterizations.
            - spread_radius: if the coastline from the geomorphology doesn't 
              match the land polygon's shoreline, we can increase the overlap
              by 'spreading' the data from the geomorphology over a wider area.
              The wider the spread, the more ranking data overlaps with the 
              coast. The spread is a convolution between the geomorphology 
              ranking data and a 2D gaussian kernel of area
              (2*spread_radius+1)^2. A radius of zero reduces the kernel to the
              scalar 1, which means no spread at all.              
            - args['intermediate_directory']: where intermediate files are
                stored

        Output:
            - Return a dictionary of all the intermediate file URIs.
            
        Detailed output:
            - aoi_reprojected_morphology.shp: the original geomorphology data
                reprojected to the AOI's projection.
            - geomorphology_rank.tif: the shore ranking as stored in the
                shapefile, rasterized within the aoi using cell_size.
            - spread_rank.tif: shore ranking smoothen by convolution of 
                geomorphology_rank.
            - geomorphology_coast.tif: rasterized coast (encoded as 1s) 
                extracted from geomorphology data.
            - spread_coast.tif: smoothed shore obtained by convolution of
                geomorphology_coast. Can't be used as is as a quotient in a
                division (see normalized_spread_rank), because of 0 values.
            - adjusted_spread_coast.tif: same as spread_coast.tif, where areas
                with zeros set to one to avoid division-by-zero as a quotient.
            - available_coastal_data.tif: coastal information that is available
                given the data.
            - missing_coastal_data.tif: portions of the shore for which rank
                information isn't available from the data.
            - normalized_spread_rank.tif: raster file of 
                spread_rank / spread_coast. The result is floating point values
                between 1 to 5. To be integer, they need to be approximated.
            - approximated_spread_rank.tif: approximation of
              normalized_spread_rank to be integers between 1 and 5.
            - geomorphology.tif: combination of rank information from
              normalized_spread_rank, completed with missing coastal
              information (from missing_coastal data) set to a moderate level
              of 3."""
    geomorphology = args['geomorphology']
    aoi = args['aoi']
    shore = args['shore']
    cell_size = args['cell_size']
    spread_radius = args['spread_radius']
    intermediate_directory = args['intermediate_directory']
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    field_name = 'RANK'
    MODERATE_RANK = 3
    # Ensure that 'field_name' exist in the file 
    assert(has_field(field_name, geomorphology))
    # Ensure projections are identical
    aoi_wkt = shapefile_wkt_projection(aoi)
    geo_wkt = shapefile_wkt_projection(geomorphology)
    if not projections_match([aoi_wkt, geo_wkt]):
        geomorphology=raster_utils.reproject_datasource(geomorphology,aoi_wkt,\
            os.path.join(intermediate_directory, 'reprojected_geo.shp'))
    # Burn the rank data to a geomorphology raster
    geo_raster = raster_from_shapefile(geomorphology, aoi, cell_size)
    layer, index = get_layer_and_index_from_field_name(field_name,geomorphology)
    gdal.RasterizeLayer(geo_raster, [1], geomorphology.GetLayer(layer),\
        options=["ATTRIBUTE=%s" % field_name, 'ALL_TOUCHED=TRUE'])
    #    options=["ATTRIBUTE=%s" % field_name])
    geo_array = geo_raster.GetRasterBand(1).ReadAsArray()
    data_uri['geomorphology_rank'] = \
        save_intermediate(geo_array, "geomorphology_rank.tif")
    # As the geomorphology coast may not perfectly overlap with 'shore',
    # we have to find a way to have rank values along every shore point.
    # Solution: 
    #   1)-spread the shore ranks with a convolution 
    #   2)-normalize by the spread (geomorphology shore set to 1,then convolved)
    # Create convolution kernel + apply it
    # avoid / by zero if R == 0
    R = (spread_radius/cell_size) if spread_radius >= cell_size else 1. 
    kernel = np.exp(-disc_kernel(R)/(2.*(R/3.)**2.))
    # 1)-Convolve the geomorphology ranking with the kernel
    spread_rank = sp.signal.convolve2d(geo_array, kernel,mode='same')
    data_uri['spread_rank'] = \
        save_intermediate(spread_rank, "spread_rank.tif")
    # Convolve the geomorphology coast with the kernel
    geo_coast = (geo_array > 0).astype(int)
    data_uri['geomorphology_coast'] = \
        save_intermediate(geo_coast, "geomorphology_coast.tif")
    spread_coast = sp.signal.convolve2d(geo_coast, kernel, mode='same')
    data_uri['spread_coast'] = \
        save_intermediate(spread_coast, "spread_coast.tif")
    # Create an array where the spread coast is normalized to 1, and 0 at sea
    norm_coast = np.copy(spread_coast)
    norm_coast[norm_coast > 0.] = 1.
    # Compute a mask for missing coast data
    available_coast = norm_coast * shore
    data_uri['available_coastal_data'] = \
        save_intermediate(available_coast, "available_coastal_data.tif")
    missing_coast = shore - available_coast
    data_uri['missing_coastal_data'] = \
        save_intermediate(missing_coast, "missing_coastal_data.tif")
    # 2)-Normalize the spread ranking by the spread coast
    spread_coast[spread_coast == 0.] = 1.     # avoid division by zero
    data_uri['adjusted_spread_coast'] = \
        save_intermediate(spread_coast, "adjusted_spread_coast.tif")
    norm_spread_rank = spread_rank / spread_coast
    data_uri['normalized_spread_rank'] = \
        save_intermediate(norm_spread_rank, "normalized_spread_rank.tif")
    # Approximate the rank to be between 1 to 5
    approximated_rank = (norm_spread_rank + 0.5).astype(int)
    data_uri['approximated_spread_rank'] = \
        save_intermediate(approximated_rank,"approximated_spread_rank.tif")
    # Available data is geomorphology rank along the shore
    available_rank = approximated_rank * shore
    data_uri['available_rank_data'] = \
        save_intermediate(available_rank, "available_rank_data.tif")
    # Geomorphology index is available data completed with missing data
    R_geomorphology = available_rank + (missing_coast * MODERATE_RANK)
    # Save the result
    data_uri['geomorphology'] = \
        save_intermediate(R_geomorphology, "geomorphology.tif")
    return data_uri

def compute_relief_rank(args):
    """ Compute the relief index as is described in InVEST's user guide.
    
        Inputs:
            - args['DEM']: the elevation data.
            - args['aoi']: region of interest.
            - args['landmass']: raster where land is 1 and sea is 0.
            - args['spread_radius']: if the coastline from the geomorphology i
                doesn't match the land polygon's shoreline, we can increase the 
                overlap by 'spreading' the data from the geomorphology over a 
                wider area. The wider the spread, the more ranking data overlaps
                with the coast. The spread is a convolution between the 
                geomorphology ranking data and a 2D gaussian kernel of area
                (2*spread_radius+1)^2. A radius of zero reduces the kernel to 
                the scalar 1, which means no spread at all.              
            - args['spread_radius']: how much the shore coast is spread to match
                the DEM's coast.
            - args['shore']: the shoreline (land = 1, sea = 0).
            - args['cell_size']: granularity of the rasterization.
            - args['intermediate_directory']: where intermediate files are
                stored
            
        Output:
            - Return R_relief as described in the user manual.
            - A rastrer file called relief.tif"""
    DEM = args['DEM']
    aoi = args['aoi']
    shore = args['shore']
    landmass = args['landmass']
    spread_radius = args['spread_radius']
    cell_size = args['cell_size']
    spread_radius = args['spread_radius']
    intermediate_directory = args['intermediate_directory']
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    # TODO: Refactor this code into 'adjust_raster_to_aoi'
    # Test that the projections are identical
    projections = [raster_wkt_projection(DEM), shapefile_wkt_projection(aoi)]
    assert(projections_match(projections))
    # Extract pixel size
    pixel_size = pixel_size_within_aoi(aoi, DEM)
    # Resample the DEM to cell_size
    resampled_DEM = raster_utils.reproject_dataset(DEM, cell_size,\
        DEM.GetProjection(), \
        os.path.join(intermediate_directory, 'resampled_DEM.tif'))
    # Clip the DEM to the AOI
    clipped_DEM_path = os.path.join(intermediate_directory, 'clipped_DEM.tif')
    clipped_DEM =raster_utils.clip_dataset(resampled_DEM, aoi,clipped_DEM_path)
    data_uri['clipped_DEM'] = clipped_DEM_path
    # Get land coverage from land polygon 
    land_coverage_raster = raster_from_shapefile(landmass, aoi, cell_size)
    land_coverage = land_coverage_raster.GetRasterBand(1).ReadAsArray()
    data_uri['land_coverage'] = \
        save_intermediate(land_coverage, 'land_coverage.tif')
    # Get the land elevation and land coverage from the DEM. Sea elevation is 0
    empty_raster = raster_utils.new_raster_from_base(land_coverage_raster, \
        os.path.join(intermediate_directory, 'empty_raster.tif'), \
        'GTiff', nodata(), gdal.GDT_Float32)
    land_elevation_uri=os.path.join(intermediate_directory,'land_elevation.tif')
    land_elevation_raster = combine_rasters(empty_raster, clipped_DEM,\
        land_elevation_uri)
    land_elevation = land_elevation_raster.GetRasterBand(1).ReadAsArray()
    land_elevation[land_elevation <0] = 0   # Set sea elevation to 0
    land_elevation_raster.GetRasterBand(1).WriteArray(land_elevation)
    data_uri['land_elevation'] = land_elevation_uri
    # Compute the aoi's raster array
    aoi_array = array_from_shapefile(aoi, aoi, cell_size)
    # Create convolution kernel + apply it
    kernel = disc_kernel(spread_radius / cell_size)
    # Convolve the land elevation with the kernel
    elevation_average = sp.signal.convolve2d(land_elevation, kernel,mode='same')
    data_uri['elevation_average'] = \
        save_intermediate(elevation_average, 'elevation_average.tif')
    # Convolve the land coverage with the kernel
    land_proportion =sp.signal.convolve2d(land_coverage, kernel, mode='same')
    land_proportion[land_proportion == 0] = 1   # Avoid division by zero
    data_uri['land_proportion'] = \
        save_intermediate(land_proportion, 'land_proportion.tif')
    # Compute average land height
    average_land_height = elevation_average / land_proportion
    data_uri['average_land_height'] = \
    save_intermediate(average_land_height, 'average_land_height.tif')
    # Per-segment average of the shore height along the coast
    average_relief = average_land_height * shore        # shore mask
    data_uri['average_relief'] = \
        save_intermediate(average_relief, "average_relief.tif")
    # Relief index: percentile -(round)-> integer -(+1)-> index
    R_relief = (average_relief /np.amax(average_relief) *5).astype(int) +shore
    # Save result
    data_uri['relief'] = \
        save_intermediate(R_relief, "relief.tif")
    return data_uri

def compute_wind_exposure(args):
    """ Compute the wind exposure for every shore segment as in equation 4.5
    
        Inputs:
            - args['wind_data']: wind information
            - args['aoi']: used to set the raster extents
            - args['fetch']: maximal fetch distance in meters.
            - args['sector_count']: number of equiangular fetch sectors.
            - args['cell_size']: granularity of the rasterization.
            - args['intermediate_directory']:where intermediate files are stored
            
        Outputs:
            - REI.tif: combined REI value of the wind exposure index for all
              sectors along the shore.
        
            - For each equiangular fetch sector n:
                - REI_n.tif: per-sector REI value (U_n * P_n * F_n)."""
    wind_data = args['wind_wave_data']
    aoi = args['aoi']
    fetch = args['fetch']
    sector_count = args['sector_count']
    cell_size = args['cell_size']
    intermediate_directory = args['intermediate_directory']
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    # Variable initializations
    sector_range = range(sector_count)
    sectors_deg = np.array(sector_range, dtype=int) *360 / sector_count
    basename = 'REI'
    extension = '.tif'  # Filename extension
    # Create a raster dict of the wind intensity U_n
    U = assign_dictionary(extract_REI_V(wind_data, aoi, cell_size))
    # Create a raster dict of the percent of all wind P_n in a given sector
    P = assign_dictionary(extract_REI_PCT(wind_data, aoi, cell_size))
    # Create a raster dict of the fetch distances F_n
    F = assign_dictionary(extract_fetch(fetch, aoi, cell_size))
    # Compute the REI
    REI = np.sum([U(n) *P(n) *F(n) for n in sectors_deg], axis=0)   
    # save intermediate data
    for n in sectors_deg:
        data_uri[basename +'_' + str(n)] = \
            save_intermediate(U(n)*P(n)*F(n), basename+'_' +str(n) +extension)
    data_uri[basename] = save_intermediate(REI, basename + extension)
    # done
    return data_uri

def compute_wave_exposure(args):
    """ Compute the wind exposure for every shore segment
    
        Inputs:
            - args['wave_data']: wave information
            - args['aoi']: used to set the raster extents
            - args['fetch']: maximal fetch distance in meters.
            - args['sector_count']: number of equiangular fetch sectors.
            - args['cell_size']: granularity of the rasterization.
            - args['H_threshold']: threshold value for the H function (eq. 7)
            - args['intermediate_directory']:where intermediate files are stored
            
        Outputs:
            - A file called wave.tif that contains the wind exposure index along
              the shore.
            - For each equiangular fetch sector k:
                - F_k.tif: per-sector fetch value (see eq. 6).
                - H_k.tif: per-sector H value (see eq. 7)
                - E_o_k.tif: per-sector average oceanic wave power (eq. 6)
                - E_l_k.tif: per-sector average wind-generated wave power (eq.9)
                - E_w_k.tif: per-sector wave power (eq.5)
                - E_w.tif: combined wave power."""
    wave_data = args['wind_wave_data']
    aoi = args['aoi']
    fetch = args['fetch']
    sector_count = args['sector_count']
    cell_size = args['cell_size']
    H_threshold = args['H_threshold']
    intermediate_directory = args['intermediate_directory']
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    # Variable initializations
    sector_range = range(sector_count)
    sectors_deg = np.array(sector_range, dtype=int) *360 / sector_count
    basename = 'wave'
    extension = '.tif'  # Filename extension
    # Set H threshold, in km
    H = set_H_threshold(H_threshold)
    # Create a raster dict of the wind power intensity P_l(k)
    P_l = assign_dictionary(extract_REI_V(wave_data, aoi, cell_size))
    # Create a raster dict of the percent of all winds O_l(k) in a given sector
    O_l = assign_dictionary(extract_REI_PCT(wave_data, aoi, cell_size))
    # Create a raster dict of the wave power intensity P_o(k)
    P_o = assign_dictionary(extract_WavP(wave_data, aoi, cell_size))
    # Create a raster dict of the percent of all waves O_o(k) in a given sector
    O_o = assign_dictionary(extract_WavPPCT(wave_data, aoi, cell_size))
    # Create a raster dict of the fetch distances F(k)
    F = assign_dictionary(extract_fetch(fetch, aoi, cell_size))
    # Compute the oceanic wave exposure E_o
    E_o = np.sum([H(F(k)) *P_o(k) *O_o(k) for k in sectors_deg], axis=0)   
    for k in sectors_deg:
        data_uri['F_' + str(k)] = \
            save_intermediate(F(k), 'F_' + str(k) +extension)
        data_uri['H_' +str(k)] = \
            save_intermediate(H(F(k)), "H_" +str(k) +extension) 
        data_uri['E_o_' + str(k)] = \
            save_intermediate(H(F(k))*P_o(k)*O_o(k),"E_o_" +str(k) +extension)
    data_uri['E_o'] = save_intermediate(E_o, "E_o" +extension)
    # Compute the local wave exposure E_l
    #E_l = np.sum([P_l(k) *O_l(k) for k in sectors_deg], axis=0)
    E_l = np.sum([P_l(k) *O_l(k) for k in sectors_deg], axis=0)
    for i in sectors_deg:
        data_uri["E_l_" + str(k)] = \
            save_intermediate(P_l(k)*O_l(k),"E_l_" + str(k) + extension)
    data_uri['E_l'] = save_intermediate(E_l, "E_l" +extension)
    # Compute the overall wave exposure
    E_w = np.amax([E_o, E_l], axis=0)
    data_uri['E_w'] = save_intermediate(E_w, "E_w" +extension)
    return data_uri

# TODO: write a unit test for this function
def combine_rasters(target, source, filename):
    """ Write source raster's contents over the target's, preserving the
    spatial structure of the data.
    
        Inputs:
            - source: where the information comes from.
            - target: where the information is going to.
        
        Output:
            - the target raster contents is overridden by the source raster's
            - The new target dataset nodata value

        Note: The function uses geotransforms to overrite data. The
        geotransform (GT) is stored in a 6-element array (0 to 5), which
        follows the convention below:

        Xgeo = GT(0) + Xpixel*GT(1) + Yline*GT(2)
        Ygeo = GT(3) + Xpixel*GT(4) + Yline*GT(5)
        
        If Up is north, then GT(2) and GT(4) == 0.
        For reference, see:
    http://www.gdal.org/classGDALDataset.html#af9593cc241e7d140f5f3c4798a43a668
    """
    # The rasters must have the same projection
    assert(projections_match([raster_wkt_projection(source), \
                              raster_wkt_projection(target)]))
    # Use the geotransform to overwrite
    sgt = source.GetGeoTransform()
    tgt = target.GetGeoTransform()
    # Set constraints on the geotransform. More constraints should be lifted
    # in the future. See the docstrings for the meaning of source_gt/target_gt
    # We want 'almost' equal within 6 orders of magnitude.
    assert(abs(abs(sgt[0])-abs(tgt[0])) < 0.001) # Same starting X
    assert(abs(abs(sgt[1])-abs(tgt[1])) < 0.001)
    assert(abs(abs(sgt[2])-abs(tgt[2])) < 0.001)
    assert(abs(abs(sgt[3])-abs(tgt[3])) < 0.001) # same starting Y
    assert(abs(abs(sgt[4])-abs(tgt[4])) < 0.001)
    assert(abs(abs(sgt[5])-abs(tgt[5])) < 0.001)
    # Extract the arrays contents and nodata
    source_nodata = source.GetRasterBand(1).GetNoDataValue()
    target_nodata = target.GetRasterBand(1).GetNoDataValue()
    source_array = source.GetRasterBand(1).ReadAsArray()
    target_array = target.GetRasterBand(1).ReadAsArray()
    # Prevent out-of-bounds overwrite
    i_max = min(source_array.shape[0], target_array.shape[0])
    j_max = min(source_array.shape[1], target_array.shape[1])
    # Change nodata so that it doesn't collide with new array values
    # Concatenate source and target...
    concatenated_array = np.zeros((i_max, j_max*2))
    concatenated_array[:i_max, :j_max] = source_array[:i_max, :j_max]
    concatenated_array[:i_max, j_max:j_max*2] = target_array[:i_max, :j_max]
    # ...so that any value not in there can be nodata.
    new_nodata = raster_utils.calculate_value_not_in_array(concatenated_array)
    n_type = type(target.GetRasterBand(1).GetNoDataValue()) 
    new_nodata = numpy_cast(raster_utils.gdal_cast(new_nodata, n_type), n_type)
    # Override old nodata values with the new nodata
    source_array[source_array == source_nodata] = new_nodata
    target_array[target_array == target_nodata] = new_nodata
    # Write valid data--skip nodata
    # We can do this, because the arrays are index-compatible:see asserts above
    target_array[:i_max, :j_max] = np.array( \
        [[source_array[i, j] \
        if (source_array[i, j] != new_nodata) \
        else target_array[i, j] \
            for j in range(j_max)] \
                for i in range(i_max)] \
        )
    # Store the result back in target
    raster = raster_utils.new_raster_from_base(\
        target, filename, 'GTiff', new_nodata, gdal.GDT_Float32)
    raster.GetRasterBand(1).WriteArray(target_array)
    return raster

# TODO: write a unit test for this function
# TODO: comment this
def numpy_cast(value, numpy_type):
    numpy_int_types = [  np.int, np.int8, np.int16, np.int32, 
                                 np.uint8,np.uint16,np.uint32 ]
    # Check if machine precision is 64 bits for integers
    if sys.maxint > 2**32:
        numpy_int_types.append(np.int64)
        numpy_int_types.append(np.uint64)
    numpy_long_types = [ np.int64, np.uint64 ]
    numpy_float_types = [np.float, np.float32, np.float64]

    if numpy_type == type(np.bool):
        value = bool(value)
    elif numpy_type in numpy_int_types:
        value = int(value)
    elif numpy_type in numpy_float_types:
        value = float(value)
    elif numpy_type == type(np.complex64):
        value = complex(value)
    else:
        LOGGER.debug("Warning: can't cast %s to a python type." % \
            str(type(value)))
    return value
# TODO: write a unit test for this function
# TODO: comment this
def enumerate_shapefile_fields(shapefile):
    layer_count = shapefile.GetLayerCount()
    for l in range(layer_count):
        layer = shapefile.GetLayer(l)
        feature_count = layer.GetFeatureCount()
        print('Layer:', l+1, '/', layer_count, ', ', feature_count, 'features.')
        feature = layer.GetFeature(0)
        field_count = feature.GetFieldCount()
        for f in range(field_count):
            field_defn = feature.GetFieldDefnRef(f)
            print('Field', f+1, '/', field_count,
                ', name:', field_defn.GetNameRef())

# TODO: write a unit test for this function
def has_field(field_name, shapefile):
    """Return True if the shapefile contains field_name, False otherwise.
        
        Inputs:
            - field_name: string to look for.
            - shapefile: where to look for the field.

        Output:
            - True if the field belongs to 'shapefile', False otherwise."""
    layer_count = shapefile.GetLayerCount()
    for l in range(layer_count):
        layer = shapefile.GetLayer(l)
        feature_count = layer.GetFeatureCount()
        assert(feature_count > 0)
        feature = layer.GetFeature(0)
        field_count = feature.GetFieldCount()
        for f in range(field_count):
            field_defn = feature.GetFieldDefnRef(f)
            if field_defn.GetNameRef() == field_name:
                return True
    return False

# TODO: write a unit test for this function
def get_layer_and_index_from_field_name(field_name, shapefile):
    """Given a field name, return its layer and field index.
        Inputs:
            - field_name: string to look for.
            - shapefile: where to look for the field.

        Output:
            - A tuple (layer, field_index) if the field exist in 'shapefile'.
            - (None, None) otherwise."""
    # Look into every layer
    layer_count = shapefile.GetLayerCount()
    for l in range(layer_count):
        layer = shapefile.GetLayer(l)
        # Make sure the layer is not empty
        feature_count = layer.GetFeatureCount()
        assert(feature_count > 0)
        feature = layer.GetFeature(0)
        # Enumerate every field
        field_count = feature.GetFieldCount()
        for f in range(field_count):
            field_defn = feature.GetFieldDefnRef(f)
            if field_defn.GetNameRef() == field_name:
                return (l, f)
    # Nothing found
    return (None, None)

# TODO: write a unit test for this function
def combined_rank(R_k):
    """Compute the combined habitats ranks as described in equation (3)
    
        Inputs:
            - R_k: the list of ranks
            
        Output:
            - R_hab as is decribed in the user guide's equation 3."""
    return 4.8 -0.5 *math.sqrt( (1.5 *max(5-R_k))**2 + \
                    sum((5-R_k)**2) -(max(5-R_k))**2)

# TODO: write a unit test for this function
def disc_kernel(r):
    """Create a (r+1)^2 disc-shaped array filled with 1s where d(i-r,j-r) <= r
    
        Input: r, the kernel radius. r=0 is a single scalar of value 1.
        
        Output: a (r+1)x(r+1) array with:
                - 1 if cell is closer than r units to the kernel center (r,r),
                - 0 otherwise.
                
            Distances are Euclidean."""
    # Create a grid of an evenly-spaced datapoints of diameter r * 2 + 1
    [X, Y] = np.mgrid[0:(r*2+1), 0:(r*2+1)]
    # Compute an array of 1s where the distance to the center (r,r) < r
    kernel = (r**2 >= np.square(X-r)+np.square(Y-r)).astype(int)
    return kernel

# TODO: write a unit test for this function
def set_H_threshold(threshold):
    """ Return 0 if fetch is strictly below a threshold in km, 1 otherwise.
    
        Inputs:
            fetch: fetch distance in meters.

        Return:
            1 if fetch >= threshold (in km)
            0 if fetch  < threshold

        Note: conforms to equation 4.8 in the invest documentation."""
    def H(fetch):
        return np.array(fetch >= (threshold * 1000)).astype(int)

    return H

# TODO: write a unit test for this function
def raster_spatial_reference(raster):
    """Extract a WKT-compliant spatial reference from a dataset (raster).
    
        Input: The dataset.
        
        Output: the dataset's WKT-compliant spatial reference."""
    srs = osr.SpatialReference()
    srs.ImportFromWkt(raster.GetProjection())
    return srs

# TODO: write a unit test for this function
def shapefile_spatial_reference(shapefile):
    """Extract a WKT-compliant spatial reference from a datasource (shapefile).
    
        Input: A raster datasource.
        
        Output: the datasource's WKT-compliant spatial reference."""
    return shapefile.GetLayer(0).GetSpatialRef()

# TODO: write a unit test for this function
# TODO: remove 'projection' from function name
def raster_wkt_projection(raster):
    """ Return the projection of a raster in the OpenGIS WKT format.
    
        Input: 
            - raster: raster file
        
        Output:
            - a projection encoded as a WKT-compliant string."""
    return raster.GetProjection()

# TODO: write a unit test for this function
# TODO: remove 'projection' from function name
def shapefile_wkt_projection(shapefile):
    """ Return the projection of a shapefile in the OpenGIS WKT format.
    
        Input: 
            - raster: raster file
        
        Output:
            - a projection encoded as a WKT-compliant string."""
    layer = shapefile.GetLayer()
    sr = layer.GetSpatialRef()
    return sr.ExportToWkt()

# TODO: write a unit test for this function
def projections_match(projection_list):
    """Check that two gdal datasets are projected identically. 
       Functionality adapted from Doug's 
       biodiversity_biophysical.check_projections 

        Inputs:
            - projection_list: list of projections to compare

        Output: 
            - False the datasets are not projected identically.
    """
    assert(len(projection_list) > 1)

    srs_1 = osr.SpatialReference()
    srs_2 = osr.SpatialReference()

    srs_1.ImportFromWkt(projection_list[0])

    for projection in projection_list:
        srs_2.ImportFromWkt(projection)

        if srs_1.IsProjected() != srs_2.IsProjected():
            LOGGER.debug('Different proj.: One of the Rasters is Not Projected')
            return False
        if srs_1.GetLinearUnits() != srs_2.GetLinearUnits():
            LOGGER.debug('Different proj.: Proj units do not match %s:%s', \
                     srs_1.GetLinearUnits(), srs_2.GetLinearUnits())
            return False
    
        if srs_1.GetAttrValue("PROJECTION") != srs_2.GetAttrValue("PROJECTION"):
            LOGGER.debug('Projections are not the same')
            return False

    return True

# TODO: write a unit test for this function
def point_from_shapefile(shapefile):
    """ Return a point that lies within the region specified by the shapefile.
    
        Inputs:
            - shapefile: shapefile that contains geometric features
            
        Output:
            - A coordinate tuple (x, y) of a point lying in the envelope of the
              first geometry in the shapefile."""
    feat = shapefile.GetLayer(0).GetNextFeature()
    geom = feat.GetGeometryRef()
    envelope = geom.GetEnvelope()
    x = (envelope[0] + envelope[1]) / 2 
    y = (envelope[2] + envelope[3]) / 2

    return (x, y)


# TODO: write a unit test for this function
def pixel_size_within_aoi(aoi, raster):
    """This function helps retrieve the pixel sizes of the global DEM 
    when given an area of interest that has a certain projection.
    
    aoi - A point shapefile datasource indicating the area of interest
    global_dem - The global DEM dataset to get the pixel size from
    
    returns - A tuple of the x and y pixel sizes of the global DEM 
              given in the units of what 'shape' is projected in
    """
    # Get the spatial reference for each object
    aoi_sr = shapefile_spatial_reference(aoi)
    raster_sr = raster_spatial_reference(raster)
    # Get forward and inverse transformations
    T = osr.CoordinateTransformation(raster_sr, aoi_sr)
    T_inverse = osr.CoordinateTransformation(aoi_sr, raster_sr)
    #Get a point in the clipped shape to determine output grid size
    x, y = point_from_shapefile(aoi)
    #Convert the point from meters to geom_x/long
    point = T_inverse.TransformPoint(x, y)
        
    #Get the size of the pixels in meters, to be used for creating rasters
    pixel_size = \
        raster_utils.pixel_size_based_on_coordinate_transform(raster, T, point)
    return pixel_size

# TODO: write a unit test for this function
def adjust_raster_to_aoi(data, aoi, cell_size, base_path = ''):
    """Adjust the raster's data to the aoi, i.e.reproject & clip data points.
    
        Inputs:
            - data: the dataset to adjust
            - aoi: area of interest
            - base_path: directory where the intermediate data will be stored.
            
        Output:
            - A reprojected raster that is clipped to the aoi."""
    data_wkt = raster_wkt_projection(data)
    aoi_wkt = shapefile_wkt_projection(aoi)
    # Reproject the aoi to data's projection
    if not projections_match([data_wkt, aoi_wkt]):
        aoi = raster_utils.reproject_datasource(aoi, data_wkt, \
           os.path.join(base_path, str(unique_value())+'_reprojected_aoi.shp'))
    # Clip all the points outside the aoi
    data = raster_utils.clip_dataset(data, aoi, \
        os.path.join(base_path,str(unique_value())+'_clipped_dataset.tif'))
    # Convert the datasource back to the original projection (aoi's)
    pixel_size = raster_utils.pixel_size(data)
    print('projection match=', projections_match([data_wkt,aoi_wkt]))
    if (not projections_match([data_wkt,aoi_wkt]) or (pixel_size!=cell_size)):
        print('trying to resample the raster with cell_size=', cell_size)
        data = raster_utils.reproject_dataset(data,aoi_wkt,cell_size, \
        os.path.join(base_path,str(unique_value())+'_reprojected_dataset.tif'))
    return data

# TODO: write a unit test for this function
def adjust_shapefile_to_aoi(data, aoi, base_path=''):
    """Adjust the shapefile's data to the aoi, i.e.reproject & clip data points.
    
        Inputs:
            - data: datasource to adjust
            - aoi: area of interest
            - base_path: directory where the intermediate data will be stored.

        Output:
            - A reprojected shapefile that is clipped to the aoi."""
    data_wkt = shapefile_wkt_projection(data)
    aoi_wkt = shapefile_wkt_projection(aoi)
    # Reproject the aoi to be in data's projection
    if not projections_match([data_wkt, aoi_wkt]):
        aoi = raster_utils.reproject_datasource(aoi, data_wkt, \
           os.path.join(base_path, str(unique_value())+'_reprojected_aoi.shp'))
    # Clip all the shapes outside the aoi
    data = clip_datasource(aoi, data, \
        os.path.join(base_path,str(unique_value()) +'_clipped_datasource.shp'))
    # Convert the datasource back to the original projection (aoi's)
    if not projections_match([data_wkt, aoi_wkt]):
        data = raster_utils.reproject_datasource(data, aoi_wkt, \
        os.path.join(base_path,str(unique_value())+'_reproj_datasource.shp'))
    return data
    
def clip_datasource(aoi_ds, orig_ds, output_uri):
    """Clip an OGR Datasource of geometry type polygon by another OGR Datasource
        geometry type polygon. The aoi_ds should be a shapefile with a layer
        that has only one polygon feature

        aoi_ds - an OGR Datasource that is the clipping bounding box
        orig_ds - an OGR Datasource to clip
        out_uri - output uri path for the clipped datasource

        returns - a clipped OGR Datasource """
    orig_layer = orig_ds.GetLayer()
    aoi_layer = aoi_ds.GetLayer()

    # If the file already exists remove it
    if os.path.isfile(output_uri):
        os.remove(output_uri)

    # Create a new shapefile from the orginal_datasource 
    output_driver = ogr.GetDriverByName('ESRI Shapefile')
    output_datasource = output_driver.CreateDataSource(output_uri)

    # Get the original_layer definition which holds needed attribute values
    original_layer_dfn = orig_layer.GetLayerDefn()

    # Create the new layer for output_datasource using same name and geometry
    # type from original_datasource as well as spatial reference
    output_layer = output_datasource.CreateLayer(
            original_layer_dfn.GetName(), orig_layer.GetSpatialRef(), 
            original_layer_dfn.GetGeomType())

    # Get the number of fields in original_layer
    original_field_count = original_layer_dfn.GetFieldCount()

    # For every field, create a duplicate field and add it to the new 
    # shapefiles layer
    for fld_index in range(original_field_count):
        original_field = original_layer_dfn.GetFieldDefn(fld_index)
        output_field = ogr.FieldDefn(
                original_field.GetName(), original_field.GetType())
        output_field.SetWidth(original_field.GetWidth())
        output_field.SetPrecision(original_field.GetPrecision())
        output_layer.CreateField(output_field)

    # Get the feature and geometry of the aoi
    aoi_feat = aoi_layer.GetFeature(0)
    aoi_geom = aoi_feat.GetGeometryRef()

    # Iterate over each feature in original layer
    for orig_feat in orig_layer:
        # Get the geometry for the feature
        orig_geom = orig_feat.GetGeometryRef()
        # Check to see if the feature and the aoi intersect. This will return a
        # new geometry if there is an intersection. If there is not an
        # intersection it will return an empty geometry or it will return None
        # and print an error to standard out
        intersect_geom = aoi_geom.Intersection(orig_geom)
       
        if not intersect_geom == None and not intersect_geom.IsEmpty():
            # Copy original_datasource's feature and set as new shapes feature
            output_feature = ogr.Feature(
                    feature_def=output_layer.GetLayerDefn())
            output_feature.SetGeometry(intersect_geom)
            # Since the original feature is of interest add it's fields and
            # Values to the new feature from the intersecting geometries
            for fld_index2 in range(output_feature.GetFieldCount()):
                orig_field_value = orig_feat.GetField(fld_index2)
                output_feature.SetField(fld_index2, orig_field_value)

            output_layer.CreateFeature(output_feature)
            output_feature = None

    return output_datasource

def extract_WavP(wave_data, aoi, cell_size):
    """ Create dictionary of raster filenames of datum for each sector n.
    
        Inputs:
            - wave_data: wave data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            
        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where U(n) is defined on each cell"""
    field_prefix = 'WavP_'
    extension = '.tif'

    return raster_list_from_interpolated_fields( \
        wave_data, field_prefix, extension, aoi, cell_size)

def extract_WavPPCT(wave_data, aoi, cell_size):
    """ Create dictionary of raster filenames of datum for each sector n.
    
        Inputs:
            - wave_data: wave data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            
        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where U(n) is defined on each cell"""
    field_prefix = 'WavPPCT'
    extension = '.tif'

    return raster_list_from_interpolated_fields( \
        wave_data, field_prefix, extension, aoi, cell_size)

def extract_REI_V(wind_data, aoi, cell_size):
    """ Create dictionary of raster filenames of datum for each sector n.
    
        Inputs:
            - wind_data: wind data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            
        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where U(n) is defined on each cell"""
    field_prefix = 'REI_V'
    extension = '.tif'

    return raster_list_from_interpolated_fields( \
        wind_data, field_prefix, extension, aoi, cell_size)

def extract_REI_PCT(wind_data, aoi, cell_size):
    """ Create dictionary of raster filenames of datum for each sector n.
    
        Inputs:
            - wind_data: wind data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            
        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where P(n) is defined on each cell"""
    field_prefix = 'REI_PCT'
    extension = '.tif'

    return raster_list_from_interpolated_fields( \
        wind_data, field_prefix, extension, aoi, cell_size)

# TODO: write a unit test for this function
# TODO: improve description
def raster_list_from_interpolated_fields(data, prefix, suffix, aoi, cell_size):
    """Interpolate user-specified data fields on rasters saved in a raster list
    
        Inputs:
            - data: the datasource
            - prefix: name used to find the fields
            - suffix: used to complete the filename. This is where the
              extension goes
            - aoi: used to set the raster size
            - cell_size: raster coarseness in meters
            
        Output:
            - A str:filename dictionary where the key is the remainder of the
              field name. If there is no remainder, then key == prefix"""
    # Get the datasource field
    layer = data.GetLayer(0)
    feature_definition = layer.GetLayerDefn()
    field_count = feature_definition.GetFieldCount()
    raster_list = {} # key is the direction angle
    for field in range(field_count):
        name = feature_definition.GetFieldDefn(field).GetNameRef()
        # Determine if the current field should be processed
        if re.match(prefix, name):
            name = name[len(prefix):]
            # Create raster
            separator = '_' if name else ''
            filename =  prefix +separator +name +suffix
            raster = raster_utils.create_raster_from_vector_extents(
                cell_size,cell_size, gdal.GDT_Float32, nodata(), filename, aoi)
            # Vectorize data
            raster_utils.vectorize_points(data, field, raster)
            # Store filename
            name = name if name else prefix
            raster_list[name] = filename

    return raster_list

# TODO: write a unit test for this function
def extract_fetch(fetch, aoi, cell_size):
    """ Create dictionary of raster filenames of fetch F(n) for each sector n.
    
        Inputs:
            - wind_data: wind data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            
        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where F(n) is defined on each cell"""
    field_prefix = 'REI_fetch'
    extension = '.tif'

    # Save computed raster
    raster_list = {} # key is the direction angle
    # Extract the # of sectors, assumed to start at angle 0 with identical span
    sector_count = 0
    fetch_keys = fetch.keys()
    for point in fetch_keys:
        sector_count = len(fetch[point])
        break
    # Build F(n), raster filename dictionary and save data to disk
    for sector in range(sector_count):
        name = str(sector *360 /sector_count)
        # Open raster file
        filename = field_prefix +'_' +name +extension
        raster =\
            raster_utils.create_raster_from_vector_extents(cell_size,cell_size,\
            gdal.GDT_Float32, nodata(), filename, aoi)
        band = raster.GetRasterBand(1)
        array = band.ReadAsArray() 
        # Build a fetch array F(n) for sector n
        array = np.ones_like(array)
        for point in fetch_keys:
            array[point] = fetch[point][sector] * float(cell_size)
        # Save F(n) to disk
        band.WriteArray(array)
        raster_list[name] = filename

    return raster_list

# TODO: write a unit test for this function
def set_save_info(base_path, aoi, cell_size):
    """Closure that saves an array in a raster constructed from an AOI.
    
        Inputs:
            - base_path: base path used to concatenate the raster filename to
            - aoi: the AOI from which to construct the template raster
            - cell_size: granularity of the rasterization in meters
            
        Output:
            - save_array: a 'save_array' object."""
    def save_array(array, filename):
        """ Save an array to a raster constructed from an AOI.
        
            Inputs:
                - array: the array to be saved
                - filename: where the array will be saved
                
            Output:
                - save the array in a raster file constructed from the AOI of
                  granularity specified by cell_sizei
                - Return the array uri."""
        file_path = os.path.join(base_path, filename)

        # Remove the file if it already exist
        if os.path.isfile(file_path):
            os.remove(file_path)
        
        no_data = float(raster_utils.calculate_value_not_in_array(array))
        raster =\
            raster_utils.create_raster_from_vector_extents(\
            cell_size,cell_size, gdal.GDT_Float32, no_data, file_path, aoi)
        band = raster.GetRasterBand(1)

        try:
            band.WriteArray(array)
        except:
            internal_array = band.ReadAsArray()
            LOGGER.debug("Error: can't save array to %s." % file_path)
            print('source array size:', array.shape)
            print('destination array size:', internal_array.shape)

        return file_path

    # Create the directory if necessary
    if not os.path.exists(base_path):
        LOGGER.debug('directory %s doesn\'t exist, creating one.' % base_path)
        os.path.makedirs(base_path)

    return save_array

# TODO: write a unit test for this function
def assign_dictionary(dictionary):
    """ Closure allowing to index raster arrays from a file dictionary by string
    
        Input: 
            - dictionary: the dictionary that points to raster files
            - sector: the key that points to an existing raster filename
            
        Output:
            - An array extracted from the raster file pointed to by the
              dictionary"""
    def raster_list(sector):
        raster_filename = dictionary[str(sector)]
        raster = gdal.Open(raster_filename)
        band = raster.GetRasterBand(1)
        raster_array = band.ReadAsArray()
        
        raster = None
        band = None
        
        return raster_array

    return raster_list

# TODO: write a unit test for this function
def convert_shape_to_array(shapefile, aoi, cell_size):
    """ Convert a shapefile to an array.
    
        - shapefile: location of the shapefile to be rasterized
        - aoi: location of the shapefile containing the area of interest
        - cell_size: granularity of the rasterization
        
        Returns an array with cell codes for land(), sea(), and nodata()."""
    # Try to open the area of interest:
    try:
        with open(aoi): 
            pass        
    except IOError:
        print("File ", aoi, " doesn't exist...")

    # Try to open the shapefile:
    try:
        with open(shapefile): 
            pass        
    except IOError:
        print("File ", shapefile, " doesn't exist...")

    # Open the shapefiles, and extract the shapes from the first layers
    aoi_datasource = ogr.Open(aoi)
    global_datasource = ogr.Open(shapefile)

    aoi_layer = aoi_datasource.GetLayer(0)
    global_layer = global_datasource.GetLayer(0)

    aoi_raster_filename = 'aoi_raster.tif'
    global_raster_filename = 'global_raster.tif'

    aoi_raster = \
        raster_utils.create_raster_from_vector_extents(cell_size, cell_size,
        gdal.GDT_Int32, nodata(), aoi_raster_filename, aoi_datasource)

    global_dataset = raster_utils.new_raster_from_base(aoi_raster, 
        global_raster_filename, 'GTiff', nodata(), gdal.GDT_Int32)
    # Get the band and nodata for each file
    aoi_band, aoi_nodata = \
        raster_utils.extract_band_and_nodata(aoi_raster)
    global_band, global_nodata = \
        raster_utils.extract_band_and_nodata(global_dataset)
    # Initialize the band and burn the shapefile on it for each file
    aoi_band.Fill(aoi_nodata)
    gdal.RasterizeLayer(aoi_raster, [1], aoi_layer, burn_values=[land()])
    global_band.Fill(global_nodata)
    gdal.RasterizeLayer(global_dataset,[1], global_layer, burn_values=[land()])
    # Extract the array from the bands
    aoi_array = aoi_band.ReadAsArray()
    global_array = global_band.ReadAsArray()
    # Initialize the raster with land, sea, and nodata cells
    landmass_array = np.ones_like(global_array) * land()
    landmass_array[global_array == global_nodata] = sea()
    landmass_array[aoi_array == aoi_nodata] = nodata()
    # cleanup before returning
    aoi_datasource = None
    global_datasource = None
    
    return landmass_array

# TODO: write a unit test for this function
# TODO: Adjust the raster data type depending on the field type!!!
def raster_from_shapefile(shapefile, aoi, cell_size, field=None):
    """Burn default or user-defined data from a shapefile on a raster.

        Inputs:
            - shapefile: the dataset to be discretized
            - cell_size: coarseness of the discretization (in meters)
            - field: optional field name where to extract the data from
        
        Output: A shapefile where:
            If field is specified, the field data is used as burn value.
            If field is not specified, then:
                - shapes on the first layer are encoded as 1s
                - the rest is encoded as 0"""
    raster_filename = str(unique_value())
    if field: raster_filename += '_' + field
    raster_filename += '_rasterized_shapefile.tif'
    # TODO: Find out why when this line is removed, detect_shore asserts
    NO_DATA = 0
    # Create the raster that will contain the new data
    raster = \
        raster_utils.create_raster_from_vector_extents(cell_size, 
        cell_size, gdal.GDT_Int32, NO_DATA, raster_filename, aoi)
    band, NO_DATA = \
        raster_utils.extract_band_and_nodata(raster)
    if field != None:
        # Will be used to reset the nodata value
        old_array = band.ReadAsArray()
        # Burn the data in 'field' to a raster
        layer, index = get_layer_and_index_from_field_name(field,shapefile)
        gdal.RasterizeLayer(raster, [1], shapefile.GetLayer(layer),\
            options=["ATTRIBUTE=%s" % field])
        # Change nodata so that it doesn't collide with new array values
        new_nodata = raster_utils.calculate_value_not_in_dataset(raster)
        nodata_type = type(raster.GetRasterBand(1).GetNoDataValue()) 
        new_nodata =numpy_cast(raster_utils.gdal_cast(new_nodata, nodata_type),\
            nodata_type)
        # Get the new data
        raster_array = raster.GetRasterBand(1).ReadAsArray()
        # Replace the old value of nodata to the new one
        raster_array[old_array == NO_DATA] = new_nodata
        # Adjust the nodata value
        raster.GetRasterBand(1).SetNoDataValue(new_nodata)
        # Write the new contents back into the raster
        raster.GetRasterBand(1).WriteArray(raster_array)
    else:
        gdal.RasterizeLayer(raster, [1], shapefile.GetLayer(0), burn_values=[1])
    return raster

# TODO: write a unit test for this function
def array_from_shapefile(shapefile, aoi, cell_size, field=None):
    """Burn shapes from a shapefile's first layer on an array.

        Inputs:
            - shapefile: the dataset to be discretized
            - cell_size: coarseness of the discretization (in meters)
        
        Output: An array where:
            - shapes are encoded as 1s
            - the rest is encoded as 0s"""
    return raster_from_shapefile(shapefile, aoi, cell_size, field).ReadAsArray()

def small_distance(path, d_max):
    """ Determines if the path is within the threshold minus an epsilon.
    
        Inputs:
        - path: coordinates used to compute the distance.
        - d_max: distance threshold.
        
        Returns False if pathlength is clearly above the distance threshold 
        (up to an epsilon)."""
    return (np.sqrt(np.sum(np.square(path))) <(d_max+0.0001))

def should_step(origin, path, d_max, raster):
    """ Returns true if the cast_ray algorithm should continue stepping.
    
        Inputs:
        - origin, path: coordinates used to get the raster position
        - d_max: maximum fetch distance
        - raster: geographic data to work with
        
        Returns True if cell is sea, within maximum fetch distance, 
        and within raster boundaries. """
    sea = False
    valid_index = (min(np.around(origin + path)) >= 0)
    try:
        sea = is_sea(np.around(origin + path), raster)
    except:
        return False

    return valid_index and sea and small_distance(path, d_max)

def cast_ray(origin, direction, d_max, raster):
    """ March from the origin towards a direction until either land or a
    maximum distance is met.
    
        Inputs:
        - origin: algorithm's starting point -- has to be on sea
        - direction: marching direction
        - d_max: maximum distance to traverse
        - raster: land mass raster
        
        Returns the distance to the origin."""
    # Initialize the origin, step and path
    O = np.around(np.array(list(origin)))
    unit_step = direction / np.fabs(direction).max()
    path = np.array([0., 0.])
    # Not on sea, no fetch
    if not is_sea(origin, raster): 
        raise NotAtSea('Not at sea, no fetch')
    # Short distance, no fetch
    if (d_max < 1.0): 
        return 0.0
    # Preliminary checks
    assert raster.size > 1      # More than 1 point
    assert(np.fabs(direction).max() > 0) # Non-zero vector
    # Step until we can't, then backtrack once
    while should_step(O, path, d_max, raster): 
        path += unit_step
    path -= unit_step

    return np.sqrt(np.sum(np.square(path)))
   
def compute_fetch(land_raster, directions, d_max):
    """ Given a land raster, return the fetch distance from a point
    in given directions 
        
        - land_raster: raster where land is encoded as 1s, sea as 0s,
            and cells outside the area of interest as anything 
            different from 0s or 1s.
        - directions: tuple of angles (in radians) from which the fetch
            will be computed for each pixel.
        - shore: numpy array the same size as land_raster encoding the shoreline
            as 1s, 0 otherwise.
        - d_max: maximum distance over which to compute the fetch
                
        
        returns:dictionary of fetch data where the key is a shore point
                coordinates, and the value is a tuple (same size as directions) 
                containing fetch distances from that point."""
    # Extract shore from raster
    shore_raster = detect_shore(land_raster)
    shore_points = np.where(shore_raster == shore())

    assert(len(shore_points[0]) > 0)
    
    # precompute directions
    direction_vectors = np.empty((len(directions), 2))

    # Raster convention: -Up is north, i.e. decreasing 'i' is towards north,
    #                       hence the minus sign in front of the cos in
    #                       direction_vectors
    # Wind convention: Wind is defined as blowing FROM and not TOWARDS. This
    #                  means that winds blowing from sector n are oriented
    #                  towards direction n + 180, wnich is the fetch direction
    for d in range(len(directions)):
        direction_vectors[d] = (round(-math.cos(directions[d]+math.pi), 10),\
                                round( math.sin(directions[d]+math.pi), 10))
    fetch = {}
    # Compute fetch for each point (i, j)
    for point in zip(shore_points[0], shore_points[1]):
        try:
            fetch[point] = ()
            for direction in direction_vectors:
                d = cast_ray(point, direction, d_max, land_raster)
                fetch[point] = fetch[point] + (d,)
        except NotAtSea:
            del(fetch[point])

    return fetch

def detect_shore(raster):
    """ Extract the boundary between land and sea from a raster.
    
        - raster: numpy array with sea, land and nodata values.
        
        returns a numpy array the same size as the input raster with the shore
        encoded as ones, and zeros everywhere else."""
    # Rich's super-short solution, which uses convolution.
    # Works if land, sea, and nodata have different values:
    assert(land()   != sea())
    assert(sea()    != nodata())
    assert(nodata() != land())
    kernel = np.array([[-1, -1, -1],
                       [-1,  8, -1],
                       [-1, -1, -1]])
    # Generate the nodata shore artifacts
    aoi = np.ones_like(raster) * sea()
    aoi[raster == nodata()] = nodata()
    negative_borders = (sp.signal.convolve2d(aoi, \
                                            kernel, \
                                            mode='same') <0 ).astype('int')
    # Generate the all the borders (including data artifacts)
    borders = (sp.signal.convolve2d(raster, \
                                 kernel, \
                                 mode='same') <0 ).astype('int')
    # Real shore = all borders - shore artifacts
    borders = ((borders - negative_borders) >0 ).astype('int') * shore()

    return borders
 
