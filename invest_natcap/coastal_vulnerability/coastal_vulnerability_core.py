#:RICH: try to avoid loading entire arrays into memory and prefer using vectorize_rasters where possible
#:RICH: pylint this and see what makes sense to do

""" Coastal vulnerability model core functions """
import os
import subprocess
import sys
import csv
import re
import math
import time 
import copy
import shutil
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
from scipy.interpolate import LinearNDInterpolator as ip
from scipy import spatial
from scipy.ndimage import measurements 
from scipy.ndimage import morphology

from osgeo import gdal
from osgeo import ogr
from osgeo import osr

import logging
from invest_natcap import raster_utils

class NotAtSea(Exception): 
    """ Exception raised by cast_ray when the point for which to compute the
    fetch is not at sea """
    pass

class NotProjected(Exception):
    """ Exception raised if trying to work on an object that needs to be
    projected """
    pass

LOGGER = logging.getLogger('coastal_vulnerability_core')
logging.basicConfig(format='%(asctime)s %(name)-15s %(levelname)-8s \
    %(message)s', level=logging.DEBUG, datefmt='%m/%d/%Y %H:%M:%S ')

SECTOR_COUNT = 16 # Number of equi-angular sectors
MAX_FETCH = 60000 # Maximum fetch distance in meters

# TODO: test this
# TODO: change this with vectorize_rasters
def adjust_dataset_ranks(input_uri, output_uri):
    """Adjust the rank of a dataset's first band using 'adjust_layer_ranks'.
    
        Inputs:
            - input_uri: the uri used to load the ranks.  :RICH:WHAT KIND OF FILE DOES IT POINT TO?
            - output_uri: the uri where the result will be stored 
            
        Output:
            - Adjust input_uri's data using 'adjust_layer_ranks', and save it
                in output_uri and return the uri.  :RICH:CAN YOU DESCRIBE 'ADJUST'?"""  
    # Open the raster
    input_raster = gdal.Open(input_uri)
    assert(input_raster)
    input_array = input_raster.GetRasterBand(1).ReadAsArray()
    unique_value = np.unique(input_array)
    # Compute adjusted rank values
    band, old_nodata, rank_data = \
        raster_utils.extract_band_and_nodata(input_raster, get_array = True)
    adjusted_ranks = adjust_layer_ranks(rank_data)
    # Compute nodata--ensure it's of compatible type
    new_nodata = raster_utils.calculate_value_not_in_array(adjusted_ranks)
    new_nodata = raster_utils.gdal_cast(new_nodata, gdal.GDT_Float32)
    # Create the new raster with appropriate data in it
    output_raster = raster_utils.new_raster_from_base(input_raster, \
        output_uri, 'GTiff', new_nodata, gdal.GDT_Byte)
    # Save the array in the raster
    band = output_raster.GetRasterBand(1)
    band.WriteArray(adjusted_ranks)
    # Done. Return the new data as a numpy array.
    return output_uri

# TODO: adapt code for vectorize_rasters
def adjust_layer_ranks(layer):
    """Adjust the rank of a layer in case there are less than 5 values.
    
        Inputs: 
            - layer: a numpy array that encodes data for a layer. :RICH:DESCRIBE THE SIZE AND TYPE OF ARRAY
            
        Output:
            - adjusted_layer: a numpy array with the adjusted rank following
              this logic: :RICH:DESCRIBE SIZE AND TYPE OF ARRAY
                - non-coastal values have a value of zero (0)
                - 1 value for all segments: all have a rank of 3
                - 2 values: - 3 for low values
                            - 4 for higher values
                - 3 values: 2, 3, and 4 by ascending level of vulnerability
                - 4 values: 2, 3, 4, and 5 by ascending level of vulnerability
    """
    # Adjusted ranks as described in Greg's notes, listed in ascending order
    # TODO: Move this to a global scope so that it's accessible from outside
    # this function
    adjusted_ranks = {}
    adjusted_ranks[0] = [1., 2., 3., 4., 5.] # Used to validate input
    adjusted_ranks[1] = [3.]
    adjusted_ranks[2] = [3., 4.]
    adjusted_ranks[3] = [2., 3., 4.]
    adjusted_ranks[4] = [2., 3., 4., 5.]
    # Find unique values, sorted in ascending order, as are the adjusted ranks
    unique_values = np.unique(layer)
    # Non coastal value has to be zero
    if unique_values[0] != 0.:
        LOGGER.warning('Non-coastal value is non-zero:')
        LOGGER.warning(unique_values)
        assert(unique_values[0] == 0.)
    # Removing non-coastal value
    unique_values = np.trim_zeros(unique_values)
    # Every element has to be valid, i.e. all elements in unique_values is in
    # adjusted_ranks
    assert(np.in1d(unique_values, adjusted_ranks[0], assume_unique =True).all())
    # If maximum number of values, no need to do anything
    if unique_values.size == len(adjusted_ranks):
        return layer
    # If less than maximum values, create the return array...
    adjusted_layer = np.copy(layer)
    # ...and replace each rank with the predefined ones in adjusted_ranks
    for value in range(unique_values.size):
        input_value = unique_values[value]
        adjusted_value = adjusted_ranks[unique_values.size][value]
        adjusted_layer[layer == input_value] = adjusted_value

    return adjusted_layer

def execute(args):
    """ Entry point for coastal vulnerability core
    
        args['foo'] - actual data structure the way I want them look like  :RICH:DESCRIBE ALL THE ARGUMENTS IN ARGS


        returns nothing"""

    #:RICH: INSTEAD OF A GIANT SINGLE FUNCTION, CAN YOU BREAK EACH OF THESE PIECES INTO ITS OWN FUNCTION, THEN EXECUTE COULD CALL IT IN THE ORDER BELOW AND BE FARILY READABLE ABOUT WHERE IT IS.  ESSENTALLY, EACH OF YOUR IF STATEMENT BODIES BELOW SHOULD HAVE ONLY THIS.  MAKES IT EASIER TO TEST AND/OR PULL APART LATER
    
    # Generation of outputs:
    #:RICH:REPLACED THE FOLLOWING IF ELSE BLOCKS WITH A SINGLE LOOP WHERE YOU LOOP OVER A LIST OF KEYWORDS
    # Indices in the layer info list
    PREFIX = 0 # String concatenated to every file generated
    NAME = 1 # Output name, used to construct other variables (output, constant)
    OUTPUT_TYPE = 2 # String used to discriminate between layer and non-layer
                    # output, but can be used for any other purpose
    PRECONDITIONS = 3 # List of keys in args required to compute a given output 
    FUNCTION = 4 # Function callback that computes a particular output

    # The preconditions field is handled the following way:
    # - Every keyword in each inner list in the preconditions is evaluated
    # - An inner list evaluates to True if all its keywords are in args
    # - The main (outer) list evaluates to true if at least one sublist is True
    # - If the precondition evaluates to true, the output is computed
    outputs_info = [ \
    ['0_', 'structures', 'non-layer',[['structures_uri']], \
        compute_structure_protection], \
    ['1_a_', 'segment exposure', 'non-layer', [['fetch_distances', \
        'fetch_depths']], compute_segment_exposure],\
    ['1_b_', 'geomorphology', 'layer', [['geomorphology']], \
        compute_geomorphology], \
    ['1_c_', 'relief', 'layer',[['relief']], compute_relief_rank], \
    ['1_d_', 'natural habitats', 'layer', [['habitats_csv_uri']], \
        compute_natural_habitats_vulnerability], \
    ['1_e_', 'wind exposure','layer',[['climatic_forcing_uri', \
        'fetch_distances']],\
        compute_wind_exposure],\
    ['1_f_', 'wave exposure', 'layer', [['climatic_forcing_uri', \
        'fetch_depths']],\
        compute_wave_exposure],\
    ['1_g_', 'surge potential', 'layer', [['bathymetry']], \
        compute_surge_potential],\
    ['1_h_', 'sea level rise', 'layer', [['sea_level_rise']], \
        compute_sea_level_rise],\
    ['1_i_', 'coastal vulnerability', 'layer', [['geomorphology'],['relief'], \
        ['surge_potential'],['sea_level_rise'],['natural_habitats'],['E_w']], \
        compute_coastal_vulnerability], \
    ['1_j_', 'coastal vulnerability no habitats', 'layer', 
        [['natural_habitats', 'coastal_vulnerability']], \
        compute_coastal_vulnerability_no_habitats],\
    ['1_k_', 'coastal vulnerability difference', 'layer', \
        [['coastal_vulnerability_no_habitats']],\
        compute_habitat_role],\
    ['1_l_', 'erosion vulnerability', 'layer', \
    [['geomorphology', 'natural_habitats', 'wave_exposure','sea_level_rise']],\
        compute_erosion_vulnerability],\
    ['1_m_', 'population map', 'non-layer', [['global_population']], \
        compute_coastal_population],\
    ['2_', 'fetch','non-layer',[['fetch_distance_uris'],['fetch_depth_uris']],\
        save_fetch_to_outputs],\
    ['3_1_', 'structure edges', 'non-layer', [['structure_edges']], \
        save_structure_to_outputs],\
    ['3_2_', 'erodible shoreline', 'non-layer', [['geomorphology']],\
        compute_erodible_shoreline],\
    ['3_3_', 'continental shelf distance', 'non-layer', \
        [['shore_shelf_distance']],\
        compute_continental_shelf_distance],\
    ['3_4_', 'surge estimate', 'non-layer', [['continental_shelf']], \
        compute_surge_estimate],\
    ['3_5_', 'wind generated waves', 'non-layer', \
        [['wave_height'], ['wave_period']], \
        save_wind_generated_waves_to_outputs],\
    ['3_6_', 'oceanic wave exposure', 'non-layer', [['E_o']], \
        save_oceanic_wave_exposure_to_outputs],\
    ['3_7_', 'local wave exposure', 'non-layer', [['E_l']], \
        save_local_wave_exposure_to_outputs], \
    ['4_', 'HTML page', 'non-layer', \
        [['coastal_vulnerability'], ['coastal_population'], ['pop_density']], \
        generate_HTML_page]]

    for index in range(len(outputs_info)):
        # All the info about this output (list)
        output = outputs_info[index]
        output_name = output[NAME]
        key = output_name.replace(' ', '_')
        # Filename in /outputs 
        output_file = output[PREFIX] + key + '.tif'
        output_path=os.path.join(args['outputs_directory'],output_file)
        # Dictionary key to the output path
        output_key = key + '_output'
        # Dictionary key to the user-defined output constant
        output_constant = key + '_constant'
        # Check preconditions and list the ones missing
        preconditions_met = np.array([ 
            np.array([keyword in args for keyword in inner_list]).all()
            for inner_list in output[PRECONDITIONS]])
        if preconditions_met.any():
            print('Processing ' + output_name + '...')
            # Compute the ranks for the output, and save in data_uri
            args['prefix'] = output[PREFIX]
            args = dict(args.items() + output[FUNCTION](args).items())
            if output[OUTPUT_TYPE] == 'layer':
                # Adjust the ranks, save in outputs and save the uri in args
                args[output_key] = \
                    adjust_dataset_ranks(args[key], output_path)
        else:
            # Preconditions are not met. Print the ones that are missing.
            missing_preconditions = []
            for inner_list in output[PRECONDITIONS]:
                for keyword in inner_list:
                    if keyword not in args:
                        missing_preconditions.append(keyword)
            print(output_name + ' not loaded. missing', missing_preconditions)
            # Assign a constant value to the entire layer if it is specified
            if output_constant in args:
                constant = int(args[output_constant])
                shore_dataset = gdal.Open(args['shore_uri'])
                shore_array = shore_dataset.GetRasterBand(1).ReadAsArray()
                print('assigning', constant, 'to ' + output_name)
                raster = raster_utils.new_raster_from_base(args['aoi_raster'], \
                    output_file, 'GTIFF', 0.0, gdal.GDT_Float32)
                raster.GetRasterBand(1).WriteArray(shore_array * constant)
                args[output_key] = output_path
            # Skip the layer entirely
            else:
                print('Skipping ' + output_name + '.')
    
    #subprocess.call(['qgis', args['shore_uri']])
       
    return

def generate_HTML_page(args):
    """ Copy local wave exposure to the outputs/ directory.
    
        Inputs:
            args['coastal_vulnerability']: uri to the coastal vulnerability data
            args['coastal_population']: uri to the coastal population data
            args['pop_density']: uri to the population density data
            args['prefix']: prefix to be appended to the new filename
            args['intermediate_directory']: directory where to save the data
            args['outputs_directory']: directory where to save the data
            
        Outputs:
            data_uri: dictionary containing the uri where the data is savedi,
            which includes the three histograms and the HTML file itself.
    """
    data_uri = {}
    prefix = args['prefix'] + 'HTML'
    data_uri['coastal_vulnerability_histogram'] = \
        os.path.join(args['intermediate_directory'],\
        prefix + '_coastal_vulnerability_histogram.png')
    data_uri['population_vulnerability_histogram'] = \
        os.path.join(args['intermediate_directory'],\
        prefix + '_population_vulnerability_histogram.png')
    data_uri['urban_vulnerability_histogram'] = \
        os.path.join(args['intermediate_directory'], \
        prefix + '_urban_vulnerability_histogram.png')
    #:RICH: say what these are for
    # Values used to create the histograms in Matplotlib
    width = 0.75    # Bin width
    ind = np.arange(5) + 1.0 - width * 0.5 # Space between bins
    bins = np.zeros(len(ind))   # Bin height container
    if 'coastal_vulnerability' in args:
        # Cell rank histogram:
        # 1- Sum all the cells with the same rank
        shore_rank_raster = gdal.Open(args['coastal_vulnerability'])    
        shore_rank = shore_rank_raster.GetRasterBand(1).ReadAsArray()
        for rank in range(len(ind)):
            bins[rank] = np.sum((shore_rank == rank+1).astype(int))
        #:RICH: JUST DESCRIBE WHAT THE MATPLOT LIB ARGUMENTS ARE DOING
        # 2- Create the figure
        fig = plt.figure()  # Get figure handle
        ax = fig.add_subplot(111) # figure array of 1row 1col, select 1st fig.
        # 3- Plot the data
        ax.bar(ind, bins, width, color = 'r')
        # 4- Adjust axes
        plt.xlabel('Vulnerability')
        plt.ylabel('Number of shore segments')
        ax.set_xlim(0.5, 5.5)   # Set the axes within an interval
        ax.grid(True)   # Add lay a grid over the graph (dotted lines)
        # 5- Save the figure to a uri
        plt.savefig(data_uri['coastal_vulnerability_histogram'])
    if 'coastal_population' in args and 'coastal_vulnerability' in args:
        # Population histogram
        # 1- Sum all the cell population with the same rank
        population_raster = gdal.Open(args['coastal_population'])
        population = population_raster.GetRasterBand(1).ReadAsArray()
        shore_rank_raster = gdal.Open(args['coastal_vulnerability'])    
        shore_rank = shore_rank_raster.GetRasterBand(1).ReadAsArray()
        for rank in range(len(ind)):
            bins[rank] = np.sum(population[shore_rank == rank+1])
        # 2- Create the figure
        fig = plt.figure()
        ax = fig.add_subplot(111)
        # 3- Plot the data
        ax.bar(ind, bins, width, color = 'r')
        # 4- Adjust axes
        plt.xlabel('Vulnerability')
        plt.ylabel('Population')
        ax.set_xlim(0.5, 5.5)
        ax.grid(True)
        # 5- Save the figure to a uri
        plt.savefig(data_uri['population_vulnerability_histogram'])
    if 'pop_density' in args and 'coastal_vulnerability' in args:
        # Urban center vulnerability histogram
        # 1- Sum all the urban centers with the same rank
        shore_rank_raster = gdal.Open(args['coastal_vulnerability'])    
        shore_rank = shore_rank_raster.GetRasterBand(1).ReadAsArray()
        pop_density_raster = gdal.Open(args['pop_density'])
        pop_density = pop_density_raster.GetRasterBand(1).ReadAsArray()
        urban_center_threshold = args['urban_center_threshold'] 
        urban_vulnerability = shore_rank[pop_density >= urban_center_threshold]
        for rank in range(len(ind)):
            bins[rank] = np.sum((urban_vulnerability == rank+1).astype(int))
        # 2- Create the figure
        fig = plt.figure()
        ax = fig.add_subplot(111)
        # 3- Plot the data
        ax.bar(ind, bins, width, color = 'r')
        # 4- Adjust axes
        plt.xlabel('Vulnerability')
        plt.ylabel('Shoreline segments close to urban centers')
        ax.set_xlim(0.5, 5.5)
        ax.grid(True)
        # 5- Save the figure to a uri
        plt.savefig(data_uri['urban_vulnerability_histogram'])
    # Creating the actual HTML page
    data_uri['run_summary'] = os.path.join(args['outputs_directory'], \
        args['prefix'] + "run_summary.html")
    try:
        #:RICH: file is a python system name, call it something else
        with open(data_uri['run_summary'], "w") as html_file:
            html_file.write("<html>")
            html_file.write("<title>" + "Marine InVEST" + "</title>")
            html_file.write("<CENTER><H1>" + "Coastal Vulnerability Model" + \
                "</H1></CENTER>")
            html_file.write("<br>")
            html_file.write("This page contains results from running \
                the Marine InVEST Coastal Vulnerability model." + "<p>")
            html_file.write("<br><br>")
            html_file.write("<HR>")
            html_file.write("<table border='1', cellpadding='5'>")
            html_file.write("<tr>")
            html_file.write("<th>Area Of Interest</th>")
            html_file.write("<th>Coastal Vulnerability Histogram</th>")
            html_file.write("</tr>")
            # Extract the layer extents
            aoi = ogr.Open(args['aoi_uri'])
            aoi_layer = aoi.GetLayer(0)
            aoi_extent = aoi_layer.GetExtent()
            # Transform the layer extents in Google map's long/lat coord. system
            lat_long_sr = osr.SpatialReference()
            lat_long_sr.SetWellKnownGeogCS("WGS84")
            aoi_sr = aoi_layer.GetSpatialRef()
            tr = osr.CoordinateTransformation(aoi_sr, lat_long_sr)
            p_min = tr.TransformPoint(aoi_extent[0], aoi_extent[2])
            p_max = tr.TransformPoint(aoi_extent[1], aoi_extent[3])
            # Compute the center, extents and zoom in Google map's coord. system
            center_x = str((p_min[0] + p_max[0]) / 2.)
            center_y = str((p_min[1] + p_max[1]) / 2.)
            extent_x = p_max[0] - p_min[0]
            extent_y = p_max[1] - p_min[1]
            max_extent = max(extent_x, extent_y)
            fraction = 360.0 / max_extent
            zoom = max(int(math.log(fraction, 2.)) - 1, 0)
            html_file.write("<tr>")
            html_file.write('<td><img \
                src=http://maps.googleapis.com/maps/api/staticmap' +
                '?center=' + center_y + ',' + center_x + \
                '&zoom=' + str(zoom) + \
                '&size=400x400' + \
                '&sensor=false ' + \
                'alt="Area Of Interest"/></td>')
            html_file.write("The site is located at:")
            html_file.write("-latitude:" + str(center_y))
            html_file.write("-longitude:" + str(center_x))
            html_file.write('<td><img src="' + os.path.join('../..', \
                data_uri['coastal_vulnerability_histogram']) + 
                '" alt="Coastal Vulnerability Histogram"' + \
                'width = "400"' + \
                '></img></td>')
            html_file.write("</tr>")
            html_file.write("<tr>")
            html_file.write("<th>Population Histogram</th>")
            html_file.write("<th>Urban Center Vulnerability Histogram</th>")
            html_file.write("</tr>")
            html_file.write("<tr>")
            html_file.write('<td><img src="' + os.path.join('../..', \
                data_uri['population_vulnerability_histogram']) + 
                '" alt="population vulnerability histogram"' + \
                'width = "400"' + \
                '></img></td>')
            html_file.write('<td><img src="' + os.path.join('../..', \
                data_uri['urban_vulnerability_histogram']) + 
                '" alt="urban center vulnerability histogram"' + \
                'width = "400"' + \
                '></img></td>')
            html_file.write("</tr>")
            html_file.write("</table>")
            #end page
            html_file.write("</html>")
    #:RICH: specify the exception type here so it doesn't catch everything
    except IOError:
        print('Failed to open the HTML output file!')
    return data_uri

def save_local_wave_exposure_to_outputs(args):
    """ Copy local wave exposure to the outputs/ directory.
    
        Inputs:
            args['E_l']: uri to the local wave exposure data
            args['prefix']: prefix to be appended to the new filename
            args['outputs_directory']: directory where to save the data
            
        Outputs:
            data_uri: dictionary containing the uri where the data is saved
    """
    data_uri = {}
    data_uri['local_wave_exposure'] = \
        os.path.join(args['outputs_directory'], \
        args['prefix'] + 'local_wave_exposure.tif')
    shutil.copy(args['E_l'], data_uri['local_wave_exposure'])
    return data_uri

def save_oceanic_wave_exposure_to_outputs(args):
    """ Copy oceanic wave exposure to the outputs/ directory.
    
        Inputs:
            args['E_o']: uri to the oceanic wave exposure data
            args['prefix']: prefix to be appended to the new filename
            args['outputs_directory']: directory where to save the data
            
        Outputs:
            data_uri: dictionary containing the uri where the data is saved
    """
    data_uri = {}
    data_uri['oceanic_wave_exposure'] = \
        os.path.join(args['outputs_directory'], \
        args['prefix'] + 'oceanic_wave_exposure.tif')
    shutil.copy(args['E_o'], data_uri['oceanic_wave_exposure'])
    return data_uri

def save_wind_generated_waves_to_outputs(args):
    """ Copy the wave height and wave period to the outputs/ directory.
    
        Inputs:
            args['wave_height'][sector]: uri to "sector"'s wave height data
            args['wave_period'][sector]: uri to "sector"'s wave period data
            args['prefix']: prefix to be appended to the new filename
            args['outputs_directory']: directory where to save the data
            
        Outputs:
            data_uri: dictionary containing the uri where the data is saved
    """
    data_uri = {}
    data_uri['wave_height_output'] = {}
    data_uri['wave_period_output'] = {}
    for sector in args['wave_height']:
        data_uri['wave_height_output'][sector] = \
            os.path.join(args['outputs_directory'], \
            args['prefix'] + 'wave_height_' + sector + '.tif')
        shutil.copy(args['wave_height'][sector], \
            data_uri['wave_height_output'][sector])
        data_uri['wave_period_output'][sector] = \
            os.path.join(args['outputs_directory'], \
            args['prefix'] + 'wave_period_' + sector + '.tif')
        shutil.copy(args['wave_period'][sector], \
            data_uri['wave_period_output'][sector])
    return data_uri

def compute_continental_shelf_distance(args):
    """ Copy the continental shelf distance data to the outputs/ directory.

        Inputs:
            args['shore_shelf_distance']: uri to the continental shelf distance
            args['prefix']:
            args['outputs_directory']:
    
        Outputs:
            data_uri: a dictionary containing the uri where the data is saved.
    """
    data_uri = {}
    data_uri['continental_shelf_distance'] = \
        os.path.join(args['outputs_directory'], \
        args['prefix'] + 'continental_shelf_distance.tif')
    shutil.copy(args['shore_shelf_distance'], \
        data_uri['continental_shelf_distance'])
    return data_uri

def compute_erodible_shoreline(args):
    """Compute the erodible shoreline as described in Greg's notes.
        The erodible shoreline is the shoreline segments of rank 5.
        
        Inputs: 
            args[geomorphology]: the geomorphology data.
            args['prefix']: prefix to be added to the new filename.
            args['outputs_directory']: the directory where to save the data
            args['aoi']: area of interest for the data
            args['cell_size']: size of a cell on the raster
            
        Outputs:
            data_uri: a dictionary containing the uri where the data is saved.
    """
    save_output = \
        set_save_info(args['outputs_directory'], args['aoi'],
        args['cell_size'], gdal_type = gdal.GDT_Byte)
    data_uri = {}
    geomorphology_raster = gdal.Open(args['geomorphology'])
    geomorphology_array=geomorphology_raster.GetRasterBand(1).ReadAsArray()
    # TODO: replace by vectorize_rasters
    #:RICH: describe this algorithm briefly
    # The erodible shoreline is defined as the shoreline segments that have a
    # rank of 5.
    geomorphology_array[geomorphology_array != 5] = 0 # Mask all non-5 segments
    geomorphology_array[geomorphology_array == 5] = 1 # Keep the rest
    data_uri['erodible_shoreline'] = \
    save_output(geomorphology_array, args['prefix'] + 'erodible_shoreline.tif')
    return data_uri

def save_structure_to_outputs(args):
    """ Save structure data to the outputs directory, under a custom prefix.
        
        Inputs: 
            args['structure_edges']: the data's uri to save to /outputs
            args['prefix']: prefix to add to the new filename. Currently used to
                mirror the labeling of outputs in Greg's notes.
            args['outputs_directory']: the directory where to save the data

        Outputs:
            data_uri: a dictionary of the uri where the data has been saved.
    """
    data_uri = {}
    data_uri['structure_edges_output'] = \
        os.path.join(args['outputs_directory'], args['prefix'] + \
        'structure_edges.tif')
    shutil.copy(args['structure_edges'], data_uri['structure_edges_output'])
    return data_uri

def save_fetch_to_outputs(args):
    """ Function that copies the fetch information (depth and distances)
        in the outputs directory.
        
        Inputs: 
            args['fetch_distance_uris']: A dictionary of ('string':string)
                entries where the first string is the sector in degrees, and
                the second string is a uri pointing to the file that contains
                the fetch distances for this sector.
            args['fetch_depths_uris']: A dictionary similar to the depth one,
                but the second string is pointing to the file that contains
                fetch depths, not distances.
            args['prefix']: String appended before the filenames. Currently
                used to follow Greg's output labelling scheme.

        Outputs:
            - data_uri that contains the uri of the new files in the outputs 
                directory, one for fetch distance and one for fetch depths for 
                each fetch direction 'n', for a total of 2n.
            """
    data_uri = {}
    prefix = args['prefix']
    fetch_distance_uris = args['fetch_distance_uris']
    for sector in fetch_distance_uris:
        destination = os.path.join(args['outputs_directory'], \
            prefix + 'fetch_distance_' + sector + '.tif')
        shutil.copy(fetch_distance_uris[sector], destination)
        data_uri['fetch_distance_' + sector] = destination
    fetch_depth_uris = args['fetch_depth_uris']
    for sector in fetch_depth_uris:
        destination = os.path.join(args['outputs_directory'], \
            prefix + 'fetch_depth_' + sector + '.tif')
        shutil.copy(fetch_depth_uris[sector], destination)
        data_uri['fetch_depth_' + sector] = destination
    return data_uri

def compute_erosion_vulnerability(args):
    outputs_directory = args['outputs_directory']
    intermediate_directory = args['intermediate_directory']
    save_intermediate = \
        set_save_info(intermediate_directory,args['aoi'],args['cell_size'])

    prefix = args['prefix'] + 'EROSION'

    data_uri = {}

    erosion_vulnerability_layers = ['geomorphology', 
                                    'habitats', 
                                    'E_w', 
                                    'sea_level_rise']
    # TODO: replace this with vectorize_rasters
    # Build 'n', the list of available layers
    n = {}
    for layer in erosion_vulnerability_layers:
        if layer in args:
            n[layer] = args[layer]
    R = set_raster_uri_dictionary(n)
    EV = np.power(product([R(k) for k in n]), 1./len(n))
    data_uri['erosion_map_float'] = \
        save_intermediate(EV, prefix + '_1_float.tif')
    EV = np.around(EV).astype(int)
    data_uri['erosion_map_int'] = \
        save_intermediate(EV, prefix + '_2_int.tif')
    source = data_uri['erosion_map_int']
    if 'structure_edges' in args:
        # Reduce shoreline segments next to structure by 1
        structure_edge_raster = gdal.Open(args['structure_edges'])
        data_uri['erosion_map_structures'] = os.path.join(
        intermediate_directory, prefix + '_3_with_structure_edges.tif')
        
        shore_raster = gdal.Open(args['shore_uri'])
        erosion_vulnerability_raster = gdal.Open(data_uri['erosion_map_int'])

        # TODO: Remove duplicates of this function
        def adjust_for_structure(nodata):
            def compute(shore, structure, rank):
                """ Reduce the vulnerability index by 1 at the strucutre edges"""
                if shore:
                    return max(0, rank-structure)
                else:
                    return nodata
            return compute

        no_data = np.finfo(np.float32).min
        adjust_structure_op = adjust_for_structure(no_data)
        dataset_list = \
            [shore_raster, structure_edge_raster, erosion_vulnerability_raster]
        EV_struct = raster_utils.vectorize_rasters(dataset_list,
            adjust_structure_op, 
            raster_out_uri = data_uri['erosion_map_structures'], \
            datatype=gdal.GDT_Byte, nodata=no_data)

        # Adjust destination
        source = data_uri['erosion_map_structures']
    # Save to file
    data_uri['erosion_vulnerability'] = \
    adjust_dataset_ranks(source, \
    os.path.join(intermediate_directory, args['prefix'] + 'erosion_map.tif'))
    return data_uri

def compute_habitat_role(args):
    outputs_directory = args['outputs_directory']
    intermediate_directory = args['intermediate_directory']
    save_intermediate = \
        set_save_info(intermediate_directory,args['aoi'],args['cell_size'])

    prefix = args['prefix'] + 'CV_difference'

    data_uri = {}

    shore_raster = gdal.Open(args['shore_uri'])
    R_no_hab_raster = gdal.Open(args['coastal_vulnerability_no_habitats'])
    R_hab_raster = gdal.Open(args['natural_habitats'])
    data_uri['CV_difference_float'] = os.path.join(intermediate_directory, \
        prefix + '_1_float.tif')


    def habitat_role(nodata):
        def compute(shore, R_hab, R_no_hab):
            """ Reduce the vulnerability index by 1 at the strucutre edges"""
            if shore:
                return (5. - R_hab) * R_no_hab / 4.
            else:
                return nodata
        return compute

    no_data = np.finfo(np.float32).min
    habitat_role_op = habitat_role(no_data)
    dataset_list = \
        [shore_raster, R_hab_raster, R_no_hab_raster]
    hab_role_map_float = \
        raster_utils.vectorize_rasters(dataset_list, habitat_role_op, 
        raster_out_uri = data_uri['CV_difference_float'], \
        datatype=gdal.GDT_Float32, nodata=no_data)

    data_uri['CV_difference_int'] = os.path.join(intermediate_directory, \
        prefix + '_2_int.tif')

    dataset_list = [hab_role_map_float]
    hab_role_map_int = \
        raster_utils.vectorize_rasters(dataset_list, lambda x: x, 
        raster_out_uri = data_uri['CV_difference_int'], \
        datatype=gdal.GDT_Byte, nodata=no_data)

    data_uri['coastal_vulnerability_difference'] = \
        adjust_dataset_ranks(data_uri['CV_difference_int'], \
        os.path.join(outputs_directory,args['prefix']+'coastal_vulnerability_difference.tif'))
    return data_uri

def compute_coastal_vulnerability_no_habitats(args):
    outputs_directory = args['outputs_directory']
    intermediate_directory = args['intermediate_directory']
    save_intermediate = \
        set_save_info(intermediate_directory,args['aoi'],args['cell_size'])

    prefix = args['prefix'] + 'CV_no_hab'

    data_uri = {}

    n = {}  # List of available layers, as in the supplementary notes.
    # Build 'n', the list of available layers
    # TODO: Get the layers from layer_info
    layer_names = [ 'geomorphology', \
                    'relief', \
                    'surge_potential', \
                    'sea_level_rise', \
                    'E_w']
    for layer in layer_names:
        if layer in args:
            n[layer] = args[layer]

    shore_raster = gdal.Open(args['shore_uri'])
    habitats_raster = gdal.Open(args['natural_habitats'])
    habitats = habitats_raster.GetRasterBand(1).ReadAsArray()
    #:RICH:WHY 5?
    # Load habitats layer and replace it with rank 5, as discussed in the 2012 
    # marine meeting.
    # TODO: replace by vectorize_rasters
    habitats_uri = args['natural_habitats']
    habitats_raster = gdal.Open(habitats_uri)
    habitats = habitats_raster.GetRasterBand(1).ReadAsArray()
    habitats[habitats > 0] = 5 
    #:RICH: CAN YOU SAY WHAT CVNH IS, OR REFERENCE USER'S GUIDE?
    R = set_raster_uri_dictionary(n)
    CV_no_hab = np.power(habitats * product([R(k) for k in n]), 1./(len(n) +1))
    data_uri['CV_no_habitat_float'] = \
        save_intermediate(CV_no_hab, prefix + '_1_float.tif')
    #rounding CVNH to closest integer
    # TODO: replace by vectorize_rasters
    CV_no_hab = (CV_no_hab + 0.5).astype(int)
    data_uri['CV_no_habitat_int'] = \
        save_intermediate(CV_no_hab, prefix + '_2_int.tif')
    source = data_uri['CV_no_habitat_int']
    if 'structure_edges' in args:
        # The structure edges data is available, so we can decrement the rank
        # of the segments next to edges by 1, as mentioned in Greg's additional
        # notes.
        structure_edge_raster = gdal.Open(args['structure_edges'])
        CV_no_hab_raster = gdal.Open(data_uri['CV_no_habitat_int'])
        data_uri['CV_no_habitat_structures'] = os.path.join( \
            intermediate_directory, prefix + '_3_with_structure_edges.tif')
        #structure_edge=structure_edge_raster.GetRasterBand(1).ReadAsArray()

        def adjust_for_structure(nodata):
            def compute(shore, structure, CV):
                """ Reduce the vulnerability index by 1 at the strucutre edges"""
                if shore:
                    return max(0, CV-structure)
                else:
                    return nodata
            return compute

        no_data = np.finfo(np.float32).min
        adjust_structure_op = adjust_for_structure(no_data)
        dataset_list = \
            [shore_raster, structure_edge_raster, CV_no_hab_raster]
        CV_no_hab_struct = raster_utils.vectorize_rasters(dataset_list,
            adjust_structure_op, 
            raster_out_uri = data_uri['CV_no_habitat_structures'], \
            datatype=gdal.GDT_Byte, nodata=no_data)

        ##:RICH:DESCRIBE THE ALGOIRHTM BELOW BRIEFLY
        ## 1- Reduce the coast vulnerability (CV_no_hab) by 1 using 
        ## structures_edge information. 
        ## 2- Ensure the shore minimum rank is 1.
        ## 3- Assign 0 to the rest (~shore).
        #shore = CV_no_hab > 0
        #CV_no_hab -= structure_edge # Step 1
        #CV_no_hab[CV_no_hab  < 1] = 1  # Step 2
        #CV_no_hab[~shore] = 0. # Step 3
        ## Save to file
        # Adjust destination
        source = data_uri['CV_no_habitat_structures']
    # Save to file
    data_uri['coastal_vulnerability_no_habitats'] = \
    adjust_dataset_ranks(source, \
    os.path.join(intermediate_directory, args['prefix'] +'CV_no_habitat.tif'))
    return data_uri

def compute_coastal_vulnerability(args):
    outputs_directory = args['outputs_directory']
    intermediate_directory = args['intermediate_directory']
    save_intermediate = \
        set_save_info(intermediate_directory,args['aoi'],args['cell_size'])

    prefix = args['prefix'] + 'CV'
    data_uri = {}

    n = {}  # List of available layers, as in the supplementary notes.
    # Build 'n', the list of available layers
    # TODO: Get the layers from layer_info
    layer_names = [ 'geomorphology', \
                    'relief', \
                    'surge_potential', \
                    'sea_level_rise', \
                    'habitats', \
                    'E_w']
    for layer in layer_names:
        if layer in args:
            n[layer] = args[layer]

    #:RICH:WHAT'S R, IT'S VERY HARD TO SEARCH FOR... MMAYBE THIS WILL BE OBVIOUS AS IT IS ABSTRACTED INTO A FUNCTION
    # TODO: replace by vectorize_rasters
    R = set_raster_uri_dictionary(n)
    # Equation (2) in the supplementary notes...
    # ...using a generator expression
    R_segments = np.power(product([R(k) for k in n]), 1./len(n))
    # Saving data
    data_uri['coastal vulnerability float'] = \
    save_intermediate(R_segments, prefix + '_coastal_vulnerability_float.tif')
    # Round ranks to closest integer
    R_segments = np.around(R_segments).astype(int)
    # Saving data
    data_uri['coastal vulnerability int'] = \
    save_intermediate(R_segments, prefix + '_coastal_vulnerability_int.tif')
    # Adjust for structures if structure data exists 
    source = os.path.join(intermediate_directory, \
        prefix + '_coastal_vulnerability_int.tif')
    if 'structure_edges' in data_uri:
        # TODO: replace by vectorize_rasters
        # Decrement the value of shoreline segments next to structure edges
        structure_edge_raster = gdal.Open(data_uri['structure_edges'])
        structure_edge=structure_edge_raster.GetRasterBand(1).ReadAsArray()
        #:RICH:CAN YOU DESCRIBE THE algorithm below?
        shore = R_segments > 0  # Shore is the set of all positive segments
        R_segments -= structure_edge # Decrement segment value near structures  
        R_segments[R_segments  < 1] = 1  # ranking has to be at least 1
        R_segments[~shore] = 0. # non-shore segments have to be 0 (convention)
        # Save to file
        data_uri['coastal_vulnerability_structures'] = \
        save_intermediate(R_segments, \
        prefix + '_coastal_vulnerability_structures_adj.tif')
        # Adjust the source
        source = data_uri['coastal vulnerability structures']
    # Reclassify ranks if less than 5 values
    data_uri['coastal_vulnerability'] = \
    adjust_dataset_ranks(source, \
    os.path.join(outputs_directory,args['prefix']+'coastal_vulnerability.tif'))
    return data_uri

def compute_segment_exposure(args):
    """ Compute exposed and sheltered shoreline segment map."""
    no_data = -1.0
    intermediate_directory = args['intermediate_directory']
    outputs_directory = args['outputs_directory']
    aoi = ogr.Open(args['aoi_uri'])
    save_output = set_save_info(outputs_directory, aoi, args['cell_size'],
    gdal_type = gdal.GDT_Int32)
    save_intermediate = \
        set_save_info(intermediate_directory, aoi, args['cell_size'], no_data)

    data_uri = {}
    fetch_distances = args['fetch_distances']
    fetch_depths = args['fetch_depths']
    max_fetch = args['max_fetch']
    depth_threshold = args['depth_threshold']
    prefix = args['prefix'] + 'SEGMENT_EXPOSURE'

    shore_raster = gdal.Open(args['shore_uri'])
    shore_array = shore_raster.GetRasterBand(1).ReadAsArray()
    # Number (count) of enclosed/shallow rays
    depth_count = np.ones_like(shore_array) * no_data
    distance_count = np.ones_like(shore_array) * no_data
    # Percentage of enclosed/shallow rays
    depth_pct = np.ones_like(shore_array) * no_data
    distance_pct = np.ones_like(shore_array) * no_data
    # Boolean value of enclosed/shallow criteria for each segment
    shallow_segments = np.ones_like(shore_array) * no_data
    enclosed_segments = np.ones_like(shore_array) * no_data

    # Compute the exposure threshold and round it to the closest integer
    sector_count = SECTOR_COUNT
    exposure_proportion = args['exposure_proportion']

#    print('intermediate_directory', args['intermediate_directory'])
#    print('outputs_directory', args['outputs_directory'])
#    print('aoi_uri', args['aoi_uri'])
#    print('cell_size', args['cell_size'])
#    print('max_fetch', args['max_fetch'])
#    print('depth_threshold', args['depth_threshold'])
#    print('prefix', args['prefix'])
#    print('shore_uri', args['shore_uri'])
#    print('exposure_proportion', args['exposure_proportion'])
#    print('sector_count', SECTOR_COUNT)
#    shore_uri = args['shore_uri']
#    _prefix = args['prefix']
#    area_computed = args['area_computed']
#    args = {}
#    args['shore_uri'] = shore_uri
#    args['prefix'] = _prefix
#    args['area_computed'] = area_computed
    # Determine if each shoreline segment is enclosed or shallow 
    # and assign the segment 3 if sheltered, or 4 if not:
    SHELTERED_SHORE = 3
    EXPOSED_SHORE = 4
    shore_segments = fetch_distances.keys()
    shore_exposure = np.copy(shore_array)
    # Each segment has a numpy array for the fetch distances and a numpy array
    # for the fetch depth
    for segment in shore_segments:
        distances = fetch_distances[segment]
        depths = fetch_depths[segment] * -1 # make depths positive
        
        # Remove rays that reach land on the first pixel:
        non_zero_rays = np.where(distances >= args['cell_size'])
        non_zero_rays_count = non_zero_rays[0].size
        count_threshold = int(round(exposure_proportion * non_zero_rays_count))
        distances = distances[non_zero_rays]
        depths = depths[non_zero_rays]

        short_rays = np.sum((distances < max_fetch).astype(int))
        #print('max fetch', max_fetch, 'distances', distances, 'short rays', short_rays)
        shallow_rays = np.sum((depths < depth_threshold).astype(int))
        #print('depth T', depth_threshold, 'depths', depths, 'shallow rays',
        #shallow_rays)
        
        is_segment_protected = short_rays >= count_threshold
        is_segment_shallow = shallow_rays >= count_threshold

        # Avoid division by zero
        if non_zero_rays_count == 0:
            distance_count[segment] = 0
            distance_pct[segment] = 1.
            enclosed_segments[segment] = 1
            depth_count[segment] = 0
            depth_pct[segment] = 1.
            shallow_segments[segment] = 1
        else:
            distance_count[segment] = short_rays
            distance_pct[segment] = \
                float(short_rays) / non_zero_rays_count
            enclosed_segments[segment] = is_segment_protected
            depth_count[segment] = shallow_rays
            depth_pct[segment] = \
                float(shallow_rays) / non_zero_rays_count
            shallow_segments[segment] = is_segment_shallow

        # segment is sheltered
        if is_segment_protected or is_segment_shallow:
            shore_exposure[segment] = SHELTERED_SHORE
        # segment is not sheltered
        else:
            shore_exposure[segment] = EXPOSED_SHORE
    
    data_uri['short_fetch_count'] = \
        save_intermediate(distance_count, \
        prefix + '_1_enclosed_segments.tif', False)
    
    data_uri['short_fetch_pct'] = \
        save_intermediate(distance_pct, \
        prefix + '_2_enclosed_segments_pct.tif', False)

    data_uri['enclosed_segments'] = \
        save_intermediate(enclosed_segments, \
        prefix + '_3_enclosed_segments_bool.tif', False)

    data_uri['shallow_fetch'] = \
        save_intermediate(depth_count, \
        prefix + '_4_shallow_segments.tif', False)
    
    data_uri['shallow_fetch_pct'] = \
        save_intermediate(depth_pct, \
        prefix + '_5_shallow_segments_pct.tif', False)
    
    data_uri['shallow_segments'] = \
        save_intermediate(depth_pct, \
        prefix + '_6_shallow_segments_bool.tif', False)
    
    data_uri['shore_exposure'] = \
        save_output(shore_exposure, args['prefix'] + 'shore_exposure.tif')
    # Mask the shoreline for which we don't want an output
    if args['area_computed'] == 'sheltered':
        shore_array[shore_exposure == EXPOSED_SHORE] = 0
    # Save the new shore over which all the computation will now take place
    shore_raster = gdal.Open(args['shore_uri'], gdal.GA_Update)
    shore_raster.GetRasterBand(1).WriteArray(shore_array)

    return data_uri 

#:RICH: instead of functions that only return constants, prefer global variables like  SEA = 1 with a comment describing what they are.
def sea():
    """ Return the code for a sea cell """
    return 0

def land():
    """ Return the code for a land cell """
    return 1

def nodata():
    """ Return the code for a nodata cell """
    return -1.0

def shore():
    """ Return the code for a shore cell """
    return 1

def is_sea(position, raster):
    """ Determine whether the cell is at sea or not.

        - position: current cell position
        - raster: array used to determine the cell value
                
        Returns True if the cell is at sea, False otrherwise."""
    return (raster[position[0], position[1]] == sea())

def is_land(position, raster):
    """ Determine whether the cell is on land or not. 

        - position: current cell position
        - raster: raster where the cell value is stored
                
        Returns True if the cell is at sea, False otrherwise."""
    return (raster[position[0], position[1]] == land())


#:RICH:I'D PREFER A CLASS AND SINGLETON FOR THIS RATHER THAN A GLOBAL COUNTER INT
# TODO: write a unit test for this function
GLOBAL_COUNTER = 0   # global value used to keep track of counts
def unique_value():
    """Return a unique integer that can be used as a chronological indicator.
        
        Input: nothing.
        
        Output: an integer extracted from time.clock(), modified if identical
            to the previous value."""
    # We're going to use this value to keep track of our unique value
    global GLOBAL_COUNTER
    # Given a .01s resolution, we assume we won't create more than 10^6 files/s
    current_value = int(time.clock()) * 10000
    # If more than 1 file is created within the time resolution, increment
    if current_value == GLOBAL_COUNTER: current_value += 1
    # Update global counter, so that we don't return the same value next time
    GLOBAL_COUNTER = current_value
    return current_value

# TODO: comment this AND :RICH:DOCSTRING
#:RICH:WHAT IS THIS FOR?
def set_raster_uri_dictionary(raster_uri_dict):
    def array_from_raster_uri(key):
        raster_uri = raster_uri_dict[key]
        dataset = gdal.Open(raster_uri)
        return dataset.GetRasterBand(1).ReadAsArray()
    return array_from_raster_uri

# TODO: comment this AND :RICH:DOCSTRING
def product(matrix_list):
    result = matrix_list[0]
    for n in range(len(matrix_list) -1):
        result *= matrix_list[n+1]
    return result

def compute_structure_protection(args):
    """Compute the structure influence on the shore to later include it in the
    computation of the layers final rankings, as is specified in Gregg's 
    the additional notes (decrement ranks around structure edges).
    
        Inputs: 
            - args['aoi_uri']: string uri to the datasource of the area of 
                interest
            - args['shore_uri']: dataset uri of the coastline within the AOI
            - args['structures_uri']: string of the structure datasource uri
            - args['cell_size']: integer of the size of a pixel in meters
            - args['intermediate_directory']: string of the uri where 
                intermediate files are stored
            - args['prefix']: string prefix appended to every intermediate file

        Outputs:
            - data_uri: a dictionary of the file uris generated in the
              intermediate directory.
            - data_uri['adjusted_structures']: string of the dataset uri
              obtained from reprojecting args['structures_uri'] and burining it 
              onto the aoi. Contains the structure information across the whole
              aoi.
            - data_uri['shore_structures']: string uri pointing to the
              structure information along the coast only.
            - data_uri['structure_influence']: string uri pointing to a
              datasource of the spatial influence of the structures.
            - data_uri['structure_edge']: string uri pointing to the datasource
              of the edges of the structures.
            """
    aoi = ogr.Open(args['aoi_uri'])
    shore_raster = gdal.Open(args['shore_uri'])
    structures = ogr.Open(args['structures_uri'])
    cell_size = args['cell_size']
    intermediate_directory = args['intermediate_directory']
    prefix = args['prefix'] + 'STRUCT'
    file_count = 0
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    shore_array = shore_raster.GetRasterBand(1).ReadAsArray()

    # Reproject, clip the data to the AOI
    adjusted_structures_uri = \
        adjust_shapefile_to_aoi(structures, aoi, prefix, intermediate_directory)
    data_uri['adjusted_structures_datasource'] = adjusted_structures_uri 
    adjusted_structures_datasource = ogr.Open(adjusted_structures_uri)
    file_count +=1
    data_uri['adjusted_structures'] = os.path.join(intermediate_directory, \
        prefix + '_' + str(file_count) + '_adjusted_structures.tif')
    adjusted_structures = \
        raster_from_shapefile(adjusted_structures_datasource, aoi, cell_size, \
        data_uri['adjusted_structures'])
    # Get the structures on the shore:
    data_uri['shore_structures'] = os.path.join(intermediate_directory, \
        prefix + '_' + str(file_count) + '_shore_structures.tif')
    file_count +=1
    shore_structures = \
        raster_utils.vectorize_rasters([adjusted_structures, shore_raster], \
        lambda x,y: x*y, aoi, data_uri['shore_structures'], \
        gdal.GDT_Byte, 0)
    # Create a kernel to detect the structure edges:
    #   - values >= 9: the shore is armored
    #   - values between 1 and 8: more vulnerable shore segment
    #   - values == 0: segment not influenced by a structure
    kernel = np.array([[-1, -1, -1],
                       [-1, -9, -1],
                       [-1, -1, -1]])
    # Compute the structure edge using convolution:
    shore_structures_array = shore_structures.GetRasterBand(1).ReadAsArray()
    structure_influence = sp.signal.convolve2d(shore_structures_array, \
        kernel, mode='same').astype('int')
    data_uri['structure_influence'] = save_intermediate(structure_influence, \
        prefix + '_' + str(file_count) + '_structure_influence.tif')
    file_count +=1
    ## Adjust shore classification from the convolution:
    ##   - armored shore segment: 2 (convolved values -9 to -17) 
    ##   - near structure shore segment: 1 (convolved values -1 to -8)
    ##   - off structure shore segment: 0 (convolved value of 0)
    #structure_influence[structure_influence <= -9] = 2
    #structure_influence[structure_influence < 0] = 1
    ## Save the shore segments that are bordered by a structure:
    #structure_edge = np.zeros_like(structure_influence)
    #structure_edge[structure_influence == 1] = 1
    #structure_edge = structure_edge * shore_array
    
    data_uri['structure_edges'] = os.path.join(intermediate_directory, \
        prefix + '_' + str(file_count) + '_structure_edges.tif')

    def compute_structure_edge(influence, shore):
        # Not on the shore: return nodata
        if shore == 0:
            return 0
        # On the shore, investigate further
        else:
            # On the structure edge: return 1
            if (influence < 0) and (influence > -9):
                return 1
            # Not on the structure edge: return nodata
            else:
                return 0

    structure_influence_raster = gdal.Open(data_uri['structure_influence'])
    shore_structures = \
        raster_utils.vectorize_rasters([structure_influence_raster, \
        shore_raster], compute_structure_edge,aoi,data_uri['structure_edges'], \
        gdal.GDT_Byte, 0)

    return data_uri

# TODO: update docstrings
def compute_coastal_population(args):
    """Compute population living along the shore within a given radius.
    
        Inputs:
            - args['aoi']: area of interest within which to save the data
            - args['shore_uri']: uri to coastline dataset within the AOI where
              coastal segments are 1s and everything else 0s.
            - args['cell_size']: size of a pixel in meters
            - args['intermediate_directory']: uri to a directory where 
              intermediate files are stored
            - args['global_population_uri']: uri to the global population 
              density dataset.
            - args['population_radius']: used to compute the population density.

        Outputs:
            - Return a uri dictionary of all the files created to generate the
              population density along the coastline

        Intermediate outputs:
            - resampled_global_population.tif: generated only if the original
                global population file has a different resolution than cell_size
            - clipped_global_population.tif: raster data clipped to the AOI
            - pop_density.tif: raster where each point indicates the total 
                population living within 'population_radius' from that point
            - coastal_population.tif: same as pop_density, but only for points
                along the shore."""
    aoi = args['aoi']
    shore_raster = gdal.Open(args['shore_uri'])
    shore_array = shore_raster.GetRasterBand(1).ReadAsArray()
    cell_size = args['cell_size']
    intermediate_directory = args['intermediate_directory']
    outputs_directory = args['outputs_directory']
    prefix = args['prefix'] + 'COASTAL_POP'
    file_count = 0
    global_population = gdal.Open(args['global_population_uri'])
    population_radius = args['population_radius']
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 

    # TODO: replace this with adjust_shapefile_to_aoi
    # The aoi should be projected
    srs = osr.SpatialReference()
    srs.ImportFromWkt(shapefile_wkt_projection(aoi))
    assert(srs.IsProjected())
    raster = global_population
    # Try to resample the raster to cell_size
    srs = osr.SpatialReference()
    srs.ImportFromWkt(raster_wkt_projection(raster))
    if srs.IsProjected():
        pixel_size = raster_utils.pixel_size(raster)
        if not pixel_size == cell_size:
            data_uri['resampled_global_population'] = \
            os.path.join(intermediate_directory, \
            prefix + '_' + str(file_count) +'_resampled_global_population.tif')
            file_count +=1
            # TODO: Use resample_dataset instead
            raster=raster_utils.reproject_dataset(raster,\
            cell_size, raster.GetProjection(), \
            data_uri['resampled_global_population'])
    # Test that the projections are identical
    projections = [raster_wkt_projection(raster), 
                   shapefile_wkt_projection(aoi)]
    projected_aoi = aoi
    if not projections_match(projections):
        raster_wkt = raster_wkt_projection(raster)
        data_uri['global_population_reprojected_aoi'] = \
            os.path.join(intermediate_directory, prefix+'_'+str(file_count)+\
            '_global_population_reprojected_aoi.shp')
        file_count +=1
        projected_aoi = raster_utils.reproject_datasource(aoi, raster_wkt, \
            data_uri['global_population_reprojected_aoi'])
    # Clip the raster to the AOI
    data_uri['clipped_global_population'] = \
        os.path.join(intermediate_directory, \
        prefix + '_' + str(file_count) + '_clipped_global_population.tif')
    file_count +=1
    raster = raster_utils.clip_dataset(raster, projected_aoi, \
        data_uri['clipped_global_population'])
    # Resample or reproject population raster back to AOI's projection.
    pixel_size = raster_utils.pixel_size(raster)
    if not projections_match(projections): 
        print('Projections don\'t match, reprojecting dataset to AOI')
        data_uri['reprojected_global_population'] = \
        os.path.join(intermediate_directory, \
        prefix + '_' + str(file_count) + '_reprojected_global_population.tif')
        file_count +=1
        aoi_wkt = shapefile_wkt_projection(aoi)
        raster = raster_utils.reproject_dataset(raster, cell_size, aoi_wkt, \
            data_uri['reprojected_global_population'], gdal.GDT_Int32)
    elif not pixel_size == cell_size:
        data_uri['resampled_global_population'] = \
        os.path.join(intermediate_directory, \
        prefix + '_' + str(file_count) + '_resampled_global_population.tif')
        aoi_wkt = shapefile_wkt_projection(aoi)
        raster = raster_utils.reproject_dataset(raster, cell_size, aoi_wkt, \
            data_uri['resampled_global_population'])
    # Adjust the raster size if its shape has changed (due to projections)
    aoi_uri = os.path.join(intermediate_directory, \
            prefix + '_' + str(file_count) + '_aoi.tif')
    file_count +=1
    data_uri['coastal_population_aoi'] = aoi_uri
    aoi_array = array_from_shapefile(aoi, aoi, cell_size, aoi_uri)
    if not aoi_array.shape == raster.GetRasterBand(1).ReadAsArray().shape:
        clipped_population_uri = \
                os.path.join(intermediate_directory, \
                prefix + '_' + str(file_count) + '_clipped_population.tif')
        file_count +=1
        raster = raster_utils.clip_dataset(raster, aoi, clipped_population_uri)
        data_uri['clipped_population'] = clipped_population_uri
    # Create a kernel to gather the total population within a given radius
    R = (population_radius/cell_size) if population_radius >= cell_size else 1. 
    kernel = disc_kernel(R)
    # Convolve the population with the kernel
    band= raster.GetRasterBand(1)
    pop_array = band.ReadAsArray()
    nodata = band.GetNoDataValue()
    pop_array[pop_array == nodata] = 0
    pop_density = sp.signal.convolve2d(pop_array, kernel,mode='same')
    data_uri['pop_density'] = save_intermediate(pop_density, \
        prefix + '_' + str(file_count) + "_pop_density.tif")
    file_count +=1
    pop_density = gdal.Open(data_uri['pop_density'])
    # Compute the population along the coast
    # NOTE: This notation is really not intuitive, compared to a straight
    # product as is on the user guide. All this because of the raster sizes
    # that are incompatible. What about a function that uniformizes the raster
    # sizes, and use it if the the raster sizes don't agree? We could write:
    # if not raster_sizes_agree(raster_list):
    #    uniformize_raster_sizes(raster_list)
    #
    # And then simply write:
    # coastal_population = pop_density * shore
    #
    data_uri['coastal_population'] = \
        os.path.join(intermediate_directory, \
        prefix + '_' + str(file_count) + '_coastal_population.tif')
    file_count +=1
    no_data = raster_utils.calculate_value_not_in_dataset(pop_density)
    coastal_pop = raster_utils.vectorize_rasters([pop_density, shore_raster], \
        lambda x,y: x*y, aoi, data_uri['coastal_population'], \
        gdal.GDT_Float32, no_data)
    # Save data in output folder
    shutil.copy(data_uri['coastal_population'], 
    os.path.join(outputs_directory, args['prefix'] + 'coastal_population.tif'))
    # Done
    return data_uri
   
def compute_surge_estimate(args):
    """Compute the surge estimate as defined in equation 4 of Greg's notes.
    
        Inputs: 
            L_s: length of the continental shelf, defined as the distance
                between the shoreline segment and the closest point on the
                edge of the continental shelf.
            h_0: average depth from the continental shelf to the shoreline
            U: Speed of the wind blowing in the same orientation as the
                shelf from the shoreline -- Note: the convention for 
                orientations is that direction vectors point where wind 
                and wave come from, and not where they go to.
        Output: surge estimate as is defined in Greg's notes, output 3.4.
        """
    def k(U):
        """Return 'k' as indicated in equation 5 of Greg's supplementary notes
        
            Input: U- Speed of the wind blowing in the same orientation as the
            shelf from the shoreline -- Note: the convention for orientations 
            is that direction vectors point where wind and wave come from, 
            and not where they go to."""
        result = 1.2 / 1000000.
        if U > 5.6:
            result += 2.25 / 1000000. * math.pow((1 - 5.6/U), 2)
        return result

    def Tau_w(U):
        """Compute \tau_w as is mentioned in section 3.4 of Greg's notes.
            
            Input: U- Speed of wind blowing in the same orientation as the
                shoreline.

            Output: \tau_w as is mentioned in section 3.4 of Greg's notes.
        """
        return 1024 * k(U) * U*U
        
    def A(L_s, U, h_0):
        """ Compute A as is mentioned in section 3.4 of Greg's notes.
    
        Inputs: L_s: length of the continental shelf, defined as the distance
                between the shoreline segment and the closest point on the
                edge of the continental shelf.
                U: Speed of wind
                h_0: average depth from the continental shelf to the shoreline
        Output: 'A' as is defined in Greg's notes"""
        return 1.225 * L_s * Tau(U) / (1024 * 9.81 * h_0 * h_0)
    
    def eta_c(L_s, U, h_0):
        """Compute surge estimate as in eq.(4) in section 3.4 of Greg's notes.
    
        Inputs: L_s: length of the continental shelf, defined as the distance
                between the shoreline segment and the closest point on the
                edge of the continental shelf.
                U: Speed of wind
                h_0: average depth from the continental shelf to the shoreline
        Output: eta_c as defined in equation 4 in Greg's notes"""
        return h_0 * (math.sqrt(1 + 2 * A / L_s) - 1)

    # TODO: replace by vectorize_rasters
    prefix = args['prefix'] + 'SURGE_EST'
    data_uri = {}
    intermediate_directory = args['intermediate_directory']
    outputs_directory = args['outputs_directory']
    # set to save in 'intermediate_directory' and 'outputs_directory'
    save_intermediate = set_save_info(intermediate_directory,\
        args['aoi'],args['cell_size'])
    # Compute continental shelf edges
    # TODO: Move this to a function
    continental_shelf_uri = args['continental_shelf']
    shelf_dataset = gdal.Open(continental_shelf_uri)
    shelf_array = shelf_dataset.GetRasterBand(1).ReadAsArray()
    kernel = np.array([[-1., -1., -1.], [-1., 8., -1.], [-1., -1., -1.]])
    shelf_edge = sp.signal.convolve2d(shelf_array, kernel, mode='same')
    # TODO: Remove the shore artifacts!
    data_uri['surge_estimate_all_edges'] = \
        save_intermediate(shelf_edge, prefix + '_0_all_edges.tif')

    # TODO: For loop for 16 sectors, extract wind speeds for the shoreline 
    # segments only.
    # Create a raster dict of the wind intensity U_n
#    wind = assign_dictionary(extract_REI_V(wind_data, aoi, cell_size, path))

#    shore_dataset = gdal.Open['shore_uri']
#    shore_array = shore_dataset.GetRasterBand(1).ReadAsArray()
#    surge_estimate = np.copy(shore_array)

#    shore_segments = np.where(shore_array == shore())
#    shelf_segments = np.where(shelf_edge   > 0)

#    tree = spatial.KDTree(shelf_segments)
#    L_s, shelf_indices = tree.query(shore_segments)

#    shore_to_shelf = shelf_edge[shelf_indices] - shore_segments
#    norm_direction = np.sqrt(np.sum(np.pow(shore_to_shelf, 2.)))
#    step_count = np.max(shore_to_shelf, axis=0)
#    unit_step = shore_to_shelf / step_count
    
#    for segment in shore_segments:
        # Compute the indices from the shore to the shelf
#        path = [shore_segments[segment] + step * unit_step[segment] \
#            for step in range(step_count[segment])]
        # Compute h_0
#        h_0 = bathymetry_array[path]
        # Find the sector's angle and U
#        angle = math.atan2(-norm_direction[segment,1],norm_direction[segment,2])
        # Find the sector
        # TODO: conversion functions:
        #   -angle_rad -> sector
        #   -direction_vector -> angle
        #   -fetch_direction -> direction_vector
#        sector = int(angle * 180.0 / math.pi)
        # Find the wind speed
#        U[segment] = wind[segment_id, sector] 
        # Compute surge estimate
#        surge_estimate[segment] = eta_c(L_s, U, h_0)
    data_uri['surge_estimate'] = data_uri['surge_estimate_all_edges']
    return data_uri

# TODO: Add option to set continental shelf depth threshold
def compute_surge_potential(args):
    """Compute surge potential index as described in the user manual.
    
        Inputs:
            - args['bathymetry']: bathymetry DEM file.
            - args['landmass']: shapefile containing land coverage data (land =
              1, sea = 0)
            - args['aoi_uri']: uri to the datasource of the area of interest
            - args['shore_uri']: uri to a shore raster where the shoreline is 1,
              and everything else is 0.
            - args['cell_size']: integer number for the cell size in meters
            - args['intermediate_directory']: uri to the directory where 
              intermediate files are stored

        Output:
            - Return R_surge as described in the user guide.
            
        Intermediate outputs:
            - rasterized_sea_level_rise.tif:rasterized version of the shapefile
            - shore_level_rise.tif: sea level rise along the shore.
            - sea_level_rise.tif: sea level rise index along the shore."""
    bathymetry = gdal.Open(args['bathymetry_uri'])
    landmass = gdal.Open(args['landmass_uri'])
    aoi = ogr.Open(args['aoi_uri'])
    shore_raster = gdal.Open(args['shore_uri'])
    shore_array = shore_raster.GetRasterBand(1).ReadAsArray()
    cell_size = args['cell_size']
    intermediate_directory = args['intermediate_directory']
    prefix = args['prefix'] + 'SURGE'
    file_count = 0
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    
    # Test that the projections are identical
    # TODO: replace this with adjust_shapefile_to_aoi
    projections = \
        [raster_wkt_projection(bathymetry), shapefile_wkt_projection(aoi)]
    assert(projections_match(projections))
    # Extract pixel size
    pixel_size = pixel_size_within_aoi(aoi, bathymetry)
    # Resample the DEM to cell_size
    resampled_bathymetry_uri = \
        os.path.join(intermediate_directory, prefix + '_' + str(file_count) + \
            '_resampled_bathymetry.tif')
    data_uri['resampled_DEM'] = resampled_bathymetry_uri
    bathymetry = raster_utils.reproject_dataset(bathymetry, cell_size, \
        bathymetry.GetProjection(), resampled_bathymetry_uri)
    file_count += 1
    # Clip the bathymetry to the AOI
    data_uri['clipped_bathymetry'] = \
        os.path.join(intermediate_directory, prefix + '_' + str(file_count) + \
            '_clipped_bathymetry.tif')
    clipped_bathymetry = raster_utils.clip_dataset(bathymetry, aoi, \
        data_uri['clipped_bathymetry'])
    file_count += 1
    # Get the land elevation and land coverage from the bathymetry.
    land_elevation_uri = os.path.join(intermediate_directory, \
        prefix + '_' + str(file_count) +'_land_elevation.tif')
    land_elevation = \
        combine_rasters(landmass, clipped_bathymetry, land_elevation_uri)
    data_uri['land_elevation'] = land_elevation_uri
    file_count += 1
    # Compute the edge of the continental shelf
    # TODO: replace by vectorize_rasters
    continental_shelf = land_elevation.GetRasterBand(1).ReadAsArray()
    continental_shelf[continental_shelf >= -150] = 1 # on the shelf
    continental_shelf[continental_shelf  < -150] = 0 # off the shelf
    data_uri['continental_shelf'] = \
        save_intermediate(continental_shelf, prefix +'_'+str(file_count)+ \
            '_continental_shelf.tif')
    file_count += 1
    # Compute the distances on the shelf from the edge
    distance_to_shelf = morphology.distance_transform_edt(continental_shelf)
    data_uri['distance_to_shelf'] = \
        save_intermediate(distance_to_shelf, prefix + '_'+str(file_count) +\
            '_distance_to_shelf.tif')
    file_count += 1
    # Compute the surge along the shore in meters
    # TODO: replace by vectorize_rasters
    shore_shelf_distance = distance_to_shelf * cell_size
    data_uri['shore_shelf_distance'] = \
        save_intermediate(shore_shelf_distance, prefix +'_' +str(file_count) +\
            '_shore_shelf_distance.tif')
    file_count += 1
    # Use percentiles to assign vulnerability index
    # TODO: replace by vectorize_rasters
    R_surge = shore_shelf_distance / np.amax(shore_shelf_distance)
    R_surge[R_surge < 0.01] = 1
    R_surge[R_surge < 0.25] = 2
    R_surge[R_surge < 0.75] = 3
    R_surge[R_surge < 0.90] = 4
    R_surge[R_surge < 1.00] = 5
    R_surge = R_surge * shore_array
    data_uri['surge_potential'] = \
        save_intermediate(R_surge, prefix +'_' +str(file_count) +\
            '_surge_potential.tif')
    # done
    return data_uri

# TODO: handle sea level rise and rate differently! But first, I need this data
def compute_sea_level_rise(args):
    """Compute the sea level rise index as described in the user manual.
    
        Inputs:
            -args['sea_level_rise']: shapefile with the sea level rise data.
            -args['aoi_uri']: uri to datasource of the area of interest
            -args['shore_uri']: uri to the shoreline dataset (land =1, sea =0)
            -args['cell_size']: integer of the cell size in meters
            -args['intermediate_directory']: uri to the intermediate file
             directory

        Output:
            - Return a dictionary of all the intermediate file URIs.

        Intermediate outputs:
            - rasterized_sea_level_rise.tif:rasterized version of the shapefile
            - shore_level_rise.tif: sea level rise along the shore.
            - sea_level_rise.tif: sea level rise index along the shore. If all 
                the shore has the same value, assign the moderate index value 3.
            """
    sea_level_rise = args['sea_level_rise']
    aoi = ogr.Open(args['aoi_uri'])
    shore_raster = gdal.Open(args['shore_uri'])
    shore_array = shore_raster.GetRasterBand(1).ReadAsArray()
    cell_size = args['cell_size']
    intermediate_directory = args['intermediate_directory']
    prefix = args['prefix'] + 'SLR'
    file_count = 0
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    # Reproject if needed
    aoi_wkt = shapefile_wkt_projection(aoi)
    slr_wkt = shapefile_wkt_projection(sea_level_rise)
    if not projections_match([aoi_wkt, slr_wkt]):
        sea_level_rise=raster_utils.reproject_datasource(sea_level_rise, \
            aoi_wkt, os.path.join(intermediate_directory, prefix + '_' +\
            str(file_count) + '_reprojected_sea_level_rise.shp'))
        file_count += 1
     
#    return data_uri

    # Extract the array from the shapefile
    sea_rise_uri = \
        os.path.join(intermediate_directory, prefix + '_' + str(file_count) +\
        '_rasterized_sea_level_rise.tif')
    # TODO: find out why this is not working...
    #sea_rise_array = \
    #    array_from_shapefile(sea_level_rise, aoi, cell_size, sea_rise_uri, \
    #    field='RANK')

    raster = \
        raster_utils.create_raster_from_vector_extents(cell_size, 
        cell_size, gdal.GDT_Float32, 0, sea_rise_uri, aoi)
    band = raster.GetRasterBand(1)
    gdal.RasterizeLayer(raster, [1], sea_level_rise.GetLayer(0),\
        options=["ATTRIBUTE=RANK"])
    data_uri['rasterized_sea_level_rise'] = sea_rise_uri
    file_count += 1
    # Compute sea level rise along the shore
    # TODO: replace by vectorize_rasters
    sea_rise_array = band.ReadAsArray()
    R_slr = sea_rise_array * shore_array
    data_uri['shore_level_rise'] = \
        save_intermediate(R_slr, prefix + '_' + str(file_count) + \
        '_shore_level_rise.tif')
    file_count += 1
    # Convert the rise to a rank
    # TODO: replace by vectorize_rasters
    shore_indices = R_slr > 0.                 # useful shorthand
    shore_min = np.amin(R_slr[shore_indices])  # min shore value
    shore_max = np.amax(R_slr[shore_indices])  # max shore value
    amplitude = shore_max - shore_min                   # normalization factor
    # Amplitude is zero: set shore to moderate index 3
    # TODO: replace by vectorize_rasters
    if amplitude == 0:
        R_slr[R_slr > 0.] = 3
    # Otherwise, shift to zero, normalize to 1 and use percentiles
    # TODO: replace by vectorize_rasters
    else:
        R_slr[shore_indices] = (R_slr[shore_indices] - shore_min) / amplitude
        R_slr[R_slr <= 0.1] = 5
        R_slr[R_slr <= 0.25] = 4
        R_slr[R_slr <= 0.75] = 3
        R_slr[R_slr <= 0.9] = 2
        R_slr[R_slr <= 1.0] = 1
        R_slr = R_slr * shore_array
    data_uri['sea_level_rise'] = \
        save_intermediate(R_slr, prefix + '_' + str(file_count) + \
            '_sea_level_rise.tif')
    # Done
    return data_uri

def compute_natural_habitats_vulnerability(args):
    """Compute the natural habitat rank as described in the user manual.
    
        Inputs:
            -args['habitats_csv_uri']: uri to a comma-separated text file
             containing the list of habitats.
            -args['habitats_directory_uri']: uri to the directory where to find
             the habitat shapefiles.
            -args['aoi_uri']: uri to the datasource of the area of interest
            -args['shore_uri']: uri to the shoreline dataset (land =1, sea =0)
            -args['cell_size']: integer cell size in meters
            -args['intermediate_directory']: uri to the directory where 
              intermediate files are stored

        Output:
            -data_uri: a dictionary of all the intermediate file URIs.
            
        Intermediate outputs:
            - For each habitat (habitat name 'ABCD', with id 'X') shapefile:
                - ABCD_X_raster.tif: rasterized shapefile data.
                - ABCD_influence.tif: habitat area of influence. Convolution
                  between the rasterized shape data and a circular kernel which 
                    radius is the habitat's area of influence, TRUNCATED TO
                    CELL_SIZE!!!
                - ABCD_influence_on_shore.tif: habitat influence along the shore
            - habitats_available_data.tif: combined habitat rank along the
                shore using equation 4.4 in the user guide.
            - habitats_missing_data.tif: shore section without habitat data.
            - habitats.tif: shore ranking using habitat and default ranks."""
    habitats_csv = args['habitats_csv_uri']
    habitats_dir = args['habitats_directory_uri']
    aoi = ogr.Open(args['aoi_uri'])
    shore = gdal.Open(args['shore_uri'])
    shore_array = shore.GetRasterBand(1).ReadAsArray()
    cell_size = args['cell_size']
    intermediate_directory = args['intermediate_directory']
    prefix = args['prefix'] + 'HAB_'
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    # Predefined constants
    NAME                = 0 # Habitat name (string)
    ID                  = 1 # Habitat ID (unique int identifier)
    RANK                = 2 # Habitat rank (int, 1 to 5)
    PROTECTION_DISTANCE = 3 # Habitat protection distance in meters (int)
    FILE_NAME           = 4 # Name of the habitat shapefile
    ATTRIBUTE_COUNT     = 5 # Total number of attributes
    extension = '.shp'      # Shapefile extension to look for
    # Read the CSV file
    csv_reader = csv.reader(open(habitats_csv)) 
    # Load CSV data
    csv_data = {}
    for item in csv_reader:
        if item[ID] != 'ID':            # Skip header information
            csv_data[item[ID]] = item
    # Retreive shapefiles by extension
    file_list = os.listdir(habitats_dir)
    for entry in file_list:
        # Look for files with '.shp' extension...
        # ...AND '.shp' only: skip extentions like '.shp.xml'
        if extension in entry and \
            (len(entry) == entry.find(extension)+len(extension)):
            # Extract the basename, so that we can find the habitat ID
            basename = entry[0:entry.find(extension)]
            # Find the habitat ID for this file
            habitat_id = basename[basename.find('_')+1:]
            # If it's a known habitat ID, add the file to the habitat data
            if habitat_id in csv_data:
                csv_data[habitat_id].append(os.path.join(habitats_dir, entry))
    
    R = {}  # Dictionary: key=segment coord., value=list of fronting habitats
    prefix_number = 1   # used to keep track of file creation order
    # Process each habitat
    for habitat in csv_data:
        habitat_data = csv_data[habitat]
        # Check if there is a habitat layer to work with:
        if len(habitat_data) != ATTRIBUTE_COUNT:
            LOGGER.warning( \
            'No shapefile data associated to habitat %s. Skipping.' % \
            habitat_data[NAME])
        else:
            # A few shorthand names, for convenience
            habitat_name        =     habitat_data[NAME]
            habitat_file_name = habitat_name.replace(' ', '_')
            habitat_id          = int(habitat_data[ID])
            habitat_rank        = int(habitat_data[RANK])
            habitat_distance    = int(habitat_data[PROTECTION_DISTANCE])
            # Open the habitat shapefile
            datasource = ogr.Open(habitat_data[FILE_NAME])
            # Reproject shapefile if necessary:
            datasource_uri = adjust_shapefile_to_aoi(datasource, aoi, \
                prefix, intermediate_directory)
            datasource = None
            datasource = ogr.Open(datasource_uri)
            # Burn the shapefile and extract the array
            data_uri[habitat_file_name + '_' + str(habitat_id) + '_raster'] = \
                os.path.join(intermediate_directory,
                prefix + str(prefix_number).zfill(2) + '_' + \
                habitat_file_name + '_' + str(habitat_id) + '_raster.tif')
            #habitat_array = array_from_shapefile(datasource, aoi, cell_size,\
            #    data_uri[habitat_file_name + '_' + str(habitat_id) + '_raster'])
            habitat_array = array_from_shapefile(datasource, aoi, 
                cell_size, data_uri[habitat_file_name + '_' + \
                str(habitat_id) + '_raster'], all_touched = True)
            #print('pixels detected for ' + habitat_name + ': detected:', 
            #np.sum(habitat_array), 'over-estimated:', np.sum(wide_habitat_array))
            prefix_number += 1
            # Create a kernel to detect where the habitat influences the shore
            kernel = disc_kernel(habitat_distance / cell_size)
            # Area of influence by convolving the kernel with the habitat raster
            influenced_area = \
                sp.signal.convolve2d(habitat_array, kernel, mode='same')
            data_uri[habitat_file_name + '_influence'] = \
            save_intermediate(influenced_area, \
                prefix + str(prefix_number).zfill(2) + '_' + \
                habitat_file_name +'_influence.tif')
            prefix_number += 1
            # Compute the influence of the habitat along the shore
            # TODO: replace by vectorize_rasters
            shore_influence = influenced_area * shore_array
            data_uri[habitat_file_name + '_influence_on_shore'] = \
            save_intermediate(shore_influence, \
                prefix + str(prefix_number).zfill(2) + '_' + \
                habitat_file_name + '_influence_on_shore.tif')
            prefix_number += 1
            # Mark all the shore segments under the iinfluence of this habitat
            [I, J] = np.where(shore_influence > 0)
            for point in zip(I,J):
                if point in R: 
                    np.concatenate([R[point], np.array([habitat_rank])])
                else:
                    R[point] = np.array([habitat_rank])
    
    # TODO: replace by vectorize_rasters
    shore_data = np.zeros_like(shore_array)   # Will hold known ranking data
    # For each point influenced by a natural habitat
    for point in R:
        # Compute the vulnerability index
        #print('Before: R[', point, ']=', R[point])
        R[point] = combined_rank(R[point])
        #print('After: R[', point, ']=', R[point])
        # Fill the array with shore data
        shore_data[point[0], point[1]] = R[point]
        
    data_uri['habitats_available_data'] = \
        save_intermediate(shore_data, \
        prefix + str(prefix_number).zfill(2) + '_' + \
        'habitats_available_data.tif')
    prefix_number += 1
    # Default shore vulnerability index
    # TODO: replace by vectorize_rasters
    default_data = (shore_data == 0).astype(int) * shore_array * 3
    data_uri['habitats_missing_data'] = \
        save_intermediate(default_data, \
        prefix + str(prefix_number).zfill(2) + '_' + \
        'habitats_missing_data.tif')
    prefix_number += 1
    # Actual shore vulnerability index
    # TODO: replace by vectorize_rasters
    R_hab = shore_data + default_data
    data_uri['R_hab'] = \
        save_intermediate(R_hab, \
        prefix + str(prefix_number).zfill(2) + '_' + \
        'R_hab.tif')
    prefix_number += 1
    # Rounded index
    # TODO: replace by vectorize_rasters
    R_hab = (R_hab + 0.5).astype(int)
    data_uri['natural_habitats'] = \
        save_intermediate(R_hab, \
        prefix + str(prefix_number).zfill(2) + '_' + \
        'habitats.tif')
    return data_uri

# TODO: update docstring
def compute_geomorphology(args):
    """ Compute the geomorphology index as is described in InVEST's user guide.
    
        Inputs:
            -args['geomorphology']: shapefile of the gemorphology ranking 
             along the coastline.
            -args['aoi_uri']: uri to the region of interest (shapefile).
            -args['shore_uri']: uri to a the shoreline dataset 
                (land = 1, sea = 0).
            -args['cell_size']: integer cell size of in meters.
            -args['spread_radius']: integer value of the area over which to
              spread (smear) the coastline. 
              If the coastline from the geomorphology doesn't 
              match the land polygon's shoreline, we can increase the overlap
              by 'spreading' the data from the geomorphology over a wider area.
              The wider the spread, the more ranking data overlaps with the 
              coast. The spread is a convolution between the geomorphology 
              ranking data and a 2D gaussian kernel of area
              (2*spread_radius+1)^2. A radius of zero reduces the kernel to the
              scalar 1, which means no spread at all.              
            - args['intermediate_directory']: uri to the directory where 
              intermediate files are stored

        Output:
            - data_uri: a dictionary of all the intermediate file URIs.
            
        Detailed output:
            - aoi_reprojected_morphology.shp: the original geomorphology data
                reprojected to the AOI's projection.
            - geomorphology_rank.tif: the shore ranking as stored in the
                shapefile, rasterized within the aoi using cell_size.
            - spread_rank.tif: shore ranking smoothen by convolution of 
                geomorphology_rank.
            - geomorphology_coast.tif: rasterized coast (encoded as 1s) 
                extracted from geomorphology data.

            - spread_coast.tif: smoothed shore obtained by convolution of
                geomorphology_coast. Can't be used as is as a quotient in a
                division (see normalized_spread_rank), because of 0 values.
            - adjusted_spread_coast.tif: same as spread_coast.tif, where areas
                with zeros set to one to avoid division-by-zero as a quotient.
            - available_coastal_data.tif: coastal information that is available
                given the data.
            - missing_coastal_data.tif: portions of the shore for which rank
                information isn't available from the data.
            - normalized_spread_rank.tif: raster file of 
                spread_rank / spread_coast. The result is floating point values
                between 1 to 5. To be integer, they need to be approximated.
            - approximated_spread_rank.tif: approximation of
              normalized_spread_rank to be integers between 1 and 5.
            - geomorphology.tif: combination of rank information from
              normalized_spread_rank, completed with missing coastal
              information (from missing_coastal data) set to a moderate level
              of 3."""
    geomorphology = args['geomorphology']
    aoi = ogr.Open(args['aoi_uri'])
    shore = gdal.Open(args['shore_uri'])
    cell_size = args['cell_size']
    spread_radius = args['spread_radius']
    intermediate_directory = args['intermediate_directory']
    prefix = args['prefix'] + 'GEO_'
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    field_name = 'RANK'
    MODERATE_RANK = 3
    # Ensure that 'field_name' exist in the file 
    assert(has_field(field_name, geomorphology))
    # Ensure projections are identical
    aoi_wkt = shapefile_wkt_projection(aoi)
    geo_wkt = shapefile_wkt_projection(geomorphology)
    if not projections_match([aoi_wkt, geo_wkt]):
        geomorphology=raster_utils.reproject_datasource(geomorphology,aoi_wkt,\
            os.path.join(intermediate_directory, \
            prefix + 'reprojected_geomorphology.shp'))
    # Burn the rank data to a geomorphology raster
    geo_rank_uri = \
    os.path.join(intermediate_directory, prefix + "01_geomorphology_rank.tif")
    geo_raster = \
        raster_from_shapefile(geomorphology, aoi, cell_size, geo_rank_uri)
    layer, index = get_layer_and_index_from_field_name(field_name,geomorphology)
    gdal.RasterizeLayer(geo_raster, [1], geomorphology.GetLayer(layer),\
        options=["ATTRIBUTE=%s" % field_name, 'ALL_TOUCHED=TRUE'])
    #    options=["ATTRIBUTE=%s" % field_name])
    geo_array = geo_raster.GetRasterBand(1).ReadAsArray()
    shore_array = shore.GetRasterBand(1).ReadAsArray()
    # As the geomorphology coast may not perfectly overlap with 'shore',
    # we have to find a way to have rank values along every shore point.
    # Solution: 
    #   1)-spread the shore ranks with a convolution 
    #   2)-normalize by the spread (geomorphology shore set to 1,then convolved)
    # Create convolution kernel + apply it
    # avoid / by zero if R == 0
    R = (spread_radius/cell_size) if spread_radius >= cell_size else 1. 
    kernel = np.exp(-disc_kernel(R)/(2.*(R/3.)**2.))
    # 1)-Convolve the geomorphology ranking with the kernel
    spread_rank = sp.signal.convolve2d(geo_array, kernel,mode='same')
    data_uri['spread_rank'] = \
    save_intermediate(spread_rank, prefix + "02_spread_rank.tif")
    # Convolve the geomorphology coast with the kernel
    geo_coast = (geo_array > 0).astype(int)
    data_uri['geomorphology_coast'] = \
        save_intermediate(geo_coast, prefix + "03_geomorphology_coast.tif")
    spread_coast = sp.signal.convolve2d(geo_coast, kernel, mode='same')
    data_uri['spread_coast'] = \
        save_intermediate(spread_coast, prefix + "04_spread_coast.tif")
    # Create an array where the spread coast is normalized to 1, and 0 at sea
    norm_coast = np.copy(spread_coast)
    norm_coast[norm_coast > 0.] = 1.
    # Compute a mask for missing coast data
    # TODO: replace by vectorize_rasters
    #segment_shallowness_op = get_segment_shallowness(depth_threshold)
    #data_uri['shallow_fetch'] = \
    #    os.path.join(intermediate_directory, prefix+'_2_shore_shallowness.tif')
    #shallowness_dataset = raster_utils.vectorize_rasters(depth_datasets,
    #    segment_shallowness_op, raster_out_uri = data_uri['shallow_fetch'], \
    #    datatype=gdal.GDT_Byte, nodata=255)

    available_coast = norm_coast * shore_array
    data_uri['available_coastal_data'] = \
    save_intermediate(available_coast, prefix +"05_available_coastal_data.tif")
    missing_coast = shore_array - available_coast
    data_uri['missing_coastal_data'] = \
    save_intermediate(missing_coast, prefix + "06_missing_coastal_data.tif")
    # 2)-Normalize the spread ranking by the spread coast
    # TODO: replace by vectorize_rasters
    spread_coast[spread_coast == 0.] = 1.     # avoid division by zero
    data_uri['adjusted_spread_coast'] = \
        save_intermediate(spread_coast, prefix + "07_adjusted_spread_coast.tif")
    norm_spread_rank = spread_rank / spread_coast
    data_uri['normalized_spread_rank'] = \
    save_intermediate(norm_spread_rank, prefix +"08_normalized_spread_rank.tif")
    # Approximate the rank to be between 1 to 5
    # TODO: replace by vectorize_rasters
    approximated_rank = (norm_spread_rank + 0.5).astype(int)
    data_uri['approximated_spread_rank']=save_intermediate(approximated_rank,\
        prefix + "09_approximated_spread_rank.tif")
    # Available data is geomorphology rank along the shore
    available_rank = approximated_rank * shore_array
    data_uri['available_rank_data'] = \
        save_intermediate(available_rank, prefix +"10_available_rank_data.tif")
    # Geomorphology index is available data completed with missing data
    # TODO: replace by vectorize_rasters
    R_geomorphology = available_rank + (missing_coast * MODERATE_RANK)
    # Save the result
    data_uri['geomorphology'] = \
        save_intermediate(R_geomorphology, prefix + "11_geomorphology.tif")
    return data_uri

def compute_relief_rank(args):
    """ Compute the relief index as is described in InVEST's user guide.
    
        Inputs:
            - args['relief_uri']: uri to an elevation dataset.
            - args['aoi_uri']: uri to the datasource of the region of interest.
            - args['landmass_uri']: uri to the landmass datasource where land is
              1 and sea is 0.
            - args['spread_radius']: if the coastline from the geomorphology i
                doesn't match the land polygon's shoreline, we can increase the 
                overlap by 'spreading' the data from the geomorphology over a 
                wider area. The wider the spread, the more ranking data overlaps
                with the coast. The spread is a convolution between the 
                geomorphology ranking data and a 2D gaussian kernel of area
                (2*spread_radius+1)^2. A radius of zero reduces the kernel to 
                the scalar 1, which means no spread at all.              
            - args['spread_radius']: how much the shore coast is spread to match
                the relief's coast.
            - args['shore']: the shoreline (land = 1, sea = 0).
            - args['cell_size']: granularity of the rasterization.
            - args['intermediate_directory']: where intermediate files are
                stored
            
        Output:
            - Return R_relief as described in the user manual.
            - A rastrer file called relief.tif"""
    relief = gdal.Open(args['relief_uri'])
    aoi = ogr.Open(args['aoi_uri'])
    shore = gdal.Open(args['shore_uri'])
    shore_array = shore.GetRasterBand(1).ReadAsArray()
    land_raster= gdal.Open(args['landmass_uri'])
    spread_radius = args['spread_radius']
    cell_size = args['cell_size']
    spread_radius = args['spread_radius']
    intermediate_directory = args['intermediate_directory']
    prefix = args['prefix'] + 'RELIEF'
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    # TODO: Refactor this code into 'adjust_raster_to_aoi'
    # Test that the projections are identical
    projections = [raster_wkt_projection(relief), shapefile_wkt_projection(aoi)]
    assert(projections_match(projections))
    # Extract pixel size
    pixel_size = pixel_size_within_aoi(aoi, relief)
    # Resample the raster to cell_size
    resampled_relief = raster_utils.reproject_dataset(relief, cell_size,\
        relief.GetProjection(), \
        os.path.join(intermediate_directory,prefix +'_01_resampled_relief.tif'))
    # Clip the raster to the AOI
    clipped_relief_path = \
        os.path.join(intermediate_directory, prefix + '_02_clipped_relief.tif')
    clipped_relief = \
        raster_utils.clip_dataset(resampled_relief, aoi,clipped_relief_path)
    data_uri['clipped_relief'] = clipped_relief_path
    # Get the land elevation and land coverage from relief. Sea elevation is 0
    # TODO: replace by vectorize_rasters
    empty_raster = raster_utils.new_raster_from_base(land_raster, \
        os.path.join(intermediate_directory, prefix + '_03_empty_raster.tif'), \
        'GTiff', nodata(), gdal.GDT_Float32)
    land_elevation_uri = os.path.join(intermediate_directory, \
        prefix + '_04_land_elevation.tif')
    land_elevation_raster = combine_rasters(empty_raster, clipped_relief,\
        land_elevation_uri)
    band = land_elevation_raster.GetRasterBand(1)
    land_elevation = band.ReadAsArray()
    land_elevation_nodata = band.GetNoDataValue()
    land_elevation[land_elevation == land_elevation_nodata] = 0
    land_elevation[land_elevation <0] = 0   # Set sea elevation to 0
    land_elevation_raster.GetRasterBand(1).WriteArray(land_elevation)
    data_uri['land_elevation'] = land_elevation_uri
    # Compute the aoi's raster array
    aoi_uri = os.path.join(intermediate_directory, prefix + '_05_aoi.tif')
    aoi_array = array_from_shapefile(aoi, aoi, cell_size, aoi_uri)
    # Create convolution kernel + apply it
    kernel = disc_kernel(spread_radius / cell_size)
    # Convolve the land elevation with the kernel
    elevation_average = sp.signal.convolve2d(land_elevation, kernel,mode='same')
    data_uri['elevation_average'] = \
        save_intermediate(elevation_average, prefix+'_06_elevation_average.tif')
    # Convolve the land coverage with the kernel
    # TODO: replace by vectorize_rasters
    land_coverage = land_raster.GetRasterBand(1).ReadAsArray()
    land_proportion =sp.signal.convolve2d(land_coverage, kernel, mode='same')
    land_proportion[land_proportion == 0] = 1   # Avoid division by zero
    data_uri['land_proportion'] = \
        save_intermediate(land_proportion, prefix + '_07_land_proportion.tif')
    # Compute average land height
    # TODO: replace by vectorize_rasters
    average_land_height = elevation_average / land_proportion
    data_uri['average_land_height'] = \
    save_intermediate(average_land_height, prefix+'_08_average_land_height.tif')
    # Per-segment average of the shore height along the coast
    # TODO: replace by vectorize_rasters
    average_relief = average_land_height * shore_array     # shore mask
    data_uri['average_relief'] = \
        save_intermediate(average_relief, prefix + '_09_average_relief.tif')
    # Relief index: percentile -(round)-> integer -(+1)-> index
    # TODO: replace by vectorize_rasters
    R_relief = \
        (average_relief /np.amax(average_relief) *4).astype(int) + shore_array
    # Save result
    data_uri['relief'] = \
        save_intermediate(R_relief, prefix + '_10_relief.tif')
    return data_uri

def compute_wind_exposure(args):
    """ Compute the wind exposure for every shore segment as in equation 4.5
    
        Inputs:
            - args['climatic_forcing_uri']: uri to the wind information
              datasource
            - args['aoi_uri']: uri to the area of interest datasource
            - args['fetch_distances']: a dictionary of (point, list) pairs
              where point is a tuple of integer (row, col) coordinates and 
              list is a maximal fetch distance in meters for each fetch sector.
            - args['fetch_depths']: same dictionary as fetch_distances, but
              list is a maximal fetch depth in meters for each fetch sector.
            - args['cell_size']: granularity of the rasterization.
            - args['intermediate_directory']:where intermediate files are stored
            - args['prefix']: string 
            
        Outputs:
            - data_uri: dictionary of the uri of all the files created in the
              function execution
        File description:
            - REI.tif: combined REI value of the wind exposure index for all
              sectors along the shore.
        
            - For each equiangular fetch sector n:
                - REI_n.tif: per-sector REI value (U_n * P_n * F_n)."""
    wind_data = ogr.Open(args['climatic_forcing_uri'])
    aoi = ogr.Open(args['aoi_uri'])
    distance = args['fetch_distances']
    depth = args['fetch_depths']
    sector_count = SECTOR_COUNT
    cell_size = args['cell_size']
    intermediate_directory = args['intermediate_directory']
    prefix = args['prefix']
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    # Variable initializations
    sector_range = range(sector_count)
    sectors_deg = np.array(sector_range, dtype=int) *360 / sector_count
    basename = prefix + 'WIND'
    f_count = 0     # File counter
    extension = '.tif'  # Filename extension
    path = intermediate_directory
    
    # Set the execution mode to skip saving files if possible
    if args['execution_mode'] == 'fast':
        fast_mode = True
    else:
        fast_mode = False
    # TODO: replace by vectorize_rasters
    # Create a raster dict of the wind intensity U_n
    U =assign_dictionary( \
        extract_REI_V(wind_data, aoi, cell_size, path, prefix, fast_mode))
    # Create a raster dict of the percent of all wind P_n in a given sector
    P = assign_dictionary( \
        extract_REI_PCT(wind_data,aoi,cell_size,path,prefix, fast_mode))
    # Create a raster dict of the fetch distances F_n
    F = assign_dictionary(args['fetch_distance_uris'])
    # Compute the REI
    REI = np.sum((U(n) * P(n) * F(n) for n in sectors_deg), axis=0)
    # Remove negative values from nodata:
    REI[REI < 0.] = 0.
    # Normalize values between 1 and 5:
    # TODO: replace by vectorize_rasters
    REI[REI>0] = ((REI[REI>0] / np.max(REI[REI>0]) * 4.0) + 1.0).astype(int)
#    # Extract water depths
    # TODO: replace by vectorize_rasters
    d = assign_dictionary(extract_water_depths(depth,aoi,cell_size,path,prefix))
    data_uri['wave_height'] = {}
    data_uri['wave_period'] = {}
    for n in sectors_deg:
        if args['execution_mode'] == 'generate all data':
            data_uri[basename + '_U_' + str(n)] = \
            save_intermediate(U(n),basename+'_'+str(f_count)+'_U_'+str(n)+extension)
            f_count += 1
            data_uri[basename + '_P_' + str(n)] = \
            save_intermediate(P(n),basename+'_'+str(f_count)+'_P_'+str(n)+extension)
            f_count += 1
            data_uri[basename + '_F_' + str(n)] = \
            save_intermediate(F(n),basename+'_'+str(f_count)+'_F_'+str(n)+extension)
            f_count += 1
            data_uri[basename +'_' + str(n)] = save_intermediate( \
                U(n)*P(n)*F(n),basename+'_'+str(f_count)+'_' +str(n) +extension)
            f_count += 1

        data_uri['wave_height'][str(n)] = \
            save_intermediate(REI, basename +'_wave_height_' +str(n) +extension)
#            save_intermediate(H[n], 'wave_height_' + str(n) + extension)
#        f_count += 1
        data_uri['wave_period'][str(n)] = \
            save_intermediate(REI, basename +'_wave_period_' +str(n) +extension)
#            save_intermediate(T[n], 'wave_period_' + str(n) + extension)
#        f_count += 1
    data_uri['wind_exposure'] = \
        save_intermediate(REI,basename+'_'+str(f_count)+extension)
    f_count += 1
    # done
    return data_uri

def compute_wave_exposure(args):
    """ Compute the wind exposure for every shore segment
    
        Inputs:
            - args['climatic_forcing_uri']: uri to wave datasource
            - args['aoi_uri']: uri to area of interest datasource
            - args['fetch_distances']: a dictionary of (point, list) pairs
              where point is a tuple of integer (row, col) coordinates and 
              list is a maximal fetch distance in meters for each fetch sector.
            - args['fetch_depths']: same dictionary as fetch_distances, but
              list is a maximal fetch depth in meters for each fetch sector.
            - args['cell_size']: cell size in meters (integer)
            - args['H_threshold']: threshold (double) for the H function (eq. 7)
            - args['intermediate_directory']: uri to the directory that
              contains the intermediate files
            
        Outputs:
            - data_uri: dictionary of the uri of all the files created in the
              function execution
        Detail of files:
            - A file called wave.tif that contains the wind exposure index along
              the shore.
            - For each equiangular fetch sector k:
                - F_k.tif: per-sector fetch value (see eq. 6).
                - H_k.tif: per-sector H value (see eq. 7)
                - E_o_k.tif: per-sector average oceanic wave power (eq. 6)
                - E_l_k.tif: per-sector average wind-generated wave power (eq.9)
                - E_w_k.tif: per-sector wave power (eq.5)
                - E_w.tif: combined wave power."""
    wave_data = ogr.Open(args['climatic_forcing_uri'])
    aoi = ogr.Open(args['aoi_uri'])
    shore = gdal.Open(args['shore_uri'])
    shore_array = shore.GetRasterBand(1).ReadAsArray()
    sector_count = SECTOR_COUNT
    cell_size = args['cell_size']
    H_threshold = args['H_threshold']
    intermediate_directory = args['intermediate_directory']
    # set to save in 'intermediate_directory'
    save_intermediate = set_save_info(intermediate_directory,aoi,cell_size)
    # Dictionary that contains the data's uri indexed by name
    data_uri = {} 
    # Variable initializations
    sector_range = range(sector_count)
    sectors_deg = np.array(sector_range, dtype=int) *360 / sector_count
    prefix= args['prefix'] + 'WAVE'
    f_count = 0     # File counter
    extension = '.tif'  # Filename extension

    # Set the execution mode to skip saving files if possible
    if args['execution_mode'] == 'fast':
        fast_mode = True
    else:
        fast_mode = False
    # Set H threshold, in km
    H = set_H_threshold(H_threshold)
    # Create a raster dict of the wind power intensity P_l(k)
    path = intermediate_directory
    # TODO: replace by vectorize_rasters
    # Create a raster dict of the wind speed U(k) in a given sector
    U = assign_dictionary( \
        extract_REI_V(wave_data, aoi, cell_size, path,prefix, fast_mode))
    # Create a raster dict of the percent of all winds O_l(k) in a given sector
    O_l=assign_dictionary( \
        extract_REI_PCT(wave_data,aoi,cell_size,path,prefix, fast_mode))
    # Create a raster dict of the wave power intensity P_o(k)
    P_o = assign_dictionary( \
        extract_WavP(wave_data,aoi,cell_size,path,prefix, fast_mode))
    # Create a raster dict of the percent of all waves O_o(k) in a given sector
    O_o=assign_dictionary( \
        extract_WavPPCT(wave_data,aoi,cell_size,path,prefix, fast_mode))
    # Create a raster dict of the fetch distances F(k)
    F = assign_dictionary(args['fetch_distance_uris'])
    # Create a raster dict of the fetch depth d_k
    d = assign_dictionary(args['fetch_depth_uris'])
    # Compute the oceanic wave exposure E_o
    E_o = np.sum((H(F(k)) *P_o(k) *O_o(k) for k in sectors_deg), axis=0)   
    #for k in sectors_deg:
    #    data_uri['F_' + str(k)] = save_intermediate( \
    #        F(k),prefix+'_'+str(f_count)+'_F_'+str(k)+extension)
    #    f_count += 1
    #    data_uri['H_' +str(k)] = save_intermediate( \
    #        H(F(k)),prefix+'_'+str(f_count)+"_H_"+str(k)+extension) 
    #    f_count += 1
    #    data_uri['E_o_' + str(k)] = save_intermediate( \
    #        H(F(k))*P_o(k)*O_o(k), \
    #        prefix+'_'+str(f_count)+"_E_o_" +str(k) +extension)
    #    f_count += 1
    data_uri['E_o'] = save_intermediate( \
        E_o, prefix+ '_' + str(f_count) + "_E_o" + extension)
    f_count += 1
    # Compute the local wave exposure E_l
    # Compute predefined variables for equation 10 in the user guide
    gravity = 0.981 # TODO: use something better!
    eps = 1.e-8
    U_sq = np.array([U(n)**2 for n in sectors_deg]) # U squared
    U_sq_pos = U_sq + (U_sq < eps).astype(float) # positive avoid division by 0
    H_t = 0.24 * U_sq
    T_t = 7.69 * U_sq
    # TODO: replace by vectorize_rasters
    F = np.array([F(n) for n in sectors_deg])
    F[F < 0.] = 0.
    #F_t = np.array([gravity * F(n) / (U(n)**2 + eps) for n in sectors_deg])
    # TODO: replace by vectorize_rasters
    F_t = gravity * F / U_sq_pos
    d = np.array([-d(n) for n in sectors_deg])
    #d_t = np.array([gravity * (-d(n)) / (U(n)**2 + eps) for n in sectors_deg])
    d[d < 0.] = 0.
    d_t = gravity * -d / U_sq_pos
    d_t = np.maximum(d_t, 0.)
    # Computing H, the wave height as in equation 10 of the user guide
    # TODO: replace by vectorize_rasters
    first_tanh = np.tanh(np.power(d_t,1.14) * 0.343)
    numerator = 0.000414 * np.power(F_t, 0.79)
    second_tanh = np.tanh(numerator / (first_tanh + eps))
    H = H_t * np.power(first_tanh * second_tanh, 0.572)
    # Compute T, the wave period as in equation 10 of the user guide
    # TODO: replace by vectorize_rasters
    first_tanh = np.tanh(0.1 * np.power(d_t, 2.01))
    numerator = 0.000000277 * np.power(F_t, 1.45)
    # TODO: replace by vectorize_rasters
    second_tanh = np.tanh(numerator / (first_tanh + eps))
    # TODO: replace by vectorize_rasters
    T = T_t * np.power(first_tanh * second_tanh, 0.187)
    P = 0.5 * H**2 * T
    E_l = np.sum((P[k] * O_l(sectors_deg[k]) for k in sector_range), axis=0)
    #for k in sectors_deg:
    #    data_uri["E_l_" + str(k)] = save_intermediate( \
    #        P_l(k)*O_l(k),prefix+'_'+str(f_count)+'_E_l_'+str(k)+extension)
    #    f_count += 1
    data_uri['E_l'] = save_intermediate( \
        E_l, prefix+ '_' + str(f_count) + '_E_l' +extension)
    f_count += 1
    # Compute the overall wave exposure
    E_w = np.amax([E_o, E_l], axis=0)
    E_w *= shore_array
    # Normalize values between 1 and 5:
    E_w[E_w>0] = ((E_w[E_w>0] / np.max(E_w[E_w>0]) * 4.0) + 1.0).astype(int)
    data_uri['wave_exposure'] = save_intermediate( \
        E_w, prefix+ '_' + str(f_count) + "_E_w" +extension)
    return data_uri

# TODO: write a unit test for this function
def combine_rasters(target, source, filename):
    """ Write source raster's contents over the target's, preserving the
    spatial structure of the data (target raster size == source raster size).
    
        Inputs:
            - source: where the information comes from.
            - target: where the information is going to.
        
        Output:
            - the target raster contents is overridden by the source raster's
            - The new target dataset nodata value

        Note: The function uses geotransforms to overrite data. The
        geotransform (GT) is stored in a 6-element array (0 to 5), which
        follows the convention below:

        Xgeo = GT(0) + Xpixel*GT(1) + Yline*GT(2)
        Ygeo = GT(3) + Xpixel*GT(4) + Yline*GT(5)
        
        If Up is north, then GT(2) and GT(4) == 0.
        For reference, see:
    http://www.gdal.org/classGDALDataset.html#af9593cc241e7d140f5f3c4798a43a668
    """
    # The rasters must have the same projection
    assert(projections_match([raster_wkt_projection(source), \
                              raster_wkt_projection(target)]))
    # Use the geotransform to overwrite
    sgt = source.GetGeoTransform()
    tgt = target.GetGeoTransform()
    # Set constraints on the geotransform. More constraints should be lifted
    # in the future. See the docstrings for the meaning of source_gt/target_gt
    # We want 'almost' equal within 6 orders of magnitude.
    assert(abs(abs(sgt[0])-abs(tgt[0])) < 0.001) # Same starting X
    assert(abs(abs(sgt[1])-abs(tgt[1])) < 0.001)
    assert(abs(abs(sgt[2])-abs(tgt[2])) < 0.001)
    assert(abs(abs(sgt[3])-abs(tgt[3])) < 0.001) # same starting Y
    assert(abs(abs(sgt[4])-abs(tgt[4])) < 0.001)
    assert(abs(abs(sgt[5])-abs(tgt[5])) < 0.001)
    # Extract the arrays contents and nodata
    source_nodata = source.GetRasterBand(1).GetNoDataValue()
    target_nodata = target.GetRasterBand(1).GetNoDataValue()
    source_array = source.GetRasterBand(1).ReadAsArray()
    target_array = target.GetRasterBand(1).ReadAsArray()
    # Prevent out-of-bounds overwrite
    i_max = min(source_array.shape[0], target_array.shape[0])
    j_max = min(source_array.shape[1], target_array.shape[1])
    # Change nodata so that it doesn't collide with new array values
    # Concatenate source and target...
    concatenated_array = np.zeros((i_max, j_max*2))
    concatenated_array[:i_max, :j_max] = source_array[:i_max, :j_max]
    concatenated_array[:i_max, j_max:j_max*2] = target_array[:i_max, :j_max]
    # ...so that any value not in there can be nodata.
    new_nodata = raster_utils.calculate_value_not_in_array(concatenated_array)
    n_type = type(target.GetRasterBand(1).GetNoDataValue()) 
    new_nodata = numpy_cast(raster_utils.gdal_cast(new_nodata, n_type), n_type)
    # Override old nodata values with the new nodata
    source_array[source_array == source_nodata] = new_nodata
    target_array[target_array == target_nodata] = new_nodata
    # Write valid data--skip nodata
    # We can do this, because the arrays are index-compatible:see asserts above
    target_array[:i_max, :j_max] = np.array( \
        [[source_array[i, j] \
        if (source_array[i, j] != new_nodata) \
        else target_array[i, j] \
            for j in range(j_max)] \
                for i in range(i_max)] \
        )
    # Store the result back in target
    raster = raster_utils.new_raster_from_base(\
        target, filename, 'GTiff', new_nodata, gdal.GDT_Float32)
    raster.GetRasterBand(1).WriteArray(target_array)
    return raster

# TODO: write a unit test for this function
# TODO: comment this
def numpy_cast(value, numpy_type):
    numpy_int_types = [  np.int, np.int8, np.int16, np.int32, 
                                 np.uint8,np.uint16,np.uint32 ]
    # Check if machine precision is 64 bits for integers
    if sys.maxint > 2**32:
        numpy_int_types.append(np.int64)
        numpy_int_types.append(np.uint64)
    numpy_long_types = [ np.int64, np.uint64 ]
    numpy_float_types = [np.float, np.float32, np.float64]

    if numpy_type == type(np.bool):
        value = bool(value)
    elif numpy_type in numpy_int_types:
        value = int(value)
    elif numpy_type in numpy_float_types:
        value = float(value)
    elif numpy_type == type(np.complex64):
        value = complex(value)
    else:
        LOGGER.debug("Warning: can't cast %s to a python type." % \
            str(type(value)))
    return value
# TODO: write a unit test for this function
# TODO: comment this
def enumerate_shapefile_fields(shapefile):
    layer_count = shapefile.GetLayerCount()
    for l in range(layer_count):
        layer = shapefile.GetLayer(l)
        feature_count = layer.GetFeatureCount()
        print('Layer:', l+1, '/', layer_count, ', ', feature_count, 'features.')
        feature = layer.GetFeature(0)
        field_count = feature.GetFieldCount()
        for f in range(field_count):
            field_defn = feature.GetFieldDefnRef(f)
            print('Field', f+1, '/', field_count,
                ', name:', field_defn.GetNameRef())

# TODO: write a unit test for this function
def has_field(field_name, shapefile):
    """Return True if the shapefile contains field_name, False otherwise.
        
        Inputs:
            - field_name: string to look for.
            - shapefile: where to look for the field.

        Output:
            - True if the field belongs to 'shapefile', False otherwise."""
    layer_count = shapefile.GetLayerCount()
    for l in range(layer_count):
        layer = shapefile.GetLayer(l)
        feature_count = layer.GetFeatureCount()
        assert(feature_count > 0)
        feature = layer.GetFeature(0)
        field_count = feature.GetFieldCount()
        for f in range(field_count):
            field_defn = feature.GetFieldDefnRef(f)
            if field_defn.GetNameRef() == field_name:
                return True
    return False

# TODO: write a unit test for this function
def get_layer_and_index_from_field_name(field_name, shapefile):
    """Given a field name, return its layer and field index.
        Inputs:
            - field_name: string to look for.
            - shapefile: where to look for the field.

        Output:
            - A tuple (layer, field_index) if the field exist in 'shapefile'.
            - (None, None) otherwise."""
    # Look into every layer
    layer_count = shapefile.GetLayerCount()
    for l in range(layer_count):
        layer = shapefile.GetLayer(l)
        # Make sure the layer is not empty
        feature_count = layer.GetFeatureCount()
        assert(feature_count > 0)
        feature = layer.GetFeature(0)
        # Enumerate every field
        field_count = feature.GetFieldCount()
        for f in range(field_count):
            field_defn = feature.GetFieldDefnRef(f)
            if field_defn.GetNameRef() == field_name:
                return (l, f)
    # Nothing found
    return (None, None)

# TODO: this equation returns different habitat values for single habitats.
# combined_rank(2) -> 2.55
# combined_rank(3) -> 3.33
# combined_rank(4) -> 4.05
# When I use a rank of 3 for missing data and combine it with a habitat of the
# same rank, the equation chages this habitat's value to 3.33. I end up with 2
# values, 3 and 3.33 for two things with the same initial ranking...
def combined_rank(R_k):
    """Compute the combined habitats ranks as described in equation (3)
    
        Inputs:
            - R_k: the list of ranks
            
        Output:
            - R_hab as decribed in the user guide's equation 3."""
    return 4.8 -0.5 *math.sqrt( (1.5 *max(5-R_k))**2 + \
                    sum((5-R_k)**2) -(max(5-R_k))**2)

# TODO: write a unit test for this function
def disc_kernel(r):
    """Create a (r+1)^2 disc-shaped array filled with 1s where d(i-r,j-r) <= r
    
        Input: r, the kernel radius. r=0 is a single scalar of value 1.
        
        Output: a (r+1)x(r+1) array with:
                - 1 if cell is closer than r units to the kernel center (r,r),
                - 0 otherwise.
                
            Distances are Euclidean."""
    # Create a grid of an evenly-spaced datapoints of diameter r * 2 + 1
    [X, Y] = np.mgrid[0:(r*2+1), 0:(r*2+1)]
    # Compute an array of 1s where the distance to the center (r,r) < r
    kernel = (r**2 >= np.square(X-r)+np.square(Y-r)).astype(int)
    return kernel

# TODO: write a unit test for this function
def set_H_threshold(threshold):
    """ Return 0 if fetch is strictly below a threshold in km, 1 otherwise.
    
        Inputs:
            fetch: fetch distance in meters.

        Return:
            1 if fetch >= threshold (in km)
            0 if fetch  < threshold

        Note: conforms to equation 4.8 in the invest documentation."""
    def H(fetch):
        return np.array(fetch >= (threshold * 1000)).astype(int)

    return H

# TODO: write a unit test for this function
def raster_spatial_reference(raster):
    """Extract a WKT-compliant spatial reference from a dataset (raster).
    
        Input: The dataset.
        
        Output: the dataset's WKT-compliant spatial reference."""
    srs = osr.SpatialReference()
    srs.ImportFromWkt(raster.GetProjection())
    return srs

# TODO: write a unit test for this function
def shapefile_spatial_reference(shapefile):
    """Extract a WKT-compliant spatial reference from a datasource (shapefile).
    
        Input: A raster datasource.
        
        Output: the datasource's WKT-compliant spatial reference."""
    return shapefile.GetLayer(0).GetSpatialRef()

# TODO: write a unit test for this function
# TODO: remove 'projection' from function name
def raster_wkt_projection(raster):
    """ Return the projection of a raster in the OpenGIS WKT format.
    
        Input: 
            - raster: raster file
        
        Output:
            - a projection encoded as a WKT-compliant string."""
    return raster.GetProjection()

# TODO: write a unit test for this function
# TODO: remove 'projection' from function name
def shapefile_wkt_projection(shapefile):
    """ Return the projection of a shapefile in the OpenGIS WKT format.
    
        Input: 
            - raster: raster file
        
        Output:
            - a projection encoded as a WKT-compliant string."""
    layer = shapefile.GetLayer()
    sr = layer.GetSpatialRef()
    return sr.ExportToWkt()

# TODO: write a unit test for this function
def projections_match(projection_list):
    """Check that two gdal datasets are projected identically. 
       Functionality adapted from Doug's 
       biodiversity_biophysical.check_projections 

        Inputs:
            - projection_list: list of projections to compare

        Output: 
            - False the datasets are not projected identically.
    """
    assert(len(projection_list) > 1)

    srs_1 = osr.SpatialReference()
    srs_2 = osr.SpatialReference()

    srs_1.ImportFromWkt(projection_list[0])

    for projection in projection_list:
        srs_2.ImportFromWkt(projection)

        if srs_1.IsProjected() != srs_2.IsProjected():
            LOGGER.debug('Different proj.: One of the Rasters is Not Projected')
            return False
        if srs_1.GetLinearUnits() != srs_2.GetLinearUnits():
            LOGGER.debug('Different proj.: Proj units do not match %s:%s', \
                     srs_1.GetLinearUnits(), srs_2.GetLinearUnits())
            return False
    
        if srs_1.GetAttrValue("PROJECTION") != srs_2.GetAttrValue("PROJECTION"):
            LOGGER.debug('Projections are not the same')
            return False

    return True

# TODO: write a unit test for this function
def point_from_shapefile(shapefile):
    """ Return a point that lies within the region specified by the shapefile.
    
        Inputs:
            - shapefile: shapefile that contains geometric features
            
        Output:
            - A coordinate tuple (x, y) of a point lying in the envelope of the
              first geometry in the shapefile."""
    feat = shapefile.GetLayer(0).GetNextFeature()
    geom = feat.GetGeometryRef()
    envelope = geom.GetEnvelope()
    x = (envelope[0] + envelope[1]) / 2 
    y = (envelope[2] + envelope[3]) / 2

    return (x, y)


# TODO: write a unit test for this function
def pixel_size_within_aoi(aoi, raster):
    """This function helps retrieve the pixel sizes of the global DEM 
    when given an area of interest that has a certain projection.
    
    aoi - A point shapefile datasource indicating the area of interest
    global_dem - The global DEM dataset to get the pixel size from
    
    returns - A tuple of the x and y pixel sizes of the global DEM 
              given in the units of what 'shape' is projected in
    """
    # Get the spatial reference for each object
    aoi_sr = shapefile_spatial_reference(aoi)
    raster_sr = raster_spatial_reference(raster)
    # Get forward and inverse transformations
    T = osr.CoordinateTransformation(raster_sr, aoi_sr)
    T_inverse = osr.CoordinateTransformation(aoi_sr, raster_sr)
    #Get a point in the clipped shape to determine output grid size
    x, y = point_from_shapefile(aoi)
    #Convert the point from meters to geom_x/long
    point = T_inverse.TransformPoint(x, y)
        
    #Get the size of the pixels in meters, to be used for creating rasters
    pixel_size = \
        raster_utils.pixel_size_based_on_coordinate_transform(raster, T, point)
    return pixel_size

# TODO: write a unit test for this function
def adjust_raster_to_aoi(data, aoi, cell_size, prefix = '', base_path = ''):
    """Adjust the raster's data to the aoi, i.e.reproject & clip data points.
    
        Inputs:
            - data: the dataset to adjust
            - aoi: area of interest
            - base_path: directory where the intermediate data will be stored.
            
        Output:
            - A reprojected raster that is clipped to the aoi."""
    data_wkt = raster_wkt_projection(data)
    aoi_wkt = shapefile_wkt_projection(aoi)
    # Reproject the aoi to data's projection
    if not projections_match([data_wkt, aoi_wkt]):
        aoi = raster_utils.reproject_datasource(aoi, data_wkt, \
           os.path.join(base_path, prefix + str(unique_value()) + \
           '_reprojected_aoi.shp'))
    # Clip all the points outside the aoi
    data = raster_utils.clip_dataset(data, aoi, \
        os.path.join(base_path,prefix + str(unique_value()) + \
        '_clipped_dataset.tif'))
    # Convert the datasource back to the original projection (aoi's)
    pixel_size = raster_utils.pixel_size(data)
    print('projection match=', projections_match([data_wkt,aoi_wkt]))
    if (not projections_match([data_wkt,aoi_wkt]) or (pixel_size!=cell_size)):
        print('trying to resample the raster with cell_size=', cell_size)
        data = raster_utils.reproject_dataset(data,aoi_wkt,cell_size, \
        os.path.join(base_path, prefix + str(unique_value()) + \
        '_reprojected_dataset.tif'))
    return data

# TODO: write a unit test for this function
def adjust_shapefile_to_aoi(data, aoi, prefix, base_path=''):
    """Adjust the shapefile's data to the aoi, i.e.reproject & clip data points.
    
        Inputs:
            - data: datasource to adjust
            - aoi: area of interest
            - base_path: directory where the intermediate files will be saved

        Output:
            - output_uri: a dataset that is clipped and/or reprojected to the 
            aoi if necessary."""
    if not prefix:
        prefix = str(unique_value())
    # TODO: Uncomment these lines at some point!
    #data = ogr.Open(data_uri)
    #aoi = ogr.Open(aoi_uri)
    data_wkt = shapefile_wkt_projection(data)
    aoi_wkt = shapefile_wkt_projection(aoi)
    # Reproject the aoi to be in data's projection
    if not projections_match([data_wkt, aoi_wkt]):
        aoi = raster_utils.reproject_datasource(aoi, data_wkt, \
           os.path.join(base_path, prefix + \
           'reprojected_aoi.shp'))
    # Clip all the shapes outside the aoi
    output_uri = os.path.join(base_path, prefix + \
        'clipped.shp')
    data = clip_datasource(aoi, data, output_uri)
    # Convert the datasource back to the original projection (aoi's)
    if not projections_match([data_wkt, aoi_wkt]):
        output_uri = os.path.join(base_path, prefix + \
            'reprojected.shp')
        data = raster_utils.reproject_datasource(data, aoi_wkt, output_uri)
    return output_uri 
    
def clip_datasource(aoi_ds, orig_ds, output_uri):
    """Clip an OGR Datasource of geometry type polygon by another OGR Datasource
        geometry type polygon. The aoi_ds should be a shapefile with a layer
        that has only one polygon feature

        aoi_ds - an OGR Datasource that is the clipping bounding box
        orig_ds - an OGR Datasource to clip
        out_uri - output uri path for the clipped datasource

        returns - a clipped OGR Datasource """
    orig_layer = orig_ds.GetLayer()
    aoi_layer = aoi_ds.GetLayer()

    # If the file already exists remove it
    if os.path.isfile(output_uri):
        os.remove(output_uri)

    # Create a new shapefile from the orginal_datasource 
    output_driver = ogr.GetDriverByName('ESRI Shapefile')
    output_datasource = output_driver.CreateDataSource(output_uri)

    # Get the original_layer definition which holds needed attribute values
    original_layer_dfn = orig_layer.GetLayerDefn()

    # Create the new layer for output_datasource using same name and geometry
    # type from original_datasource as well as spatial reference
    output_layer = output_datasource.CreateLayer(
            original_layer_dfn.GetName(), orig_layer.GetSpatialRef(), 
            original_layer_dfn.GetGeomType())

    # Get the number of fields in original_layer
    original_field_count = original_layer_dfn.GetFieldCount()

    # For every field, create a duplicate field and add it to the new 
    # shapefiles layer
    for fld_index in range(original_field_count):
        original_field = original_layer_dfn.GetFieldDefn(fld_index)
        output_field = ogr.FieldDefn(
                original_field.GetName(), original_field.GetType())
        output_field.SetWidth(original_field.GetWidth())
        output_field.SetPrecision(original_field.GetPrecision())
        output_layer.CreateField(output_field)

    # Get the feature and geometry of the aoi
    aoi_feat = aoi_layer.GetFeature(0)
    aoi_geom = aoi_feat.GetGeometryRef()

    # Iterate over each feature in original layer
    for orig_feat in orig_layer:
        # Get the geometry for the feature
        orig_geom = orig_feat.GetGeometryRef()
        # Check to see if the feature and the aoi intersect. This will return a
        # new geometry if there is an intersection. If there is not an
        # intersection it will return an empty geometry or it will return None
        # and print an error to standard out
        intersect_geom = aoi_geom.Intersection(orig_geom)
       
        if not intersect_geom == None and not intersect_geom.IsEmpty():
            # Copy original_datasource's feature and set as new shapes feature
            output_feature = ogr.Feature(
                    feature_def=output_layer.GetLayerDefn())
            output_feature.SetGeometry(intersect_geom)
            # Since the original feature is of interest add it's fields and
            # Values to the new feature from the intersecting geometries
            for fld_index2 in range(output_feature.GetFieldCount()):
                orig_field_value = orig_feat.GetField(fld_index2)
                output_feature.SetField(fld_index2, orig_field_value)

            output_layer.CreateFeature(output_feature)
            output_feature = None

    return output_datasource

def extract_WavP(wave_data, aoi, cell_size, base_path, prefix, fast):
    """ Create dictionary of raster filenames of datum for each sector n.
    
        Inputs:
            - wave_data: wave data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            - base_path: base path where the generated raster will be saved
            - prefix: string token prefixed before any filename
            - fast: extraction mode that skips writing on disk if True
            
        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where U(n) is defined on each cell"""
    field_prefix = 'WavP_'
    extension = '.tif'

    return raster_list_from_interpolated_fields( \
        wave_data, field_prefix, extension, aoi, cell_size, base_path, \
        prefix, fast)

def extract_WavPPCT(wave_data, aoi, cell_size, base_path, prefix, fast):
    """ Create dictionary of raster filenames of datum for each sector n.
    
        Inputs:
            - wave_data: wave data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            - base_path: base path where the generated raster will be saved
            - prefix: string token prefixed before any filename
            - fast: extraction mode that skips writing on disk if True
            
        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where U(n) is defined on each cell"""
    field_prefix = 'WavPPCT'
    extension = '.tif'

    return raster_list_from_interpolated_fields( \
        wave_data, field_prefix, extension, aoi, cell_size, base_path, \
        prefix, fast)

def extract_REI_V(wind_data, aoi, cell_size, base_path, prefix, fast):
    """ Create dictionary of raster filenames of datum for each sector n.
    
        Inputs:
            - wind_data: wind data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            - base_path: base path where the generated raster will be saved
            - prefix: string token prefixed before any filename
            - fast: extraction mode that skips writing on disk if True

        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where U(n) is defined on each cell"""
    field_prefix = 'REI_V'
    extension = '.tif'

    assert(wind_data != None)
    
    return raster_list_from_interpolated_fields( \
        wind_data, field_prefix, extension, aoi, cell_size, base_path, \
        prefix, fast)

def extract_REI_PCT(wind_data, aoi, cell_size, base_path, prefix, fast):
    """ Create dictionary of raster filenames of datum for each sector n.
    
        Inputs:
            - wind_data: wind data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            - base_path: base path where the generated raster will be saved
            - prefix: string token prefixed before any filename
            - fast: extraction mode that skips writing on disk if True
            
        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where P(n) is defined on each cell"""
    field_prefix = 'REI_PCT'
    extension = '.tif'

    return raster_list_from_interpolated_fields( \
        wind_data, field_prefix, extension, aoi, cell_size, base_path, \
        prefix, fast)

# TODO: write a unit test for this function
# TODO: improve description
def raster_list_from_interpolated_fields(data, key, suffix, aoi, cell_size, \
    path, prefix, fast):
    """Interpolate user-specified data fields on rasters saved in a raster list
    
        Inputs:
            - data: the datasource
            - key: name used to find the fields
            - suffix: used to complete the filename. This is where the
              extension goes
            - aoi: used to set the raster size
            - cell_size: raster coarseness in meters
            - path: directory where the data will be saved
            - prefix: string token prefixed before any filename
            - fast: extraction mode that skips writing on disk if True
            
        Output:
            - A str:filename dictionary where the key is the remainder of the
              field name. If there is no remainder, then key == prefix"""
    # Get the datasource field
    layer = data.GetLayer(0)
    feature_definition = layer.GetLayerDefn()
    field_count = feature_definition.GetFieldCount()
    raster_list = {} # key is the direction angle
    for field in range(field_count):
        name = feature_definition.GetFieldDefn(field).GetNameRef()
        # Determine if the current field should be processed
        if re.match(key, name):
            name = name[len(key):]
            # Create raster
            filename = os.path.join(path, prefix +'_' +key +'_' +name + suffix)
            # If fast option is True, skip file creation if it already exist
            if (fast == False) or (not os.path.isfile(filename)):
                raster = raster_utils.create_raster_from_vector_extents( \
                cell_size,cell_size, gdal.GDT_Float32, nodata(), filename, aoi)
                # Vectorize data
                raster_utils.vectorize_points(data, field, raster)
            # Store filename
            name = name if name else key
            raster_list[name] = filename
    # field doesn't exist in data
    assert(len(raster_list) > 0, "field %r doesn't exist in datasource" % key)

    return raster_list

# TODO: write a unit test for this function
# TODO: Update docstrings!
def save_fetch_distances(fetch, aoi, cell_size, base_path, prefix = ''):
    """ Create dictionary of raster filenames of fetch F(n) for each sector n.
    
        Inputs:
            - wind_data: wind data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            - base_path: base path where the generated raster will be saved
            
        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where F(n) is defined on each cell"""
    no_data = -32768.
    field_prefix = os.path.join(base_path, prefix + 'fetch_distance')
    extension = '.tif'

    # Save computed raster
    raster_list = {} # key is the direction angle
    # Extract the # of sectors, assumed to start at angle 0 with identical span
    sector_count = 0
    fetch_keys = fetch.keys()
    for point in fetch_keys:
        sector_count = len(fetch[point])
        break
    # Build F(n), raster filename dictionary and save data to disk
    for sector in range(sector_count):
        name = str(sector *360 /sector_count)
        # Open raster file
        filename = field_prefix +'_' +name +extension
        raster =\
            raster_utils.create_raster_from_vector_extents(cell_size,cell_size,\
            gdal.GDT_Float32, no_data, filename, aoi)
        band = raster.GetRasterBand(1)
        array = band.ReadAsArray() 
        # Build a fetch array F(n) for sector n
        array = np.ones_like(array) * no_data
        for point in fetch_keys:
            array[point] = fetch[point][sector]
        # Save F(n) to disk
        band.WriteArray(array)
        raster_list[name] = filename

    return raster_list

# TODO: write a unit test for this function
# TODO: Update docstrings!
def save_fetch_depths(fetch, aoi, cell_size, base_path, prefix):
    """ Create dictionary of raster filenames of fetch F(n) for each sector n.
    
        Inputs:
            - wind_data: wind data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            - base_path: base path where the generated raster will be saved
            
        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where F(n) is defined on each cell"""
    field_prefix = os.path.join(base_path, prefix + 'fetch_depth')
    extension = '.tif'

    # Save computed raster
    raster_list = {} # key is the direction angle
    # Extract the # of sectors, assumed to start at angle 0 with identical span
    sector_count = 0
    fetch_keys = fetch.keys()
    for point in fetch_keys:
        sector_count = len(fetch[point])
        break
    # Build F(n), raster filename dictionary and save data to disk
    for sector in range(sector_count):
        name = str(sector *360 /sector_count)
        # Open raster file
        filename = field_prefix +'_' +name +extension
        raster =\
            raster_utils.create_raster_from_vector_extents(cell_size,cell_size,\
            gdal.GDT_Float32, -32768., filename, aoi)
        band = raster.GetRasterBand(1)
        array = band.ReadAsArray() 
        # Build a fetch array F(n) for sector n
        array = np.zeros_like(array)
        for point in fetch_keys:
            array[point] = fetch[point][sector]
        # Save F(n) to disk
        band.WriteArray(array)
        raster_list[name] = filename

    return raster_list

# TODO: write a unit test for this function
# TODO: Update docstrings!
def extract_water_depths(water_depths, aoi, cell_size, base_path, prefix):
    """ Create dictionary of water depth rasters d(n) for each sector n.
    
        Inputs:
            - wind_data: wind data points adjusted to the aoi
            - aoi: used to create the rasters for each sector
            - cell_size: raster granularity in meters
            - base_path: base path where the generated raster will be saved
            
        Output:
            A dictionary where keys are sector angles in degrees and values are
            raster filenames where F(n) is defined on each cell"""
    field_prefix = os.path.join(base_path, prefix + 'fetch_depth')
    extension = '.tif'

    # Save computed raster
    raster_list = {} # key is the direction angle
    # Extract the # of sectors, assumed to start at angle 0 with identical span
    sector_count = 0
    depth_keys = water_depths.keys()
    for point in depth_keys:
        sector_count = len(water_depths[point])
#        print('water_depth[point]', water_depths[point])
        break
    # Build F(n), raster filename dictionary and save data to disk
    for sector in range(sector_count):
        name = str(sector *360 /sector_count)
        # Open raster file
        filename = field_prefix +'_' +name +extension
        raster =\
            raster_utils.create_raster_from_vector_extents(cell_size,cell_size,\
            gdal.GDT_Float32, nodata(), filename, aoi)
        band = raster.GetRasterBand(1)
        array = band.ReadAsArray() 
        # Build a water depth array d(n) for sector n
        array = np.zeros_like(array)
        for point in depth_keys:
            array[point] = water_depths[point][sector]
        # Save F(n) to disk
        band.WriteArray(array)
        raster_list[name] = filename

    return raster_list

# TODO: write a unit test for this function
#:RICH:WHAT DO YOU GET FROM THE CLOSURE HERE?  I WOULD PREFER A STATIC FUNCTION AND EXPLICITLY PASSING ALL THE ARGUMENTS
def set_save_info(base_path, aoi, cell_size, default_nodata = nodata(),
gdal_type = gdal.GDT_Float32):
    """Closure that saves an array in a raster constructed from an AOI.
    
        Inputs:
            - base_path: base path used to concatenate the raster filename to
            - aoi: the AOI from which to construct the template raster
            - cell_size: granularity of the rasterization in meters
            
        Output:
            - save_array: a 'save_array' object."""
    def save_array(array, filename, recompute_no_data = True):
        """ Save an array to a raster constructed from an AOI.
        
            Inputs:
                - array: the array to be saved
                - filename: where the array will be saved
                
            Output:
                - save the array in a raster file constructed from the AOI of
                  granularity specified by cell_sizei
                - Return the array uri."""
        file_path = os.path.join(base_path, filename)

        # Remove the file if it already exist
        if os.path.isfile(file_path):
            os.remove(file_path)
        
        no_data = default_nodata
        if recompute_no_data:
            no_data = float(raster_utils.calculate_value_not_in_array(array))
        no_data = raster_utils.gdal_cast(no_data, gdal_type)

        raster =\
            raster_utils.create_raster_from_vector_extents(\
            cell_size,cell_size, gdal_type, no_data, file_path, aoi)
        band = raster.GetRasterBand(1)

        try:
            band.WriteArray(array)
            raster_utils.calculate_raster_stats(raster)
        except:
            internal_array = band.ReadAsArray()
            LOGGER.debug("Error: can't save array to %s." % file_path)
            print('source array size:', array.shape)
            print('destination array size:', internal_array.shape)

        return file_path

    # Create the directory if necessary
    if not os.path.exists(base_path):
        LOGGER.debug('directory %s doesn\'t exist, creating one.' % base_path)
        os.makedirs(base_path)

    return save_array

# TODO: write a unit test for this function
#:RICH: SAME COMMENT ABOUT CLOSURE, DO YOU GET SOMETHING BENEFICIAL HERE?
def assign_dictionary(dictionary):
    """ Closure allowing to index raster arrays from a file dictionary by string
    
        Input: 
            - dictionary: the dictionary that points to raster files
            - sector: the key that points to an existing raster filename
            
        Output:
            - An array extracted from the raster file pointed to by the
              dictionary"""
    assert(len(dictionary) > 0)

    def raster_list(sector):
        try:
            raster_filename = dictionary[str(sector)]
        except KeyError:
            print('key', sector, '= ' + str(sector) + " doesn't exist")
            print('available keys', dictionary.keys())
            assert(False)

        raster = gdal.Open(raster_filename)
        band = raster.GetRasterBand(1)
        raster_array = band.ReadAsArray()
        
        raster = None
        band = None
        
        return raster_array

    return raster_list

# TODO: write a unit test for this function
# TODO: Adjust the raster data type depending on the field type!!!

#:RICH: this function looks very similar to raster_utils.create_raster_from_vector_extents, can you see if there is duplication?
def raster_from_shapefile(shapefile, aoi, cell_size, output_uri, field=None,
all_touched=False):
    """Burn default or user-defined data from a shapefile on a raster.

        Inputs:
            - shapefile: the dataset to be discretized
            - aoi: area extents over which the array is mapped
            - cell_size: coarseness of the discretization (in meters)
            - output_uri: uri where the raster will be saved
            - field: optional field name where to extract the data from
            - all_touched: optional boolean that indicates if we use GDAL's
              ALL_TOUCHED parameter when rasterizing.

        Output: A shapefile where:
            If field is specified, the field data is used as burn value.
            If field is not specified, then:
                - shapes on the first layer are encoded as 1s
                - the rest is encoded as 0"""
#    assert(not os.path.exists(output_uri))
    raster_filename = output_uri
    # TODO: Find out why when this line is removed, detect_shore asserts
    NO_DATA = 0
    # Create the raster that will contain the new data
    raster = \
        raster_utils.create_raster_from_vector_extents(cell_size, 
        cell_size, gdal.GDT_Float32, NO_DATA, raster_filename, aoi)
    band, NO_DATA = \
        raster_utils.extract_band_and_nodata(raster)
    if field != None:
        print('field:', field)
        # Will be used to reset the nodata value
        old_array = band.ReadAsArray()
        # Burn the data in 'field' to a raster
        layer, index = get_layer_and_index_from_field_name(field,shapefile)

        if all_touched == True:
            gdal.RasterizeLayer(raster, [1], shapefile.GetLayer(layer),\
                options=["ATTRIBUTE=%s" % field, 'ALL_TOUCHED=TRUE'])
        else:
            gdal.RasterizeLayer(raster, [1], shapefile.GetLayer(layer),\
                options=["ATTRIBUTE=%s" % field])
        # Change nodata so that it doesn't collide with new array values
        new_nodata = raster_utils.calculate_value_not_in_dataset(raster)
        nodata_type = type(raster.GetRasterBand(1).GetNoDataValue()) 
        new_nodata =numpy_cast(raster_utils.gdal_cast(new_nodata, nodata_type),\
            nodata_type)
        # Get the new data
        raster_array = raster.GetRasterBand(1).ReadAsArray()
        # Replace the old value of nodata to the new one
        raster_array[old_array == NO_DATA] = new_nodata
        # Adjust the nodata value
        raster.GetRasterBand(1).SetNoDataValue(new_nodata)
        # Write the new contents back into the raster
        raster.GetRasterBand(1).WriteArray(raster_array)
    else:
        if all_touched == True:
            gdal.RasterizeLayer(raster, [1], shapefile.GetLayer(0),
            burn_values=[1], options=['ALL_TOUCHED=TRUE'])
        else:
            gdal.RasterizeLayer(raster, [1], shapefile.GetLayer(0), burn_values=[1])
    return raster

# TODO: write a unit test for this function
#:RICH: this is a pretty hefty function for just calling through to another function and saying ReadAsArray, can it be removed?
def array_from_shapefile(shapefile, aoi, cell_size, output_uri, field=None,
all_touched=False):
    """Burn shapes from a shapefile's first layer on an array.

        Inputs:
            - shapefile: the dataset to be discretized
            - aoi: area extents over which the raster is mapped
            - cell_size: coarseness of the discretization (in meters)
            - output_uri: uri where the new raster will be saved
            - field: optional field in the shapefile from where values will be 
                pulled to create the raster. 
            - all_touched: optional boolean that indicates if we use GDAL's
              ALL_TOUCHED parameter when rasterizing.
        
        Output: An array where:
            - shapes are encoded as 1s
            - the rest is encoded as 0s"""
    return raster_from_shapefile(shapefile, aoi, cell_size, output_uri, \
        field, all_touched=all_touched).ReadAsArray()

def cast_ray_fast(land_raster, direction, d_max):
    """ March from the origin towards a direction until either land or a
    maximum distance is met.
    
        Inputs:
        - origin: algorithm's starting point -- has to be on sea
        - direction: marching direction
        - d_max: maximum distance to traverse
        - raster: land mass raster
        
        Returns the distance to the origin."""
    # Rescale the stepping vector so that its largest coordinate is 1
    unit_step = direction / np.fabs(direction).max()
    # Compute the length of the normalized vector
    unit_step_length = np.sqrt(np.sum(unit_step**2))
    # Compute the number of steps to take
    # Use ceiling to make sure to include any cell that is within the range of
    # max_fetch
    step_count = int(math.ceil(d_max / unit_step_length))
    I = np.array([i*unit_step[0] for i in range(step_count+1)])
    J = np.array([j*unit_step[1] for j in range(step_count+1)])

    return ((I, J), unit_step_length)
   
def fetch_vectors(angles):
    """convert the angles passed as arguments to raster vector directions.
    
        Input:
            -angles: list of angles in radians
            
        Outputs:
            -directions: vector directions numpy array of size (len(angles), 2)
    """
    # Raster convention: Up is north, i.e. decreasing 'i' is towards north.
    # Wind convention: Wind is defined as blowing FROM and not TOWARDS. This
    #                  means that fetch rays are going where the winds are
    #                  blowing from:
    # top angle: cartesian convention (x axis: J, y axis: negative I)
    # parentheses: (oceanographic   
    #               convention)    Angle   direction   ray's I  ray's J
    #                                                  coord.   coord. 
    #              90                  0      north       -1        0
    #             (90)                90       east        0        1
    #               |                180      south        1        0
    #               |                270       west        0       -1
    #     0         |         180 
    #   (180)-------+-------->(0)  Cartesian to oceanographic
    #               |              angle transformation: a' = 180 - a  
    #               |              
    #               |              so that: [x, y] -> [I, J]
    #              270  
    #             (270)
    #            
    directions = np.empty((len(angles), 2))

    for a in range(len(angles)):
        pi = math.pi
        directions[a] =(round(math.cos(pi - angles[a]), 10),\
                        round(math.sin(pi - angles[a]), 10))
    return directions

def compute_fetch_fast(land_raster, rays_per_sector, d_max, cell_size, \
    shore_array, bathymetry, bathymetry_nodata):
    """ Given a land raster, return the fetch distance from a point
    in given directions 
        
        - land_raster: raster where land is encoded as 1s, sea as 0s,
            and cells outside the area of interest as anything 
            different from 0s or 1s.
        - rays_per_sector: number of rays that must be cast per sector.
        - d_max: maximum distance in meters over which to compute the fetch
        - cell_size: size of a cell in meters
        - shore: raster encoding the shoreline as 1s, 0 otherwise.
        - bathymetry_nodata: the nodata value for the bathymetry
        
        returns: a tuple (distances, depths) where:
            distances is a dictionary of fetch data where the key is a shore
            point (tuple of integer coordinates), and the value is a 1*sectors 
            numpy array containing fetch distances (float) from that point for
            each sector. The first sector (0) points eastward."""
    # Extract shore from raster
    shore_points = np.where(shore_array == shore())

    assert(shore_points[0].size > 0)
    
    # precompute directions
    direction_count = SECTOR_COUNT * rays_per_sector
    direction_range = range(direction_count)
    direction_step = 2.0 * math.pi / direction_count
    directions_rad = [a * direction_step for a in direction_range]
    direction_vectors = fetch_vectors(directions_rad)
    unit_step_length = np.empty(direction_vectors.shape[0])
    # Compute the ray paths in each direction to their full length (d_max).
    # We'll clip them for each point in two steps (raster boundaries & land)
    # The points returned by the cast function are relative to the origin (0,0)
    ray_path = {}
    for d in range(len(directions_rad)):
        result = \
        cast_ray_fast(land_raster,direction_vectors[d], d_max/cell_size)
        ray_path[directions_rad[d]] = result[0]
        unit_step_length[d] = result[1]
    # For each point, we use the rays in ray_path, and clip them in 2 steps:
    # 1)- clip the ray paths that go beyond the raster boundaries
    # 2)- If a ray passes over a landmass, remove that section till the end
    # All this computation has to be done on a per-point basis.
    point_list = np.array(zip(shore_points[0], shore_points[1]))
    distance = {}
    avg_depth = {}
    for point in point_list:
        key = (point[0], point[1])
        distance[key] = np.empty(direction_count)
        avg_depth[key] = np.empty(direction_count)
        for d in range(direction_count):
            direction = directions_rad[d]
            # Make rays paths relative to the current point
            I = ray_path[direction][0]+point[0]
            J = ray_path[direction][1]+point[1]
            # We need integer indices to index arrays: round I and J
            I = np.around(I).astype(int)
            J = np.around(J).astype(int)
            # Constrain the ray within the raster size limit
            (i_raster, j_raster) = land_raster.shape
            (i_bathy, j_bathy) = bathymetry.shape
            i_count = min(i_raster, i_bathy)
            j_count = min(j_raster, j_bathy)
            # valid indices must be within raster bounds
            valid_i = np.where((I>=0) & (I<i_count))
            valid_j = np.where((J>=0) & (J<j_count))
            # If not all indices within bounds, remove those out of bounds
            if valid_i[0].size < I.size:
                I = I[valid_i]
                J = J[valid_i]
            if valid_j[0].size < J.size:
                I = I[valid_j]
                J = J[valid_j]
            # At this point, all indices are within bounds
            # Extract only those ray indices that are over water
            sea_indices = np.where(land_raster[(I, J)] == 0)[0]
            # If not all indices over water -> keep first section over water
            if sea_indices[-1] != len(sea_indices) - 1:
                # Find the index after the first section over water:
                # Since ray indices over land don't show up in sea_indices, do
                # 1- Subtract each index from its predecessor (D_i=i_n - i_n-1)
                # 2- Continuous indices over water have a D_i == 1, otherwise
                #   the indices are separated by land (D_i > 1). We extract 
                #   all indices that have a discontinuity, I=np.where(D_i > 1)
                non_consecutive = \
                    np.where(sea_indices[1:] - sea_indices[0:-1] > 1)[0]
                # 3- The index at I[0] is the end of the ray we want to keep
                #   (before the first time the ray hits land). We use I[0]+1
                #   for slicing ray.
                sea_indices = (sea_indices[:non_consecutive[0]+1],)
            # At this point, the ray stops before the first landmass
            # We use sea_indices to extract the accurate portion of the ray to
            # compute the distance
            I = I[sea_indices]
            J = J[sea_indices]
            # We now compute the distance traversed by the ray, which we almost
            # have already: we know the length of the ray when moving 1 cell 
            # (by taking 1 step). The number of steps to get the ray is the
            # biggest of its coordinates:
            step_count = max(math.fabs(I[-1]-I[0]), math.fabs(J[-1]-J[0]))
            D = step_count * unit_step_length[d]
            # We want to return the maximum fetch distance: it's max_fetch if
            # the ray is not stopped by land before, else it's the maximum
            # distance the ray traversed over water. D is not this maximum
            # length: the marching algorithm makes 1 pixel jumps in either the
            # x or y direction starting at the center of the first pixel. So,
            # 1/2 of the last pixel is not accounted for in D. We have to take
            # this into account.
            to_last_pixel_edge = unit_step_length[d] / 2.
            # Fetch distance is distance from pixel center to edge of water
            distance[key][d] = min(d_max, (D + to_last_pixel_edge) * cell_size)
            # Remove nodata from depth values
            non_nodata = np.where(bathymetry[(I, J)] != bathymetry_nodata)[0]
            if non_nodata.size > 0:
                I = I[non_nodata]
                J = J[non_nodata]
            # Average depth is mean bathymetry along the ray
            avg_depth[key][d] = np.average(bathymetry[(I, J)])
        # We have the distances for all the directions, now we combine them
        # Shift the arrays so that each sector has an equal number of rays on 
        # each side of its center
        distance[key] = np.roll(distance[key], -rays_per_sector / 2)
        avg_depth[key] = np.roll(distance[key], -rays_per_sector / 2)
        #print('distance', distance[key])
        # Reshape the fetch arrays so that a row corresponds to a sector
        distance[key] = \
            np.reshape(distance[key], (SECTOR_COUNT, rays_per_sector))
        avg_depth[key] = \
            np.reshape(avg_depth[key], (SECTOR_COUNT, rays_per_sector))
        #print('reshaped', distance[key])
        # Compute the weights by taking the cos of the appropriately shifted 
        # angles
        angles = np.array(directions_rad[:rays_per_sector])
        #print('angles', angles)
        angles -= directions_rad[rays_per_sector / 2]
        #print('shifted angles', angles)
        cos = np.cos(angles)
        #print('cos', cos)
        # Take the weighted rows average column-wise
        distance[key] = np.average(distance[key], axis = 1, weights = cos)
        #print('averaged', distance[key])
        avg_depth[key] = np.average(avg_depth[key], axis = 1, weights = cos)
        #sys.exit(0)
    return (distance, avg_depth)

def detect_shore(land_sea_array):
    """ Extract the boundary between land and sea from a raster.
    
    #:RICH: maybe call 'land_sea_array' instead of raster so we don't think of GDAL?, also consider operating on the gdal raster instead of the array for memory issues later?
        - raster: numpy array with sea, land and nodata values.
        
        returns a numpy array the same size as the input raster with the shore
        encoded as ones, and zeros everywhere else."""
    # Rich's super-short solution, which uses convolution.
    # Works if land, sea, and nodata have different values:
    assert(land()   != sea())
    assert(sea()    != nodata())
    assert(nodata() != land())
    
    # Don't bother computing anything if there is only land or only sea
    land_size = np.where(land_sea_array > 0)[0].size

    if land_size == 0:
        LOGGER.warning('There is no shore to detect: land area = 0')
        return np.zeros_like(land_sea_array)
    elif land_size == land_sea_array.size:
        LOGGER.warning('There is no shore to detect: sea area = 0')
        return np.zeros_like(land_sea_array)
    else:
        kernel = np.array([[-1, -1, -1],
                           [-1,  8, -1],
                           [-1, -1, -1]])
        # Generate the nodata shore artifacts
        aoi_array = np.ones_like(land_sea_array)
        aoi_array[land_sea_array == nodata()] = nodata()
        aoi_borders = (sp.signal.convolve2d(aoi_array, \
                                                kernel, \
                                                mode='same') <0 ).astype('int')
        # Generate all the borders (including data artifacts)
        borders = (sp.signal.convolve2d(land_sea_array, \
                                     kernel, \
                                     mode='same') <0 ).astype('int')
        # Real shore = all borders - shore artifacts
        borders = ((borders - aoi_borders) >0 ).astype('int') * shore()

        return borders

#:RICH: move this stuff to the 'UI' layer
def preprocess_inputs(args):
    print('---------------------------------------------------------')
    print('args:')
    for item in args.items():
        print(item)
    print('---------------------------------------------------------')
    # Convert anything in unicode to utf-8:
    for key in args.keys():
        if type(args[key]) is unicode:
            args[key] = args[key].encode('utf-8')

    prefix = '00_PRE_'
    args['intermediate_directory'] = \
        os.path.join(args['workspace_dir'], 'intermediate')
    args['outputs_directory'] = \
        os.path.join(args['workspace_dir'], 'outputs')
    sector_range = range(SECTOR_COUNT)
    sectors_deg = np.array(sector_range, dtype=int) *360 / \
        SECTOR_COUNT
    # Create intermediate directories
    if not os.path.isdir(args['intermediate_directory']):
        os.makedirs(args['intermediate_directory'])
    if not os.path.isdir(args['outputs_directory']):
        os.makedirs(args['outputs_directory'])
    # Open the files
    args['aoi_raster_uri'] = os.path.join(args['intermediate_directory'],
        prefix + 'aoi.tif')
    args['aoi'] = ogr.Open(args['aoi_uri'])
    args['aoi_raster'] = raster_utils.create_raster_from_vector_extents( \
        args['cell_size'], args['cell_size'], gdal.GDT_Float32, 0., \
        args['aoi_raster_uri'], args['aoi'])
    aoi_layer = args['aoi'].GetLayer()
    gdal.RasterizeLayer(args['aoi_raster'], [1], aoi_layer, burn_values=[1])
    args['unadjusted_landmass'] = ogr.Open(args['landmass_uri'])
    if 'climatic_forcing_uri' in args:
        raw_climatic_forcing_uri = args['climatic_forcing_uri']
        args['climatic_forcing_uri'] = ''
        # Fast execution mode: try to bypass climatic forcing pre-processing
        if args['execution_mode'] == 'fast':
            print('Fast execution mode: trying to bypass pre-processing of the \
            climatic forcing layer...')
            reprojected_layer_uri = \
                os.path.join(args['intermediate_directory'], \
                prefix + 'climatic_forcing_reprojected.shp')
            clipped_layer_uri = \
                os.path.join(args['intermediate_directory'], \
                prefix + 'climatic_forcing_clipped.shp')
            # If there is a reprojected layer, use it
            if os.path.isfile(reprojected_layer_uri):
                print('Found reprojected URI of climatic forcing layer.')
                args['climatic_forcing_uri'] = reprojected_layer_uri
            # Else, the layer might not have needed to be reprojected, so
            # see if there is a clipped layer uri:
            elif os.path.isfile(clipped_layer_uri):
                print('Found clipped URI of climatic forcing layer.')
                args['climatic_forcing_uri'] = clipped_layer_uri
        # No climatic forcing data available, pre-process it
        if args['climatic_forcing_uri'] == '':
            print('Pre-processing climatic forcing.')
            args['climatic_forcing_uri'] = \
                adjust_shapefile_to_aoi(ogr.Open(raw_climatic_forcing_uri),\
                ogr.Open(args['aoi_uri']), prefix + 'climatic_forcing_', \
                args['intermediate_directory'])
    if 'bathymetry_uri' in args:
        bathymetry = gdal.Open(args['bathymetry_uri'])
        preprocessed_bathymetry = ''
        # Fast execution mode: bypass bathymetry pre-processing if possible
        if args['execution_mode'] == 'fast':
            file_list = os.listdir(args['intermediate_directory'])
            for current_file in file_list:
                if 'clipped_bathymetry.tif' in current_file:
                    print('found pre-processed bathymetry')
                    preprocessed_bathymetry = current_file
                    break
            if preprocessed_bathymetry:
                args['bathymetry_uri'] = \
                os.path.join(args['intermediate_directory'], \
                preprocessed_bathymetry)
                assert(os.path.isfile(args['bathymetry_uri']))
        
        if preprocessed_bathymetry == '':
            aoi_wkt = shapefile_wkt_projection(args['aoi'])
            bathymetry_wkt = raster_wkt_projection(bathymetry)
            assert(projections_match([bathymetry_wkt, aoi_wkt]))
            bathymetry_cell_size = raster_utils.pixel_size(bathymetry)
            if bathymetry_cell_size != args['cell_size']:
                resampled_bathymetry_uri = \
                os.path.join(args['intermediate_directory'], prefix + \
                str(unique_value()) + '_resampled_bathymetry.tif')
                print('resampling bathymetry...')
                raster_utils.resample_dataset(args['bathymetry_uri'],\
                args['cell_size'], resampled_bathymetry_uri)
                print('resampling done.')
                args['bathymetry_uri'] = resampled_bathymetry_uri
                bathymetry = None
                bathymetry = gdal.Open(args['bathymetry_uri'])
            clipped_bathymetry_uri = os.path.join(args['intermediate_directory'], \
            prefix + str(unique_value()) + '_clipped_bathymetry.tif')
            print('clipping bathymetry...', clipped_bathymetry_uri)
            clipped_bathymetry = raster_utils.clip_dataset(bathymetry,args['aoi'],\
            clipped_bathymetry_uri)
            print('clipping done.')
            args['bathymetry_uri'] = clipped_bathymetry_uri
        args['bathymetry'] = gdal.Open(args['bathymetry_uri'])
    if 'relief_uri' in args:
        args['relief']= gdal.Open(args['relief_uri'])
    if 'geomorphology_uri' in args:
        args['geomorphology'] = ogr.Open(args['geomorphology_uri'])
    if 'sea_level_rise_uri' in args:
        args['sea_level_rise'] = ogr.Open(args['sea_level_rise_uri'])
    if 'global_population_uri' in args:
        args['global_population'] = gdal.Open(args['global_population_uri'])
    # Set to save in 'intermediate_directory'
    save_intermediate = set_save_info(args['intermediate_directory'], \
        args['aoi'], args['cell_size'])
    # Convert the landmass to the right projection (aoi's)
    args['unfiltered_landmass_uri'] = ''
    args['landmass_uri'] = ''
    # Fast execution mode: try to bypass global landmass pre-processing
    if args['execution_mode'] == 'fast':
        print('Fast execution mode: trying to bypass pre-processing of the \
        landmass...')
        filtered_landmass_uri = \
            os.path.join(args['intermediate_directory'], \
            prefix + 'landmass.tif')
        unfiltered_landmass_uri = \
            os.path.join(args['intermediate_directory'], \
            prefix + 'unfiltered_landmass.tif')
        # If there is a filtered landmass, use it
        if os.path.isfile(unfiltered_landmass_uri) and \
            os.path.isfile(filtered_landmass_uri):
            print('Found filtered landmass URI.')
            args['landmass_uri'] = filtered_landmass_uri
            args['unfiltered_landmass_uri'] = unfiltered_landmass_uri
    # No filtered landmass, compute it
    if args['landmass_uri'] == '':
        print('Pre-processing landmass.')
        # Extract the AOI and the landmasses as arrays
        aoi_array = \
            array_from_shapefile(args['aoi'], args['aoi'], args['cell_size'], \
            os.path.join(args['intermediate_directory'], prefix + 'aoi.tif'))
        # TODO: make array_from_shapefile uri-based and remove the line below
        unfiltered_land_array = \
            array_from_shapefile(args['unadjusted_landmass'], \
            args['aoi'], args['cell_size'], \
            os.path.join(args['intermediate_directory'], \
            prefix + 'unfiltered_landmass.tif'))
        # Remove landmasses that are smaller in area than 'land_area_filter'
        # As a reference, see the Scipy tutorial on image processing:
        # http://scipy-lectures.github.com/advanced/image_processing/index.html
        land_area_filter = args['land_area_filter']
        # Generate a structuring element that will consider features connected even
        # if they touch diagonally:
        s = [[1,1,1],
            [1,1,1],
            [1,1,1]]
        ## Associate each unconnected component with a unique label using our
        ## structuring element:
        #labelled_land, label_count = \
        #    measurements.label(unfiltered_land_array, structure=s)
        ## Compute the size of each connected component (in pixels):
        #area_pixel = measurements.sum(unfiltered_land_array, \
        #    labelled_land, range(label_count + 1))
        ## Convert from pixels to square meters:
        #area_meters = area_pixel * args['cell_size'] * args['cell_size']
        ## Remove small islands
        #labels_to_remove = area_meters < land_area_filter
        #land_array = np.copy(unfiltered_land_array)
        #land_array[labels_to_remove[labelled_land]] = 0
        ## store the new landmass raster
        args['unfiltered_landmass_uri'] = \
            save_intermediate(unfiltered_land_array, \
            prefix +'unfiltered_landmass.tif')
        args['landmass_uri'] = save_intermediate(unfiltered_land_array, prefix +'landmass.tif')
        args['landmass'] = gdal.Open(args['landmass_uri'])
    # Use the arrays to construct the shore data
    #aoi_raster = gdal.Open(args['aoi_raster_uri'])
    #aoi_array = aoi_raster.GetRasterBand(1).ReadAsArray()
    aoi_array = args['aoi_raster'].GetRasterBand(1).ReadAsArray()
    unfiltered_land_raster = gdal.Open(args['unfiltered_landmass_uri'])
    unfiltered_land_array = \
        unfiltered_land_raster.GetRasterBand(1).ReadAsArray()
    unfiltered_land_array[unfiltered_land_array == 0] = sea()
    unfiltered_land_array[aoi_array == 0] = nodata()
    landmass_raster = gdal.Open(args['landmass_uri'])
    land_array = landmass_raster.GetRasterBand(1).ReadAsArray()
    #land_array = args['landmass'].GetRasterBand(1).ReadAsArray()
    land_array[land_array == 0] = sea()
    land_array[aoi_array == 0] = nodata()
    args['shore_uri'] = ''
    args['unfiltered_shore_uri'] = '' 
    # compute shoreline and fetch
    if args['execution_mode'] == 'fast':
        shore_uri = os.path.join(args['intermediate_directory'], 
            prefix + 'shore.tif')
        unfiltered_shore_uri = os.path.join(args['intermediate_directory'],
            prefix + 'unfiltered_shore.tif')

        if os.path.isfile(shore_uri):
            LOGGER.debug('Found shore data. Skipping shore detection.')
            args['shore_uri'] = shore_uri
        
        if os.path.isfile(unfiltered_shore_uri):
            LOGGER.debug('Found unfiltered shore data. Skipping detection.')
            args['unfiltered_shore_uri'] = unfiltered_shore_uri
        
    if args['shore_uri'] == '':
        print('detecting shore...')
        shore_array = detect_shore(land_array)
        args['shore_uri'] = \
            save_intermediate(shore_array, prefix + 'shore.tif') 
        shore_array = None

    if args['unfiltered_shore_uri'] == '':
        print('detecting unfiltered shore...')
        unfiltered_shore_array = detect_shore(unfiltered_land_array)
        args['unfiltered_shore_uri'] = \
            save_intermediate(unfiltered_shore_array, \
            prefix + 'unfiltered_shore.tif') 
        unfiltered_shore_array = None

    if 'bathymetry_uri' in args:
        args['fetch_distance_uris'] = ''
        args['fetch_depth_uris'] = ''
        # Try to bypass fetch computation if not necessary
        if args['execution_mode'] == 'fast':
            # Look for fetch distance and depth data on disk
            # Create the list of all the files that should be present:
            distance_files = {}
            depth_files = {}
            for angle in sectors_deg:
                distance_files[str(angle)] =  \
                    os.path.join(args['intermediate_directory'], \
                    prefix + 'fetch_distance_' + str(angle) + '.tif')
                depth_files[str(angle)] = \
                    os.path.join(args['intermediate_directory'], \
                    prefix + 'fetch_depth_' + str(angle) + '.tif')
            # Build the list of files in the intermediate directory
            inter_files = os.listdir(args['intermediate_directory'])
            # Add the full path to the files:
            inter_files = set([os.path.join(args['intermediate_directory'], f) \
                for f in inter_files])
            # Intersect the sets to find the common URIs
            dist = set(distance_files.values())
            dist = len(dist.intersection(inter_files))
            depth = set(depth_files.values())
            depth = len(depth.intersection(inter_files))
            # If all the files are there, 1 depth and 1 distance per sector,
            # then we can bypass fetch computation
            if (dist + depth) == (2 * SECTOR_COUNT):
                args['fetch_distance_uris'] = distance_files
                args['fetch_depth_uris'] = depth_files
                # Re-create fetch_distances and fetch_depths:
                fetch_distances = {}
                fetch_depths = {}
                # Extract the points along the shore
                shore_raster = gdal.Open(args['shore_uri'])
                shore_array = shore_raster.GetRasterBand(1).ReadAsArray()
                shore_points = np.where(shore_array > 0)
                # Fill up the dictionaries with the values of the shore points
                fetch_distances = {}
                fetch_depths = {}
                # We need numpy arrays later on
                for index in range(shore_points[0].size):
                    point = (shore_points[0][index], shore_points[1][index])
                    fetch_distances[point] = np.zeros(SECTOR_COUNT)
                    fetch_depths[point] = np.zeros(SECTOR_COUNT)
                    
                # Extract the arrays and iterate throught the dictionaries
                # to retreive the points that we are interested in
                for sector in range(len(sectors_deg)):
                    angle = sectors_deg[sector]
                    # Extract distances
                    distances_uri = \
                        os.path.join(args['intermediate_directory'], \
                        prefix + 'fetch_distance_' + str(angle) + '.tif')
                    distances_raster = gdal.Open(distances_uri)
                    distances_array = \
                        distances_raster.GetRasterBand(1).ReadAsArray()
                    shore_distances = distances_array[shore_points]
                    # Extract depths
                    depths_uri = \
                        os.path.join(args['intermediate_directory'], \
                        prefix + 'fetch_depth_' + str(angle) + '.tif')
                    depths_raster = gdal.Open(depths_uri)
                    depths_array = depths_raster.GetRasterBand(1).ReadAsArray()
                    shore_depths = depths_array[shore_points]
                    # Initialize distance and depth dictionaries
                    for index in range(shore_points[0].size):
                        point = (shore_points[0][sector],
                            shore_points[1][sector])
                        fetch_distances[point][sector] = \
                            shore_distances[sector]
                        fetch_depths[point][sector] = \
                            shore_depths[sector]
                 
                args['fetch_distances'] = fetch_distances
                args['fetch_depths'] = fetch_depths

        # No fetch available, compute it
        if args['fetch_distance_uris'] == '' and \
            args['fetch_depth_uris'] == '':
            print('computing fetch...')
            shore_raster = gdal.Open(args['unfiltered_shore_uri'])
            shore_array = shore_raster.GetRasterBand(1).ReadAsArray()
            bathymetry_raster = gdal.Open(args['bathymetry_uri'])
            bathymetry_array = bathymetry_raster.GetRasterBand(1).ReadAsArray()
            bathymetry_nodata = \
                bathymetry_raster.GetRasterBand(1).GetNoDataValue()
            args['fetch_distances'],args['fetch_depths'] = \
                 compute_fetch_fast(unfiltered_land_array,
                 args['rays_per_sector'], args['max_fetch'], \
                 args['cell_size'], shore_array,
                 bathymetry_array, bathymetry_nodata)
            args['fetch_distance_uris'] = \
                save_fetch_distances(args['fetch_distances'], args['aoi'], \
                args['cell_size'], args['intermediate_directory'], prefix)
            args['fetch_depth_uris'] = \
                save_fetch_depths(args['fetch_depths'], args['aoi'], \
                args['cell_size'], args['intermediate_directory'], prefix)
            print('done.')
    else:
        print('Skipping the computation of fetch.')

    return args
 
